{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HMM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDJIV2EVBuFZ"
      },
      "source": [
        "# Fun with Hidden Markov Models\n",
        "*by Loren Lugosch*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rWFkdjYOlk8"
      },
      "source": [
        "This notebook introduces the Hidden Markov Model (HMM), a simple model for sequential data.\n",
        "\n",
        "We will see:\n",
        "- what an HMM is and when you might want to use it;\n",
        "- the so-called \"three problems\" of an HMM; and \n",
        "- how to implement an HMM in PyTorch.\n",
        "\n",
        "(The code in this notebook can also be found at https://github.com/lorenlugosch/pytorch_HMM.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4g7IG7CBx-Y"
      },
      "source": [
        "## The HMM setup\n",
        "\n",
        "The hypothetical scenario of the friend travelling between cities and sending you selfies can be modeled using an HMM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMPrA6Uv-u-K"
      },
      "source": [
        "An HMM makes two key assumptions:\n",
        "- **Assumption 1:** The state at time $t$ depends *only* on the state at the previous time $t-1$. \n",
        "- **Assumption 2:** The output at time $t$ depends *only* on the state at time $t$.\n",
        "\n",
        "These two assumptions make it possible to efficiently compute certain quantities that we may be interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRNhSK7LgEIS"
      },
      "source": [
        "## Components of an HMM\n",
        "An HMM has three sets of trainable parameters.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pu3zm77vXwp"
      },
      "source": [
        "- The **transition model** is a square matrix $A$, where $A_{s, s'}$ represents $p(z_t = s|z_{t-1} = s')$, the probability of jumping from state $s'$ to state $s$. \n",
        "\n",
        "- The **emission model** $b_s(x_t)$ tells us $p(x_t|z_t = s)$, the probability of generating $x_t$ when the system is in state $s$. For discrete observations, which we will use in this notebook, the emission model is just a lookup table, with one row for each state, and one column for each observation. For real-valued observations, it is common to use a Gaussian mixture model or neural network to implement the emission model. \n",
        "\n",
        "- The **state priors** tell us $p(z_1 = s)$, the probability of starting in state $s$. We use $\\pi$ to denote the vector of state priors, so $\\pi_s$ is the state prior for state $s$.\n",
        "\n",
        "Let's program an HMM class in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as distrib\n",
        "import torch.distributions.transforms as transform\n",
        "# Imports for plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OSjVMTgrK8Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZbW6Pj0og7K"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class HMM(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Hidden Markov Model with discrete observations.\n",
        "  \"\"\"\n",
        "  def __init__(self, N, distributions):\n",
        "    super(HMM, self).__init__()\n",
        "    #self.M = M # number of possible observations\n",
        "    self.N = N # number of states\n",
        "\n",
        "    # A\n",
        "    self.transition_model = TransitionModel(self.N)\n",
        "\n",
        "    # b(x_t)\n",
        "    self.emission_model = EmissionModel(self.N, distributions)\n",
        "\n",
        "    # pi\n",
        "    self.unnormalized_state_priors = torch.nn.Parameter(torch.randn(self.N))\n",
        "\n",
        "    # use the GPU\n",
        "    self.is_cuda = torch.cuda.is_available()\n",
        "    if self.is_cuda: self.cuda()\n",
        "\n",
        "class TransitionModel(torch.nn.Module):\n",
        "  def __init__(self, N):\n",
        "    super(TransitionModel, self).__init__()\n",
        "    self.N = N\n",
        "    self.unnormalized_transition_matrix = torch.nn.Parameter(torch.randn(N,N))\n",
        "\n",
        "class EmissionModel(torch.nn.Module):\n",
        "  def __init__(self, N, distributions):\n",
        "    super(EmissionModel, self).__init__()\n",
        "    self.N = N\n",
        "    self.distributions = distributions ## list of distributions\n",
        "\n",
        "  def pdf(self, hidden_state, observation):\n",
        "    current_distribution = self.distributions[hidden_state]\n",
        "    return torch.exp(current_distribution.log_prob(torch.Tensor(observation)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpgkwNyVwmyM"
      },
      "source": [
        "def sample(self, T=10):\n",
        "  state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
        "  transition_matrix = torch.nn.functional.softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
        "  #emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
        "\n",
        "  # sample initial state\n",
        "  z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
        "  z = []; x = []\n",
        "  z.append(z_t)\n",
        "  for t in range(0,T):\n",
        "    # sample emission\n",
        "    # x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
        "    current_distribution = self.emission_model.distributions[z_t]\n",
        "    x_t = current_distribution.sample()\n",
        "    x.append(x_t)\n",
        "\n",
        "    # sample transition\n",
        "    z_t = torch.distributions.categorical.Categorical(transition_matrix[:,z_t]).sample().item()\n",
        "    if t < T-1: z.append(z_t)\n",
        " \n",
        "  return torch.stack(x), z\n",
        "\n",
        "# Add the sampling method to our HMM class\n",
        "HMM.sample = sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One dimentional sampling test"
      ],
      "metadata": {
        "id": "QgHSpJB-mDtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.Normal(-1,1), distrib.Normal(1, 1)])"
      ],
      "metadata": {
        "id": "fvuibzlKQGbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, z = test_model.sample(20)"
      ],
      "metadata": {
        "id": "f-2ZlXeEQU-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.functional.softmax(test_model.transition_model.unnormalized_transition_matrix, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_JoA2TGQm25",
        "outputId": "f5ac075f-8f03-4125-c97b-c65ec44d03ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7758, 0.2836],\n",
              "        [0.2242, 0.7164]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.scatter(list(range(len(x))),x, c=z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ZDqNbbUARKq7",
        "outputId": "a6472967-c559-4b6f-eb11-d03af922ef88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f264e6dd910>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn2igp0GQe60",
        "outputId": "090647f0-7dc2-4ada-aabf-1e39ed2b9895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-1.3998e+00, -1.2692e-03, -6.5672e-01, -1.6916e+00, -9.1379e-01,\n",
              "          2.0331e+00,  9.6820e-01,  2.4606e+00,  1.1227e+00,  3.0998e-01,\n",
              "          1.5695e+00, -1.6654e+00, -1.7475e+00, -3.2506e+00, -8.0724e-01,\n",
              "          1.5555e+00,  2.3876e+00,  4.1671e-01, -2.9125e-02, -1.1235e+00]),\n",
              " [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### two dimentional sampling test"
      ],
      "metadata": {
        "id": "1WNbMlwdmKH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.MultivariateNormal(torch.zeros(2),torch.eye(2)), distrib.MultivariateNormal(torch.ones(2)*5,torch.eye(2))])"
      ],
      "metadata": {
        "id": "PeTbT-VWSkSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, z = test_model.sample(200)"
      ],
      "metadata": {
        "id": "7lYjGYu4TLPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.functional.softmax(test_model.transition_model.unnormalized_transition_matrix, dim=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfWtNWpmTOuE",
        "outputId": "a09e970a-cc62-49b9-e20e-4939d634dea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7884, 0.0731],\n",
              "        [0.2116, 0.9269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x[:,0],x[:,1], c=z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "HIudU1RcTRU-",
        "outputId": "f3e58d91-cfe8-49f0-a33a-cefbb534d6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f265e1b2090>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohsdYScawkRG"
      },
      "source": [
        "Let's try hard-coding an HMM for generating fake words. (We'll also add some helper functions for encoding and decoding strings.)\n",
        "\n",
        "We will assume that the system has one state for generating vowels and one state for generating consonants, and the transition matrix has 0s on the diagonal---in other words, the system cannot stay in the vowel state or the consonant state for one than one timestep; it has to switch.\n",
        "\n",
        "Since we pass the transition matrix through a softmax, to get 0s we set the unnormalized parameter values to $-\\infty$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_distr = distrib.Normal(5,1)"
      ],
      "metadata": {
        "id": "GO3bHRzkkQkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(test_distr.log_prob(torch.Tensor([4])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nomKg-z9kVZn",
        "outputId": "ddc71202-aa8b-485c-aef5-42cde9df40fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2420])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 10, 1000)\n"
      ],
      "metadata": {
        "id": "ygpIvgX4kz9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q0_density = torch.exp(test_distr.log_prob(torch.Tensor(x))).numpy()\n"
      ],
      "metadata": {
        "id": "vnlFqHsQkk5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(1, 1, sharex=True, figsize=(15, 5))\n",
        "ax1.plot(x, q0_density); ax1.fill_between(x, q0_density, 0, alpha=0.5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "FvZwh66UlAa6",
        "outputId": "22a3b4b0-3236-4885-adc2-99d05d4db0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f265e1eff90>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKzlTlfRgZod"
      },
      "source": [
        "## The Three Problems\n",
        "\n",
        "In a [classic tutorial](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf) on HMMs, Lawrence Rabiner describes \"three problems\" that need to be solved before you can effectively use an HMM. They are:\n",
        "- Problem 1: How do we efficiently compute $p(\\mathbf{x})$?\n",
        "- Problem 2: How do we find the most likely state sequence $\\mathbf{z}$ that could have generated the data? \n",
        "- Problem 3: How do we train the model?\n",
        "\n",
        "In the rest of the notebook, we will see how to solve each problem and implement the solutions in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_RfIAnmN2RZ"
      },
      "source": [
        "### Problem 1: How do we compute $p(\\mathbf{x})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zUUYH0giKV"
      },
      "source": [
        "\n",
        "#### *Why?*\n",
        "Why might we care about computing $p(\\mathbf{x})$? Here's two reasons.\n",
        "* Given two HMMs, $\\theta_1$ and $\\theta_2$, we can compute the likelihood of some data $\\mathbf{x}$ under each model, $p_{\\theta_1}(\\mathbf{x})$ and $p_{\\theta_2}(\\mathbf{x})$, to decide which model is a better fit to the data. \n",
        "\n",
        "  (For example, given an HMM for English speech and an HMM for French speech, we could compute the likelihood given each model, and pick the model with the higher likelihood to infer whether the person is speaking English or French.)\n",
        "* Being able to compute $p(\\mathbf{x})$ gives us a way to train the model, as we will see later.\n",
        "\n",
        "#### *How?*\n",
        "Given that we want $p(\\mathbf{x})$, how do we compute it?\n",
        "\n",
        "We've assumed that the data is generated by visiting some sequence of states $\\mathbf{z}$ and picking an output $x_t$ for each $z_t$ from the emission distribution $p(x_t|z_t)$. So if we knew $\\mathbf{z}$, then the probability of $\\mathbf{x}$ could be computed as follows:\n",
        "\n",
        "$$p(\\mathbf{x}|\\mathbf{z}) = \\prod_{t} p(x_t|z_t) p(z_t|z_{t-1})$$\n",
        "\n",
        "However, we don't know $\\mathbf{z}$; it's hidden. But we do know the probability of any given $\\mathbf{z}$, independent of what we observe. So we could get the probability of $\\mathbf{x}$ by summing over the different possibilities for $\\mathbf{z}$, like this:\n",
        "\n",
        "$$p(\\mathbf{x}) = \\sum_{\\mathbf{z}} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) = \\sum_{\\mathbf{z}} \\prod_{t} p(x_t|z_t) p(z_t|z_{t-1})$$\n",
        "\n",
        "The problem is: if you try to take that sum directly, you will need to compute $N^T$ terms. This is impossible to do for anything but very short sequences. For example, let's say the sequence is of length $T=100$ and there are $N=2$ possible states. Then we would need to check $N^T = 2^{100} \\approx 10^{30}$ different possible state sequences.\n",
        "\n",
        "We need a way to compute $p(\\mathbf{x})$ that doesn't require us to explicitly calculate all $N^T$ terms. For this, we use the forward algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrH0YdUAhS6J"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Forward Algorithm</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\alpha_{s,1} := b_s(x_1) \\cdot \\pi_s$ \n",
        "> \n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "> $\\alpha_{s,t} := b_s(x_t) \\cdot \\underset{s'}{\\sum} A_{s, s'} \\cdot \\alpha_{s',t-1} $\n",
        "> \n",
        "> $p(\\mathbf{x}) := \\underset{s}{\\sum} \\alpha_{s,T}$\\\n",
        "> return $p(\\mathbf{x})$\n",
        "________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAdpwRiMn8Vn"
      },
      "source": [
        "The forward algorithm is much faster than enumerating all $N^T$ possible state sequences: it requires only $O(N^2T)$ operations to run, since each step is mostly multiplying the vector of forward variables by the transition matrix. (And very often we can reduce that complexity even further, if the transition matrix is sparse.)\n",
        "\n",
        "There is one practical problem with the forward algorithm as presented above: it is prone to underflow due to multiplying a long chain of small numbers, since probabilities are always between 0 and 1. Instead, let's do everything in the log domain. In the log domain, a multiplication becomes a sum, and a sum becomes a [logsumexp](https://lorenlugosch.github.io/posts/2020/06/logsumexp/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ8VsLFxA3iT"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Forward Algorithm (Log Domain)</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\text{log }\\alpha_{s,1} := \\text{log }b_s(x_1) + \\text{log }\\pi_s$ \n",
        "> \n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "> $\\text{log }\\alpha_{s,t} := \\text{log }b_s(x_t) +  \\underset{s'}{\\text{logsumexp}} \\left( \\text{log }A_{s, s'} + \\text{log }\\alpha_{s',t-1} \\right)$\n",
        "> \n",
        "> $\\text{log }p(\\mathbf{x}) := \\underset{s}{\\text{logsumexp}} \\left( \\text{log }\\alpha_{s,T} \\right)$\\\n",
        "> return $\\text{log }p(\\mathbf{x})$\n",
        "________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g55ik6ZCEiJU"
      },
      "source": [
        "Now that we have a numerically stable version of the forward algorithm, let's implement it in PyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CMdK1EfE1SJ"
      },
      "source": [
        "\n",
        "def HMM_forward(self, x, T):\n",
        "  \"\"\"\n",
        "  x : IntTensor of shape (batch size, T_max)\n",
        "  T : IntTensor of shape (batch size)\n",
        "\n",
        "  Compute log p(x) for each example in the batch.\n",
        "  T = length of each example\n",
        "  \"\"\"\n",
        "  if self.is_cuda:\n",
        "  \tx = x.cuda()\n",
        "  \tT = T.cuda()\n",
        "\n",
        "  batch_size = x.shape[0]; T_max = x.shape[1]\n",
        "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "  log_alpha = torch.zeros(batch_size, T_max, self.N) # table (sample, t, state) containing log probability of observations from sample to time t and being in state (in time t)\n",
        "  if self.is_cuda: log_alpha = log_alpha.cuda()\n",
        "\n",
        "  print(log_state_priors)\n",
        "  log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
        "  for t in range(1, T_max):\n",
        "    log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
        "\n",
        "  # Select the sum for the final timestep (each x may have different length).\n",
        "  #print(\"alpha\\n\", log_alpha)\n",
        "  log_sums = log_alpha.logsumexp(dim=2)\n",
        "  #print(\"log_sums\\n\", log_sums)\n",
        "  #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
        "  log_probs = torch.gather(log_sums, 1, T.view(-1,1)-1)\n",
        "  return log_probs\n",
        "\n",
        "def emission_model_forward(self, x_t): ## TODO\n",
        "  #out = self.distributions.log_prob(x_t)\n",
        "  #out = \n",
        "  out  = []\n",
        "  for state in range(self.N):\n",
        "    out.append( self.distributions[state].log_prob(x_t) )\n",
        "  result = torch.stack(out, dim = 1)\n",
        "  #print(\"emission probs\\n\",result)\n",
        "  return result\n",
        "\n",
        "def transition_model_forward(self, log_alpha):\n",
        "  \"\"\"\n",
        "  log_alpha : Tensor of shape (batch size, N)\n",
        "  Multiply previous timestep's alphas by transition matrix (in log domain)\n",
        "  \"\"\"\n",
        "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "  # Matrix multiplication in the log domain\n",
        "  out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
        "  return out\n",
        "\n",
        "def log_domain_matmul(log_A, log_B):\n",
        "\t\"\"\"\n",
        "\tlog_A : m x n\n",
        "\tlog_B : n x p\n",
        "\toutput : m x p matrix\n",
        "\n",
        "\tNormally, a matrix multiplication\n",
        "\tcomputes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
        "\n",
        "\tA log domain matrix multiplication\n",
        "\tcomputes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
        "\t\"\"\"\n",
        "\tm = log_A.shape[0]\n",
        "\tn = log_A.shape[1]\n",
        "\tp = log_B.shape[1]\n",
        "\n",
        "\t# log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
        "\t# log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
        "    # fix for PyTorch > 1.5 by egaznep on Github:\n",
        "\tlog_A_expanded = torch.reshape(log_A, (m,n,1))\n",
        "\tlog_B_expanded = torch.reshape(log_B, (1,n,p))\n",
        "\n",
        "\telementwise_sum = log_A_expanded + log_B_expanded\n",
        "\tout = torch.logsumexp(elementwise_sum, dim=1)\n",
        "\n",
        "\treturn out\n",
        "\n",
        "TransitionModel.forward = transition_model_forward\n",
        "EmissionModel.forward = emission_model_forward\n",
        "HMM.forward = HMM_forward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test forward method"
      ],
      "metadata": {
        "id": "K4mnlI8muU6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.Normal(-1,1), distrib.Normal(1, 1)])"
      ],
      "metadata": {
        "id": "qPLPQRF-uXid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, states = test_model.sample(30)"
      ],
      "metadata": {
        "id": "Xm_JPMKxucOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T =torch.Tensor([10,10,10], dtype= torch.int64)\n",
        "T = torch.ones([1,3], dtype=torch.int64)*10\n",
        "T.view(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtSSX_nuvn-L",
        "outputId": "a59f8e24-2a02-4dc1-e766-94b9105e5805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10],\n",
              "        [10],\n",
              "        [10]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(torch.reshape(x, (3, 10)), T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaBm7pbou8d2",
        "outputId": "4dafdafe-e38a-4248-fe7b-e0eaa096b775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.3451, -1.2314], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
            "emission probs\n",
            " tensor([[-1.0122, -2.1485],\n",
            "        [-3.7534, -0.9915],\n",
            "        [-1.6459, -1.2343]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-4.3764, -1.1172],\n",
            "        [-2.5312, -0.9398],\n",
            "        [-0.9971, -2.2065]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-2.5485, -0.9379],\n",
            "        [-1.1631, -4.5607],\n",
            "        [-1.2755, -4.9643]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-0.9397, -3.3475],\n",
            "        [-0.9247, -3.1390],\n",
            "        [-2.5072, -0.9426]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-2.5871, -0.9340],\n",
            "        [-1.9484, -1.0786],\n",
            "        [-2.4330, -0.9527]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.0313, -3.9793],\n",
            "        [-1.1017, -1.8925],\n",
            "        [-3.3649, -9.7884]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.0225, -3.9325],\n",
            "        [-3.0008, -0.9198],\n",
            "        [-3.8865, -1.0141]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.9762, -6.8845],\n",
            "        [-1.4726, -1.3680],\n",
            "        [-3.5035, -0.9564]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[ -5.9556, -14.3033],\n",
            "        [ -1.0194,  -2.1230],\n",
            "        [ -2.5178,  -0.9414]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-0.9235, -3.1154],\n",
            "        [-3.1284, -0.9242],\n",
            "        [-1.8859, -1.1046]], device='cuda:0')\n",
            "alpha\n",
            " tensor([[[ -1.3573,  -3.3799],\n",
            "         [ -6.3718,  -2.9787],\n",
            "         [ -6.6616,  -4.2562],\n",
            "         [ -6.2480,  -7.9035],\n",
            "         [ -9.4398,  -7.6208],\n",
            "         [ -9.6083, -11.8513],\n",
            "         [-11.2841, -14.0756],\n",
            "         [-13.9402, -18.7580],\n",
            "         [-20.6083, -28.9019],\n",
            "         [-22.2493, -24.3927]],\n",
            "\n",
            "        [[ -4.0985,  -2.2229],\n",
            "         [ -5.7218,  -3.4198],\n",
            "         [ -5.6213,  -8.2735],\n",
            "         [ -7.2204,  -9.3383],\n",
            "         [ -9.8138,  -8.8171],\n",
            "         [-10.6419, -10.8331],\n",
            "         [-13.9438, -11.4792],\n",
            "         [-14.0111, -13.1506],\n",
            "         [-14.8402, -15.3664],\n",
            "         [-18.3717, -15.8449]],\n",
            "\n",
            "        [[ -1.9911,  -2.4657],\n",
            "         [ -3.3770,  -4.2549],\n",
            "         [ -5.1390,  -8.5634],\n",
            "         [ -8.3436,  -6.7077],\n",
            "         [-10.0564,  -7.8904],\n",
            "         [-12.2738, -17.9618],\n",
            "         [-16.8757, -13.9526],\n",
            "         [-18.5603, -15.2342],\n",
            "         [-18.8829, -16.5136],\n",
            "         [-19.4468, -17.9156]]], device='cuda:0', grad_fn=<CopySlices>)\n",
            "log_sums\n",
            " tensor([[ -1.2331,  -2.9456,  -4.1698,  -6.0732,  -7.4705,  -9.5074, -11.2246,\n",
            "         -13.9322, -20.6081, -22.1384],\n",
            "        [ -2.0803,  -3.3245,  -5.5532,  -7.1068,  -8.5030, -10.0398, -11.3976,\n",
            "         -12.7978, -14.3759, -15.7681],\n",
            "        [ -1.5074,  -3.0294,  -5.1069,  -6.5297,  -7.7818, -12.2704, -13.9002,\n",
            "         -15.1989, -16.4241, -17.7198]], device='cuda:0',\n",
            "       grad_fn=<LogsumexpBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-22.1384],\n",
              "        [-15.7681],\n",
              "        [-17.7198]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBCrFobsEM8X"
      },
      "source": [
        "#### *Side note: deriving the forward algorithm*\n",
        "\n",
        "If you're interested in understanding how the forward algorithm actually computes $p(\\mathbf{x})$, read this section; if not, skip to the next part on \"Problem 2\" (finding the most likely state sequence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpHWWKcxhjkx"
      },
      "source": [
        "\n",
        "\n",
        "To derive the forward algorithm, start by deriving the forward variable:\n",
        "\n",
        "$\n",
        "\\begin{align} \n",
        "    \\alpha_{s,t} &= p(x_1, x_2, \\dots, x_t, z_t=s) \\\\\n",
        "     &= p(x_t | x_1, x_2, \\dots, x_{t-1}, z_t = s) \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_t = s)  \\\\ \n",
        "    &= p(x_t | z_t = s) \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_t = s) \\\\\n",
        "    &= p(x_t | z_t = s) \\cdot \\left( \\sum_{s'} p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s', z_t = s) \\right)\\\\\n",
        "    &= p(x_t | z_t = s) \\cdot \\left( \\sum_{s'} p(z_t = s | x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s') \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s') \\right)\\\\\n",
        "    &= \\underbrace{p(x_t | z_t = s)}_{\\text{emission model}} \\cdot \\left( \\sum_{s'} \\underbrace{p(z_t = s | z_{t-1}=s')}_{\\text{transition model}} \\cdot \\underbrace{p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s')}_{\\text{forward variable for previous timestep}} \\right)\\\\\n",
        "    &= b_s(x_t) \\cdot \\left( \\sum_{s'} A_{s, s'} \\cdot \\alpha_{s',t-1} \\right)\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "I'll explain how to get to each line of this equation from the previous line. \n",
        "\n",
        "Line 1 is the definition of the forward variable $\\alpha_{s,t}$.\n",
        "\n",
        "Line 2 is the chain rule ($p(A,B) = p(A|B) \\cdot p(B)$, where $A$ is $x_t$ and $B$ is all the other variables).\n",
        "\n",
        "In Line 3, we apply Assumption 2: the probability of observation $x_t$ depends only on the current state $z_t$.\n",
        "\n",
        "In Line 4, we marginalize over all the possible states in the previous timestep $t-1$.\n",
        "\n",
        "In Line 5, we apply the chain rule again.\n",
        "\n",
        "In Line 6, we apply Assumption 1: the current state depends only on the previous state.\n",
        "\n",
        "In Line 7, we substitute in the emission probability, the transition probability, and the forward variable for the previous timestep, to get the complete recursion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh1ovNjWDbIA"
      },
      "source": [
        "The formula above can be used for $t = 2 \\rightarrow T$. At $t=1$, there is no previous state, so instead of the transition matrix $A$, we use the state priors $\\pi$, which tell us the probability of starting in each state. Thus for $t=1$, the forward variables are computed as follows:\n",
        "\n",
        "$$\\begin{align} \n",
        "\\alpha_{s,1} &= p(x_1, z_1=s) \\\\\n",
        "  &= p(x_1 | z_1 = s) \\cdot p(z_1 = s)  \\\\ \n",
        "&= b_s(x_1) \\cdot \\pi_s\n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRzSqkRkEWKX"
      },
      "source": [
        "Finally, to compute $p(\\mathbf{x}) = p(x_1, x_2, \\dots, x_T)$, we marginalize over $\\alpha_{s,T}$, the forward variables computed in the last timestep:\n",
        "\n",
        "$$\\begin{align*} \n",
        "p(\\mathbf{x}) &= \\sum_{s} p(x_1, x_2, \\dots, x_T, z_T = s) \\\\ \n",
        "&= \\sum_{s} \\alpha_{s,T}\n",
        "\\end{align*}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLBU8Iu7I5Tb"
      },
      "source": [
        "You can get from this formulation to the log domain formulation by taking the log of the forward variable, and using these identities:\n",
        "- $\\text{log }(a \\cdot b) = \\text{log }a + \\text{log }b$\n",
        "- $\\text{log }(a + b) = \\text{log }(e^{\\text{log }a} + e^{\\text{log }b}) = \\text{logsumexp}(\\text{log }a, \\text{log }b)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxivzF8hgpiW"
      },
      "source": [
        "### Problem 2: How do we compute $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Kv2yyiN7SX"
      },
      "source": [
        "Given an observation sequence $\\mathbf{x}$, we may want to find the most likely sequence of states that could have generated $\\mathbf{x}$. (Given the sequence of selfies, we want to infer what cities the friend visited.) In other words, we want $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x})$.\n",
        "\n",
        "We can use Bayes' rule to rewrite this expression:\n",
        "$$\\begin{align*} \n",
        "    \\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x}) &= \\underset{\\mathbf{z}}{\\text{argmax }} \\frac{p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{p(\\mathbf{x})} \\\\ \n",
        "    &= \\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})\n",
        "\\end{align*}$$\n",
        "\n",
        "Hmm! That last expression, $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})$, looks suspiciously similar to the intractable expression we encountered before introducing the forward algorithm, $\\underset{\\mathbf{z}}{\\sum} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})$.\n",
        "\n",
        "And indeed, just as the intractable *sum* over all $\\mathbf{z}$ can be implemented efficiently using the forward algorithm, so too this intractable *argmax* can be implemented efficiently using a similar divide-and-conquer algorithm: the legendary Viterbi algorithm!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niKZEX5xWeWR"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Viterbi Algorithm</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta_{s,1} := b_s(x_1) \\cdot \\pi_s$\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\psi_{s,1} := 0$\n",
        ">\n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta_{s,t} := b_s(x_t) \\cdot \\left( \\underset{s'}{\\text{max }} A_{s, s'} \\cdot \\delta_{s',t-1} \\right)$\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\psi_{s,t} := \\underset{s'}{\\text{argmax }} A_{s, s'} \\cdot \\delta_{s',t-1}$\n",
        "> \n",
        "> $z_T^* := \\underset{s}{\\text{argmax }} \\delta_{s,T}$\\\n",
        "> for $t = T-1 \\rightarrow 1$:\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_{t}^* := \\psi_{z_{t+1}^*,t+1}$\n",
        "> \n",
        "> $\\mathbf{z}^* := \\{z_{1}^*, \\dots, z_{T}^* \\}$\\\n",
        "return $\\mathbf{z}^*$\n",
        "________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcHVTCucZV6K"
      },
      "source": [
        "The Viterbi algorithm looks somewhat gnarlier than the forward algorithm, but it is essentially the same algorithm, with two tweaks: 1) instead of taking the sum over previous states, we take the max; and 2) we record the argmax of the previous states in a table, and loop back over this table at the end to get $\\mathbf{z}^*$, the most likely state sequence. (And like the forward algorithm, we should run the Viterbi algorithm in the log domain for better numerical stability.) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlN7IY_JZ5A-"
      },
      "source": [
        "Let's add the Viterbi algorithm to our PyTorch model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeDG8DVmZ-P0"
      },
      "source": [
        "def viterbi(self, x, T):\n",
        "  \"\"\"\n",
        "  x : IntTensor of shape (batch size, T_max)\n",
        "  T : IntTensor of shape (batch size)\n",
        "  Find argmax_z log p(x|z) for each (x) in the batch.\n",
        "  \"\"\"\n",
        "  if self.is_cuda:\n",
        "    x = x.cuda()\n",
        "    T = T.cuda()\n",
        "\n",
        "  batch_size = x.shape[0]; T_max = x.shape[1]\n",
        "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "  log_delta = torch.zeros(batch_size, T_max, self.N).float()\n",
        "  psi = torch.zeros(batch_size, T_max, self.N).long()\n",
        "  if self.is_cuda:\n",
        "    log_delta = log_delta.cuda()\n",
        "    psi = psi.cuda()\n",
        "\n",
        "  log_delta[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
        "  for t in range(1, T_max):\n",
        "    max_val, argmax_val = self.transition_model.maxmul(log_delta[:, t-1, :])\n",
        "    log_delta[:, t, :] = self.emission_model(x[:,t]) + max_val\n",
        "    psi[:, t, :] = argmax_val\n",
        "\n",
        "  # Get the log probability of the best path\n",
        "  log_max = log_delta.max(dim=2)[0]\n",
        "  best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
        "\n",
        "  # This next part is a bit tricky to parallelize across the batch,\n",
        "  # so we will do it separately for each example.\n",
        "  z_star = []\n",
        "  for i in range(0, batch_size):\n",
        "    z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
        "    for t in range(T[i] - 1, 0, -1):\n",
        "      z_t = psi[i, t, z_star_i[0]].item()\n",
        "      z_star_i.insert(0, z_t)\n",
        "\n",
        "    z_star.append(z_star_i)\n",
        "\n",
        "  return z_star, best_path_scores # return both the best path and its log probability\n",
        "\n",
        "def transition_model_maxmul(self, log_alpha):\n",
        "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "  out1, out2 = maxmul(log_transition_matrix, log_alpha.transpose(0,1))\n",
        "  return out1.transpose(0,1), out2.transpose(0,1)\n",
        "\n",
        "def maxmul(log_A, log_B):\n",
        "\t\"\"\"\n",
        "\tlog_A : m x n\n",
        "\tlog_B : n x p\n",
        "\toutput : m x p matrix\n",
        "\n",
        "\tSimilar to the log domain matrix multiplication,\n",
        "\tthis computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
        "\t\"\"\"\n",
        "\tm = log_A.shape[0]\n",
        "\tn = log_A.shape[1]\n",
        "\tp = log_B.shape[1]\n",
        "\n",
        "\tlog_A_expanded = torch.stack([log_A] * p, dim=2)\n",
        "\tlog_B_expanded = torch.stack([log_B] * m, dim=0)\n",
        "\n",
        "\telementwise_sum = log_A_expanded + log_B_expanded\n",
        "\tout1,out2 = torch.max(elementwise_sum, dim=1)\n",
        "\n",
        "\treturn out1,out2\n",
        "\n",
        "TransitionModel.maxmul = transition_model_maxmul\n",
        "HMM.viterbi = viterbi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test viterbi"
      ],
      "metadata": {
        "id": "ddLYM5OH8IzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.Normal(-1,1), distrib.Normal(1, 1)])"
      ],
      "metadata": {
        "id": "OLPEYOgY8FZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, states = test_model.sample(10)"
      ],
      "metadata": {
        "id": "G0fI_4vX8QPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgx_mORFC9na",
        "outputId": "4b3d666a-d808-48ad-fbaf-d13a366cedfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1.9858,  0.0714,  1.2443,  3.0269, -1.8505, -0.5393,  0.8847, -0.5858,\n",
              "          1.0002, -0.4509]), [1, 1, 1, 1, 0, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = torch.ones([1], dtype=torch.int64)*10\n"
      ],
      "metadata": {
        "id": "wJkumbFN8U25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viterbi_results = test_model.viterbi(torch.reshape(x, (1, 10)), T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBxpXgvbDHvU",
        "outputId": "6463cab8-3085-42e6-8bd4-07fea3da7a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emission probs\n",
            " tensor([[-5.3763, -1.4048]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.4929, -1.3501]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-3.4374, -0.9488]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-9.0271, -2.9732]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.2806, -4.9815]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.0251, -2.1036]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-2.6951, -0.9256]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.0047, -2.1763]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-2.9194, -0.9189]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.0697, -1.9716]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "viterbi_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvOSLuk6DmKD",
        "outputId": "d8d7e8dc-09a2-4f4f-a659-7cd6b0aba296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[1, 1, 1, 1, 0, 0, 1, 1, 1, 1]],\n",
              " tensor([[-20.1063]], device='cuda:0', grad_fn=<GatherBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.forward(torch.reshape(x, (1, 10)), T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS7w6yLLEKYt",
        "outputId": "a9c24844-6e80-4cac-c604-2ab3b2ca6631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.7065, -0.6800], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
            "emission probs\n",
            " tensor([[-5.3763, -1.4048]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.4929, -1.3501]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-3.4374, -0.9488]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-9.0271, -2.9732]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.2806, -4.9815]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.0251, -2.1036]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-2.6951, -0.9256]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.0047, -2.1763]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-2.9194, -0.9189]], device='cuda:0')\n",
            "emission probs\n",
            " tensor([[-1.0697, -1.9716]], device='cuda:0')\n",
            "alpha\n",
            " tensor([[[ -6.0828,  -2.0848],\n",
            "         [ -5.5228,  -3.5680],\n",
            "         [ -8.4743,  -4.6095],\n",
            "         [-15.5688,  -7.7149],\n",
            "         [-11.0337, -12.8356],\n",
            "         [-12.3748, -13.9677],\n",
            "         [-15.3790, -14.0570],\n",
            "         [-16.2054, -16.2867],\n",
            "         [-19.3141, -17.0341],\n",
            "         [-19.7015, -19.1111]]], device='cuda:0', grad_fn=<CopySlices>)\n",
            "log_sums\n",
            " tensor([[ -2.0666,  -3.4356,  -4.5888,  -7.7145, -10.8810, -12.1897, -13.8206,\n",
            "         -15.5521, -16.9367, -18.6702]], device='cuda:0',\n",
            "       grad_fn=<LogsumexpBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-18.6702]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H1tpPT7DpKO",
        "outputId": "7bdd6fd2-5254-4760-b6fd-7e1d581b7e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 0, 0, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InF6PJVOfHwH"
      },
      "source": [
        "The two scores are the same! That's because in this instance there is only one possible path through the HMM, so the probability of the most likely path is the same as the sum of the probabilities of all possible paths.\n",
        "\n",
        "In general, though, the forward score and Viterbi score will always be somewhat close. This is because of a property of the $\\text{logsumexp}$ function: $\\text{logsumexp}(\\mathbf{x}) \\approx \\max (\\mathbf{x})$. ($\\text{logsumexp}$ is sometimes referred to as the \"smooth maximum\" function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x__70tB6gnkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c0a543-0770-4958-f84c-2de18b52f9ed"
      },
      "source": [
        "x = torch.tensor([1., 2., 3.])\n",
        "print(x.max(dim=0)[0])\n",
        "print(x.logsumexp(dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n",
            "tensor(3.4076)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvFtiWhzgy0V"
      },
      "source": [
        "### Problem 3: How do we train the model?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3JaykRalSBZ"
      },
      "source": [
        "Earlier, we hard-coded an HMM to have certain behavior. What we would like to do instead is have the HMM learn to model the data on its own. And while it is possible to use supervised learning with an HMM (by hard-coding the emission model or the transition model) so that the states have a particular interpretation, the really cool thing about HMMs is that they are naturally unsupervised learners, so they can learn to use their different states to represent different patterns in the data, without the programmer needing to indicate what each state means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K471fT4N-PR"
      },
      "source": [
        "Like many machine learning models, an HMM can be trained using maximum likelihood estimation, i.e.:\n",
        "\n",
        "$$\\theta^* = \\underset{\\theta}{\\text{argmin }} -\\sum_{\\mathbf{x}^i}\\text{log }p_{\\theta}(\\mathbf{x}^i)$$\n",
        "\n",
        "where $\\mathbf{x}^1, \\mathbf{x}^2, \\dots$ are training examples. \n",
        "\n",
        "The standard method for doing this is the Expectation-Maximization (EM) algorithm, which for HMMs is also called the \"Baum-Welch\" algorithm. In EM training, we alternate between an \"E-step\", where we estimate the values of the latent variables, and an \"M-step\", where the model parameters are updated given the estimated latent variables. (Think $k$-means, where you guess which cluster each data point belongs to, then reestimate where the clusters are, and repeat.) The EM algorithm has some nice properties: it is guaranteed at each step to decrease the loss function, and the E-step and M-step may have an exact closed form solution, in which case no pesky learning rates are required.\n",
        "\n",
        "But because the HMM forward algorithm is differentiable with respect to all the model parameters, we can also just take advantage of automatic differentiation methods in libraries like PyTorch and try to minimize $-\\text{log }p_{\\theta}(\\mathbf{x})$ directly, by backpropagating through the forward algorithm and running stochastic gradient descent. That means we don't need to write any additional HMM code to implement training: `loss.backward()` is all you need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVh0-369qZDC"
      },
      "source": [
        "Here we will implement SGD training for an HMM in PyTorch. First, some helper classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqiFobGHwdzc"
      },
      "source": [
        "import torch.utils.data\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, lines):\n",
        "    self.lines = lines # list of strings\n",
        "    collate = Collate() # function for generating a minibatch from strings\n",
        "    self.loader = torch.utils.data.DataLoader(self, batch_size=1024, num_workers=1, shuffle=True, collate_fn=collate)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lines)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    line = self.lines[idx].lstrip(\" \").rstrip(\"\\n\").rstrip(\" \").rstrip(\"\\n\")\n",
        "    return line\n",
        "\n",
        "class Collate:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    \"\"\"\n",
        "    Returns a minibatch of strings, padded to have the same length.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    batch_size = len(batch)\n",
        "    for index in range(batch_size):\n",
        "      x_ = batch[index]\n",
        "\n",
        "      # convert letters to integers\n",
        "      x.append(encode(x_))\n",
        "\n",
        "    # pad all sequences with 0 to have same length\n",
        "    x_lengths = [len(x_) for x_ in x]\n",
        "    T = max(x_lengths)\n",
        "    for index in range(batch_size):\n",
        "      x[index] += [0] * (T - len(x[index]))\n",
        "      x[index] = torch.tensor(x[index])\n",
        "\n",
        "    # stack into single tensor\n",
        "    x = torch.stack(x)\n",
        "    x_lengths = torch.tensor(x_lengths)\n",
        "    return (x,x_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpDpwnPnAEA9"
      },
      "source": [
        "Let's load some training/testing data. By default, this will use the unix \"words\" file, but you could also use your own text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52NqFHg8ANsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d367cf-3e66-472b-d8de-0623d986f28d"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/lorenlugosch/pytorch_HMM/master/data/train/training.txt\n",
        "\n",
        "filename = \"training.txt\"\n",
        "\n",
        "with open(filename, \"r\") as f:\n",
        "  lines = f.readlines() # each line of lines will have one word\n",
        "\n",
        "alphabet = list(Counter((\"\".join(lines))).keys())\n",
        "train_lines, valid_lines = train_test_split(lines, test_size=0.1, random_state=42)\n",
        "train_dataset = TextDataset(train_lines)\n",
        "valid_dataset = TextDataset(valid_lines)\n",
        "\n",
        "M = len(alphabet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 08:57:32--  https://raw.githubusercontent.com/lorenlugosch/pytorch_HMM/master/data/train/training.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2493109 (2.4M) [text/plain]\n",
            "Saving to: ‘training.txt’\n",
            "\n",
            "training.txt        100%[===================>]   2.38M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-06-12 08:57:33 (70.1 MB/s) - ‘training.txt’ saved [2493109/2493109]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0AqmyrK7IUn"
      },
      "source": [
        "We will use a Trainer class for training and testing the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iypy_neX9cpq"
      },
      "source": [
        "from tqdm import tqdm # for displaying progress bar\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, lr):\n",
        "    self.model = model\n",
        "    self.lr = lr\n",
        "    self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=0.00001)\n",
        "  \n",
        "  def train(self, dataset):\n",
        "    train_loss = 0\n",
        "    num_samples = 0\n",
        "    self.model.train()\n",
        "    print_interval = 50\n",
        "    for idx, batch in enumerate(tqdm(dataset.loader)):\n",
        "      x,T = batch\n",
        "      batch_size = len(x)\n",
        "      num_samples += batch_size\n",
        "      log_probs = self.model(x,T)\n",
        "      loss = -log_probs.mean()\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      train_loss += loss.cpu().data.numpy().item() * batch_size\n",
        "      if idx % print_interval == 0:\n",
        "        print(\"loss:\", loss.item())\n",
        "        for _ in range(5):\n",
        "          sampled_x, sampled_z = self.model.sample()\n",
        "          print(decode(sampled_x))\n",
        "          print(sampled_z)\n",
        "    train_loss /= num_samples\n",
        "    return train_loss\n",
        "\n",
        "  def test(self, dataset):\n",
        "    test_loss = 0\n",
        "    num_samples = 0\n",
        "    self.model.eval()\n",
        "    print_interval = 50\n",
        "    for idx, batch in enumerate(dataset.loader):\n",
        "      x,T = batch\n",
        "      batch_size = len(x)\n",
        "      num_samples += batch_size\n",
        "      log_probs = self.model(x,T)\n",
        "      loss = -log_probs.mean()\n",
        "      test_loss += loss.cpu().data.numpy().item() * batch_size\n",
        "      if idx % print_interval == 0:\n",
        "        print(\"loss:\", loss.item())\n",
        "        sampled_x, sampled_z = self.model.sample()\n",
        "        print(decode(sampled_x))\n",
        "        print(sampled_z)\n",
        "    test_loss /= num_samples\n",
        "    return test_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUR8qbHm9dMg"
      },
      "source": [
        "Finally, initialize the model and run the main training loop. Every 50 batches, the code will produce a few samples from the model. Over time, these samples should look more and more realistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-NGIK1Q9g2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53092fb7-c32d-4981-c124-f3a1dd5a386a"
      },
      "source": [
        "# Initialize model\n",
        "model = HMM(N=64, M=M)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "trainer = Trainer(model, lr=0.01)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "        print(\"========= Epoch %d of %d =========\" % (epoch+1, num_epochs))\n",
        "        train_loss = trainer.train(train_dataset)\n",
        "        valid_loss = trainer.test(valid_dataset)\n",
        "\n",
        "        print(\"========= Results: epoch %d of %d =========\" % (epoch+1, num_epochs))\n",
        "        print(\"train loss: %.2f| valid loss: %.2f\\n\" % (train_loss, valid_loss) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= Epoch 1 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 38.220359802246094\n",
            "ovS\n",
            "BqfdTS\n",
            "[4, 17, 26, 24, 49, 27, 29, 47, 12, 47]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 3/208 [00:00<00:23,  8.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SzhiieL\n",
            "tZ\n",
            "[32, 36, 55, 47, 35, 57, 14, 60, 59, 38]\n",
            "-OVlKNufaM\n",
            "[60, 18, 60, 50, 32, 57, 20, 51, 33, 58]\n",
            "mJkSOib\n",
            "Kw\n",
            "[18, 39, 60, 17, 38, 55, 51, 47, 32, 50]\n",
            "vsIWdPnBnA\n",
            "[0, 8, 56, 27, 17, 23, 47, 39, 58, 57]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:10, 14.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 32.680179595947266\n",
            "gdiaEscYSs\n",
            "[32, 30, 50, 14, 48, 43, 20, 42, 26, 29]\n",
            "iUvMlHiiwo\n",
            "[29, 43, 30, 14, 48, 3, 46, 59, 29, 40]\n",
            "SfatoBrGIK\n",
            "[20, 53, 2, 46, 2, 57, 41, 11, 40, 24]\n",
            "mMilVb\n",
            "Yxx\n",
            "[44, 19, 40, 40, 60, 27, 58, 50, 0, 13]\n",
            "sUhYqrlIMx\n",
            "[33, 27, 52, 54, 39, 41, 48, 63, 22, 38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:07<00:07, 13.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 30.349931716918945\n",
            "pRrsinkell\n",
            "[50, 62, 12, 45, 62, 22, 58, 38, 59, 15]\n",
            "ihLhnbisud\n",
            "[6, 54, 51, 16, 31, 36, 54, 34, 16, 39]\n",
            "oraonioiqs\n",
            "[0, 41, 17, 52, 0, 59, 63, 22, 0, 14]\n",
            "juciweufcn\n",
            "[44, 62, 49, 1, 50, 0, 43, 47, 35, 20]\n",
            "sicfadnxrd\n",
            "[44, 19, 49, 49, 1, 46, 22, 48, 3, 38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:10<00:03, 15.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 28.456218719482422\n",
            "viycelurTb\n",
            "[44, 62, 12, 14, 45, 16, 14, 41, 63, 39]\n",
            "oiomomyorb\n",
            "[18, 62, 24, 45, 14, 45, 15, 24, 41, 39]\n",
            "ecSlqvFnag\n",
            "[44, 49, 31, 59, 13, 51, 15, 22, 63, 53]\n",
            "hrsoniedil\n",
            "[18, 12, 31, 24, 0, 47, 12, 39, 62, 22]\n",
            "penfniTlru\n",
            "[51, 54, 63, 33, 22, 50, 49, 31, 37, 15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 14.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.394794464111328\n",
            "dascidomyH\n",
            "[34, 14, 5, 46, 62, 34, 24, 45, 24, 45]\n",
            "tFshXdrduV\n",
            "[44, 15, 22, 31, 54, 34, 54, 34, 40, 57]\n",
            "twitcgoesr\n",
            "[44, 21, 43, 12, 5, 58, 31, 54, 5, 39]\n",
            "cudu\n",
            "ltilr\n",
            "[44, 62, 55, 14, 5, 9, 46, 62, 44, 41]\n",
            "ramsVfroag\n",
            "[18, 24, 45, 24, 45, 24, 31, 62, 35, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:14<00:00, 14.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.797958374023438\n",
            "bessrinive\n",
            "[44, 63, 22, 49, 31, 62, 44, 62, 53, 54]\n",
            "========= Results: epoch 1 of 10 =========\n",
            "train loss: 30.79| valid loss: 26.61\n",
            "\n",
            "========= Epoch 2 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:40,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.685302734375\n",
            "Marusfliih\n",
            "[44, 63, 41, 62, 35, 63, 39, 59, 63, 46]\n",
            "ateemmcala\n",
            "[14, 0, 54, 24, 45, 62, 35, 63, 39, 63]\n",
            "ufKsllpXti\n",
            "[14, 45, 62, 46, 53, 0, 26, 62, 46, 62]\n",
            "eOmogoates\n",
            "[33, 14, 45, 24, 35, 20, 49, 31, 54, 5]\n",
            "-lianhertr\n",
            "[18, 39, 59, 63, 22, 31, 54, 22, 49, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:10, 15.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.105327606201172\n",
            "rietreeguc\n",
            "[41, 62, 49, 31, 41, 62, 63, 35, 62, 35]\n",
            "rinnibuDrE\n",
            "[41, 62, 5, 45, 62, 44, 14, 1, 41, 62]\n",
            "srarcronha\n",
            "[44, 41, 63, 22, 35, 41, 63, 22, 50, 63]\n",
            "hotingifal\n",
            "[18, 63, 31, 62, 22, 38, 62, 34, 63, 39]\n",
            "pIbcantres\n",
            "[16, 15, 22, 35, 63, 22, 31, 41, 63, 46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:06<00:07, 14.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 25.067440032958984\n",
            "radatepnty\n",
            "[41, 63, 39, 63, 31, 62, 46, 22, 31, 62]\n",
            "iasiaslial\n",
            "[62, 63, 45, 62, 63, 46, 6, 54, 63, 53]\n",
            "ancigeroab\n",
            "[63, 22, 31, 62, 35, 54, 41, 24, 14, 39]\n",
            "pedigaceXa\n",
            "[16, 54, 34, 62, 35, 49, 35, 63, 34, 63]\n",
            "socuschewe\n",
            "[48, 63, 35, 14, 45, 46, 50, 63, 39, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:10<00:03, 13.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.631916046142578\n",
            "giTmmcroov\n",
            "[18, 63, 61, 45, 45, 35, 41, 24, 47, 34]\n",
            "ultoupipia\n",
            "[15, 22, 31, 54, 5, 45, 62, 39, 62, 63]\n",
            "macencrepl\n",
            "[18, 63, 38, 63, 22, 35, 17, 12, 16, 6]\n",
            "phocarnoer\n",
            "[16, 6, 63, 22, 63, 22, 38, 6, 54, 41]\n",
            "racotlipre\n",
            "[41, 63, 35, 63, 31, 41, 62, 35, 41, 54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 14.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.934513092041016\n",
            "sireinapet\n",
            "[44, 15, 22, 38, 62, 22, 24, 45, 49, 31]\n",
            "unocismomr\n",
            "[15, 22, 63, 46, 62, 46, 50, 24, 45, 17]\n",
            "udsliglant\n",
            "[15, 22, 44, 6, 62, 35, 6, 63, 22, 31]\n",
            "tribineaai\n",
            "[33, 41, 62, 39, 62, 31, 54, 5, 43, 62]\n",
            "dumaPtesen\n",
            "[32, 14, 45, 63, 46, 31, 54, 45, 62, 22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:13<00:00, 14.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.86551284790039\n",
            "couchide-u\n",
            "[35, 63, 14, 46, 50, 62, 34, 54, 5, 52]\n",
            "========= Results: epoch 2 of 10 =========\n",
            "train loss: 25.27| valid loss: 24.60\n",
            "\n",
            "========= Epoch 3 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.48318099975586\n",
            "Anunbofich\n",
            "[21, 0, 15, 22, 38, 63, 22, 62, 46, 50]\n",
            "broteeyoca\n",
            "[26, 41, 63, 31, 59, 49, 25, 24, 35, 63]\n",
            "pmflousiim\n",
            "[18, 36, 44, 6, 24, 14, 46, 50, 62, 31]\n",
            "pqygiaRica\n",
            "[26, 6, 28, 38, 59, 63, 22, 62, 35, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 3/208 [00:00<00:20, 10.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "viotederee\n",
            "[34, 62, 63, 31, 62, 34, 54, 41, 19, 49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 51/208 [00:04<00:15, 10.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.424537658691406\n",
            "cpanUidado\n",
            "[46, 31, 63, 22, 24, 47, 34, 63, 38, 54]\n",
            "rilitrodin\n",
            "[41, 63, 39, 62, 31, 41, 63, 34, 62, 35]\n",
            "croncKrous\n",
            "[35, 41, 63, 22, 35, 54, 41, 24, 14, 46]\n",
            "onlersnong\n",
            "[63, 22, 31, 54, 5, 44, 6, 63, 22, 38]\n",
            "calosmianD\n",
            "[18, 63, 39, 63, 5, 45, 62, 63, 22, 49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:07<00:07, 14.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.104280471801758\n",
            "Qandeblver\n",
            "[18, 63, 22, 38, 63, 39, 6, 0, 54, 41]\n",
            "ssheruoian\n",
            "[46, 31, 50, 54, 41, 14, 31, 62, 63, 45]\n",
            "inuuisiote\n",
            "[15, 22, 24, 17, 62, 45, 62, 63, 31, 62]\n",
            "tlDritrito\n",
            "[26, 6, 1, 41, 62, 31, 41, 62, 31, 24]\n",
            "priprighei\n",
            "[26, 41, 62, 35, 41, 62, 35, 50, 62, 46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:11<00:03, 14.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.04503631591797\n",
            "Dystrinlem\n",
            "[18, 1, 46, 31, 41, 62, 0, 43, 1, 45]\n",
            "anurateaef\n",
            "[63, 22, 62, 34, 63, 31, 54, 17, 62, 2]\n",
            "sentialcce\n",
            "[44, 63, 22, 31, 62, 63, 39, 49, 31, 62]\n",
            "cholyemall\n",
            "[35, 50, 63, 39, 62, 63, 45, 63, 39, 6]\n",
            "cortailexl\n",
            "[35, 63, 22, 31, 63, 39, 6, 62, 39, 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:14<00:00, 13.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.293285369873047\n",
            "allewovong\n",
            "[63, 39, 6, 54, 41, 63, 34, 63, 22, 38]\n",
            "wanvorNima\n",
            "[18, 63, 22, 34, 54, 5, 44, 62, 45, 63]\n",
            "phiturride\n",
            "[26, 50, 62, 31, 54, 5, 41, 62, 34, 54]\n",
            "mastideoch\n",
            "[18, 63, 22, 31, 62, 34, 62, 63, 35, 50]\n",
            "ffionadept\n",
            "[60, 2, 62, 63, 34, 63, 22, 62, 22, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:14<00:00, 13.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.761920928955078\n",
            "ofeadogidt\n",
            "[47, 34, 54, 63, 39, 24, 32, 62, 34, 54]\n",
            "========= Results: epoch 3 of 10 =========\n",
            "train loss: 24.22| valid loss: 24.04\n",
            "\n",
            "========= Epoch 4 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:38,  5.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.110631942749023\n",
            "haditonlyd\n",
            "[18, 63, 34, 62, 31, 63, 39, 6, 28, 38]\n",
            "thounulahe\n",
            "[26, 50, 24, 14, 31, 63, 39, 49, 31, 62]\n",
            "pritiotere\n",
            "[26, 41, 62, 31, 62, 63, 31, 54, 41, 62]\n",
            "lireormoge\n",
            "[34, 54, 41, 62, 63, 5, 45, 63, 39, 62]\n",
            "abolicapop\n",
            "[63, 39, 63, 39, 62, 35, 63, 39, 24, 16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:10, 14.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.95294189453125\n",
            "divabtiala\n",
            "[34, 62, 34, 63, 39, 31, 62, 63, 39, 49]\n",
            "nanelnevcu\n",
            "[18, 63, 22, 63, 39, 34, 62, 46, 35, 14]\n",
            "sidiUmalel\n",
            "[44, 62, 34, 62, 63, 45, 63, 39, 62, 39]\n",
            "nalitickmp\n",
            "[18, 63, 39, 62, 31, 62, 35, 19, 45, 16]\n",
            "bousistore\n",
            "[18, 24, 14, 46, 62, 46, 31, 54, 41, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:06<00:07, 13.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.891273498535156\n",
            "phtflatial\n",
            "[26, 50, 31, 2, 6, 49, 31, 62, 63, 39]\n",
            "oncreliaph\n",
            "[63, 22, 35, 41, 63, 39, 62, 24, 16, 50]\n",
            "Thotwusheu\n",
            "[26, 50, 63, 31, 37, 62, 46, 50, 63, 22]\n",
            "tysmizusSr\n",
            "[31, 54, 5, 45, 62, 35, 14, 46, 26, 41]\n",
            "scelathyme\n",
            "[44, 35, 63, 39, 49, 31, 50, 28, 45, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 151/208 [00:10<00:04, 12.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.020217895507812\n",
            "Runtearent\n",
            "[18, 15, 22, 31, 54, 49, 41, 63, 22, 38]\n",
            "eundanevil\n",
            "[51, 15, 22, 38, 63, 34, 62, 34, 63, 39]\n",
            "outthredie\n",
            "[24, 14, 46, 31, 50, 41, 62, 34, 62, 63]\n",
            "rurdisnern\n",
            "[41, 14, 39, 34, 62, 46, 0, 54, 5, 0]\n",
            "Mativavnfe\n",
            "[18, 63, 31, 62, 34, 63, 34, 0, 2, 54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 13.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.61150360107422\n",
            "kuoprymoku\n",
            "[34, 54, 24, 16, 41, 62, 34, 63, 31, 14]\n",
            "Atollesaro\n",
            "[21, 31, 63, 39, 6, 54, 45, 49, 41, 24]\n",
            "covoKolype\n",
            "[35, 63, 34, 63, 39, 24, 32, 28, 16, 54]\n",
            "teroninwys\n",
            "[18, 54, 41, 24, 45, 62, 22, 31, 28, 46]\n",
            "ghoussthor\n",
            "[26, 50, 24, 14, 5, 46, 31, 50, 54, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:14<00:00, 14.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.6522216796875\n",
            "vedgydrero\n",
            "[34, 54, 5, 32, 28, 29, 41, 54, 41, 24]\n",
            "========= Results: epoch 4 of 10 =========\n",
            "train loss: 23.89| valid loss: 23.85\n",
            "\n",
            "========= Epoch 5 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:38,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.433408737182617\n",
            "borsumphal\n",
            "[18, 63, 5, 43, 14, 45, 16, 50, 63, 22]\n",
            "hancorhate\n",
            "[18, 63, 22, 35, 63, 31, 50, 63, 46, 62]\n",
            "bastymangi\n",
            "[48, 49, 46, 31, 28, 45, 63, 22, 38, 62]\n",
            "barnonsour\n",
            "[18, 63, 22, 34, 63, 22, 31, 24, 14, 41]\n",
            "tistJuildo\n",
            "[27, 62, 46, 31, 41, 20, 62, 39, 55, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:10, 15.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.753307342529297\n",
            "umayistedy\n",
            "[15, 45, 49, 37, 62, 46, 31, 54, 38, 28]\n",
            "poloteatyp\n",
            "[18, 63, 39, 63, 31, 62, 46, 31, 28, 16]\n",
            "inralearir\n",
            "[15, 22, 41, 63, 39, 62, 63, 22, 62, 46]\n",
            "rasciveopy\n",
            "[18, 63, 46, 31, 62, 34, 62, 24, 16, 28]\n",
            "Maumeniabr\n",
            "[18, 63, 15, 45, 63, 22, 62, 63, 48, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:07<00:07, 13.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.89297866821289\n",
            "ochigousta\n",
            "[63, 35, 50, 62, 53, 24, 14, 46, 31, 63]\n",
            "Aspresledn\n",
            "[21, 44, 16, 41, 54, 5, 6, 54, 29, 0]\n",
            "vursanuido\n",
            "[34, 54, 5, 44, 63, 31, 20, 62, 34, 63]\n",
            "subemohrea\n",
            "[44, 20, 10, 54, 45, 63, 50, 41, 62, 63]\n",
            "sulogryort\n",
            "[44, 20, 39, 24, 32, 41, 62, 63, 22, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:10<00:03, 14.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.550996780395508\n",
            "ceustibral\n",
            "[35, 63, 14, 46, 31, 62, 48, 41, 63, 39]\n",
            "bortabastu\n",
            "[48, 63, 22, 31, 63, 48, 63, 46, 31, 12]\n",
            "elfucthteo\n",
            "[63, 39, 3, 14, 46, 31, 50, 31, 62, 63]\n",
            "inendesmic\n",
            "[15, 22, 62, 22, 34, 54, 5, 45, 62, 35]\n",
            "alanepyler\n",
            "[63, 39, 63, 34, 63, 16, 28, 6, 54, 5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 14.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.570589065551758\n",
            "wyencrivil\n",
            "[18, 61, 63, 22, 35, 41, 62, 34, 62, 6]\n",
            "psedmegdsi\n",
            "[26, 42, 54, 5, 45, 62, 53, 56, 30, 62]\n",
            "untesIidan\n",
            "[15, 22, 31, 54, 5, 18, 62, 34, 63, 22]\n",
            "pofficalle\n",
            "[18, 63, 11, 2, 62, 35, 63, 39, 6, 54]\n",
            "proorinaom\n",
            "[26, 41, 24, 47, 34, 62, 22, 63, 1, 45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:13<00:00, 14.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.552005767822266\n",
            "weckraisne\n",
            "[41, 62, 35, 19, 41, 63, 62, 46, 0, 54]\n",
            "========= Results: epoch 5 of 10 =========\n",
            "train loss: 23.75| valid loss: 23.74\n",
            "\n",
            "========= Epoch 6 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:38,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.48934555053711\n",
            "dioserespe\n",
            "[27, 62, 24, 42, 54, 41, 62, 46, 31, 54]\n",
            "rospkitest\n",
            "[41, 24, 44, 16, 31, 62, 31, 62, 46, 31]\n",
            "tiitaliDet\n",
            "[26, 50, 62, 35, 63, 39, 62, 34, 62, 31]\n",
            "cruamosril\n",
            "[26, 41, 20, 63, 45, 63, 31, 41, 62, 39]\n",
            "toledetrim\n",
            "[26, 63, 39, 62, 34, 49, 31, 41, 62, 45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:10, 14.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.70694351196289\n",
            "aptupeedis\n",
            "[63, 16, 31, 20, 10, 0, 54, 56, 62, 46]\n",
            "otretionem\n",
            "[63, 31, 41, 62, 31, 62, 63, 22, 1, 45]\n",
            "dremoustis\n",
            "[26, 41, 1, 45, 24, 14, 46, 31, 62, 46]\n",
            "Isleropham\n",
            "[21, 46, 6, 54, 41, 24, 16, 6, 12, 45]\n",
            "relsesdicu\n",
            "[41, 62, 39, 42, 54, 5, 34, 62, 35, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:06<00:07, 14.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.88528823852539\n",
            "ingifelasi\n",
            "[15, 22, 38, 62, 2, 54, 6, 63, 46, 62]\n",
            "soerealise\n",
            "[18, 63, 54, 22, 31, 63, 39, 62, 46, 54]\n",
            "Tinchiollo\n",
            "[26, 15, 22, 35, 50, 62, 63, 39, 6, 24]\n",
            "phelluletr\n",
            "[26, 50, 62, 39, 6, 20, 39, 62, 31, 41]\n",
            "dirarraust\n",
            "[27, 62, 34, 63, 5, 41, 63, 14, 46, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:10<00:03, 14.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.371965408325195\n",
            "ennineacle\n",
            "[15, 22, 34, 62, 22, 1, 49, 46, 6, 62]\n",
            "rachersane\n",
            "[18, 63, 35, 50, 54, 5, 44, 63, 34, 54]\n",
            "untidatoph\n",
            "[15, 22, 31, 62, 34, 49, 31, 63, 16, 50]\n",
            "imalyshins\n",
            "[15, 45, 49, 17, 62, 46, 50, 62, 22, 46]\n",
            "tretreicac\n",
            "[26, 41, 62, 31, 41, 1, 62, 35, 63, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 14.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.569175720214844\n",
            "whraunxier\n",
            "[26, 50, 41, 63, 15, 22, 45, 62, 63, 5]\n",
            "untmuskiva\n",
            "[15, 22, 31, 24, 14, 46, 31, 62, 34, 49]\n",
            "nycatelyla\n",
            "[18, 28, 35, 63, 31, 54, 6, 28, 39, 49]\n",
            "frassaster\n",
            "[26, 41, 63, 5, 44, 1, 46, 31, 54, 5]\n",
            "Sitiausklo\n",
            "[18, 62, 31, 62, 63, 14, 46, 31, 6, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:13<00:00, 14.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.102989196777344\n",
            "hanalobNus\n",
            "[18, 63, 22, 63, 39, 63, 10, 9, 14, 46]\n",
            "========= Results: epoch 6 of 10 =========\n",
            "train loss: 23.65| valid loss: 23.65\n",
            "\n",
            "========= Epoch 7 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:39,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.449460983276367\n",
            "anroidienr\n",
            "[15, 22, 41, 24, 47, 56, 62, 63, 22, 41]\n",
            "privesmyti\n",
            "[26, 41, 62, 34, 54, 5, 45, 62, 31, 62]\n",
            "biphopterp\n",
            "[48, 62, 16, 50, 63, 16, 31, 54, 5, 16]\n",
            "aridurwhad\n",
            "[63, 39, 62, 34, 54, 5, 43, 50, 63, 34]\n",
            "paterbalal\n",
            "[26, 63, 31, 54, 5, 25, 63, 39, 63, 39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:11, 13.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.892345428466797\n",
            "tripessuti\n",
            "[26, 41, 1, 16, 54, 5, 44, 14, 31, 62]\n",
            "seagesichr\n",
            "[44, 1, 49, 38, 54, 5, 62, 35, 50, 41]\n",
            "Cssomatrei\n",
            "[63, 5, 44, 24, 45, 49, 31, 41, 1, 62]\n",
            "messiayyio\n",
            "[18, 1, 46, 31, 62, 63, 23, 36, 59, 12]\n",
            "eperivossc\n",
            "[1, 51, 54, 5, 62, 34, 54, 5, 46, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:07<00:07, 14.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.258758544921875\n",
            "psereenusm\n",
            "[26, 42, 54, 41, 1, 63, 22, 24, 46, 45]\n",
            "peatolerst\n",
            "[18, 1, 63, 31, 54, 6, 54, 5, 46, 31]\n",
            "phaloulous\n",
            "[26, 50, 63, 39, 24, 14, 39, 24, 14, 46]\n",
            "suilineshi\n",
            "[44, 20, 62, 39, 62, 22, 62, 46, 50, 62]\n",
            "acksthalil\n",
            "[63, 35, 19, 46, 31, 50, 63, 39, 62, 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:10<00:04, 13.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.47913932800293\n",
            "ingcreessl\n",
            "[15, 22, 38, 54, 41, 1, 54, 5, 44, 6]\n",
            "ormphypler\n",
            "[63, 5, 45, 16, 50, 28, 16, 6, 54, 5]\n",
            "ivicogypus\n",
            "[47, 34, 62, 35, 24, 32, 28, 16, 14, 46]\n",
            "susridiner\n",
            "[44, 20, 5, 41, 62, 34, 62, 34, 54, 5]\n",
            "stridolric\n",
            "[46, 31, 41, 62, 56, 63, 39, 41, 62, 35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 14.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.940227508544922\n",
            "liristodit\n",
            "[18, 62, 39, 62, 46, 31, 63, 39, 62, 31]\n",
            "trouscless\n",
            "[26, 41, 24, 14, 46, 35, 6, 54, 5, 44]\n",
            "Uutympangl\n",
            "[21, 14, 31, 12, 45, 16, 63, 22, 38, 6]\n",
            "bigalpKync\n",
            "[44, 62, 53, 63, 39, 16, 50, 62, 22, 35]\n",
            "q-erseguia\n",
            "[44, 6, 54, 5, 44, 1, 53, 20, 62, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:14<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.485172271728516\n",
            "bilouleble\n",
            "[48, 62, 39, 24, 14, 39, 62, 48, 6, 54]\n",
            "========= Results: epoch 7 of 10 =========\n",
            "train loss: 23.55| valid loss: 23.55\n",
            "\n",
            "========= Epoch 8 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.490684509277344\n",
            "rivistevio\n",
            "[17, 62, 34, 62, 46, 31, 62, 34, 62, 63]\n",
            "proosteoce\n",
            "[26, 41, 24, 14, 46, 31, 62, 63, 35, 63]\n",
            "pantatesti\n",
            "[18, 63, 22, 31, 49, 31, 63, 46, 31, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 3/208 [00:00<00:20,  9.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deywhonbuc\n",
            "[38, 54, 7, 43, 50, 63, 22, 51, 14, 46]\n",
            "enentlembr\n",
            "[21, 34, 54, 22, 31, 6, 54, 45, 51, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:10, 14.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.670957565307617\n",
            "blivobersh\n",
            "[48, 6, 62, 34, 54, 10, 54, 5, 43, 50]\n",
            "tryoullear\n",
            "[26, 41, 28, 24, 14, 39, 6, 1, 63, 5]\n",
            "Trocionerc\n",
            "[26, 41, 1, 35, 62, 24, 0, 54, 5, 31]\n",
            "Pohimurgss\n",
            "[26, 63, 50, 62, 45, 14, 22, 38, 46, 31]\n",
            "ragharsedl\n",
            "[41, 49, 38, 50, 63, 5, 45, 54, 56, 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:07<00:07, 14.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.539636611938477\n",
            "sednatkess\n",
            "[46, 54, 29, 0, 24, 35, 19, 54, 5, 45]\n",
            "vilictaaid\n",
            "[34, 62, 17, 62, 46, 31, 49, 17, 62, 56]\n",
            "isterricab\n",
            "[21, 46, 31, 54, 5, 41, 62, 35, 63, 4]\n",
            "Crapeloror\n",
            "[26, 41, 1, 18, 63, 39, 24, 41, 24, 41]\n",
            "sangsumosi\n",
            "[44, 63, 22, 38, 46, 12, 45, 24, 42, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:10<00:03, 14.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.555219650268555\n",
            "odcancuram\n",
            "[47, 56, 35, 63, 22, 31, 54, 41, 62, 45]\n",
            "sthistonio\n",
            "[46, 31, 50, 62, 46, 31, 63, 37, 62, 63]\n",
            "cochoginip\n",
            "[35, 24, 35, 50, 24, 32, 62, 22, 1, 16]\n",
            "stethority\n",
            "[46, 31, 62, 31, 50, 63, 5, 62, 31, 23]\n",
            "luperseuth\n",
            "[6, 1, 16, 54, 5, 42, 54, 5, 46, 50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 14.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.617158889770508\n",
            "peniollala\n",
            "[26, 63, 22, 62, 63, 39, 6, 63, 39, 1]\n",
            "medlsousol\n",
            "[18, 1, 56, 59, 42, 24, 14, 46, 63, 39]\n",
            "acanguisti\n",
            "[21, 35, 63, 22, 38, 20, 62, 46, 31, 62]\n",
            "grartiaxsu\n",
            "[26, 41, 63, 5, 31, 62, 63, 22, 44, 20]\n",
            "Jonissluda\n",
            "[18, 24, 37, 62, 46, 44, 6, 12, 56, 59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:14<00:00, 14.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.595508575439453\n",
            "Flintroplo\n",
            "[26, 6, 62, 22, 31, 41, 24, 16, 6, 24]\n",
            "========= Results: epoch 8 of 10 =========\n",
            "train loss: 23.46| valid loss: 23.47\n",
            "\n",
            "========= Epoch 9 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.8101749420166\n",
            "ecosterioE\n",
            "[21, 35, 63, 46, 31, 54, 41, 62, 63, 15]\n",
            "tantedrahv\n",
            "[26, 63, 22, 31, 49, 29, 41, 63, 39, 34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 3/208 [00:00<00:21,  9.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oxhichocyl\n",
            "[15, 22, 50, 62, 35, 50, 24, 35, 28, 39]\n",
            "cksugricom\n",
            "[35, 19, 44, 20, 53, 41, 1, 35, 24, 45]\n",
            "braposomer\n",
            "[26, 41, 1, 16, 24, 44, 24, 45, 54, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:10, 14.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.47339630126953\n",
            "Momanwlvea\n",
            "[18, 24, 45, 63, 22, 25, 39, 34, 54, 49]\n",
            "honstednes\n",
            "[18, 63, 22, 46, 31, 54, 29, 0, 54, 5]\n",
            "glodronous\n",
            "[38, 6, 24, 32, 41, 63, 22, 24, 14, 42]\n",
            "centylynet\n",
            "[35, 63, 22, 31, 23, 6, 13, 37, 49, 31]\n",
            "apbrosthae\n",
            "[21, 10, 51, 41, 63, 46, 31, 41, 63, 40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:07<00:07, 14.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.555282592773438\n",
            "phaanachel\n",
            "[26, 50, 63, 15, 22, 1, 35, 50, 62, 6]\n",
            "rQoniferai\n",
            "[10, 55, 24, 45, 62, 2, 54, 41, 1, 62]\n",
            "Dousereste\n",
            "[18, 24, 14, 42, 54, 41, 1, 46, 31, 54]\n",
            "gethterede\n",
            "[18, 1, 31, 50, 31, 54, 5, 54, 56, 63]\n",
            "sphaincone\n",
            "[44, 16, 50, 63, 15, 22, 35, 63, 37, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:10<00:03, 14.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.41158103942871\n",
            "unilallyhe\n",
            "[15, 22, 14, 39, 49, 39, 6, 28, 50, 54]\n",
            "unajuiatin\n",
            "[15, 22, 1, 44, 20, 62, 49, 31, 62, 22]\n",
            "eizictatri\n",
            "[40, 47, 34, 62, 46, 31, 49, 31, 41, 1]\n",
            "nepoldicur\n",
            "[18, 1, 16, 63, 39, 56, 62, 35, 63, 22]\n",
            "Ulisiveran\n",
            "[21, 39, 62, 46, 62, 34, 54, 41, 63, 22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 14.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.691513061523438\n",
            "eblifermpc\n",
            "[21, 39, 6, 62, 34, 54, 5, 45, 16, 31]\n",
            "chronicart\n",
            "[35, 50, 41, 63, 37, 62, 35, 63, 39, 31]\n",
            "dalledyunt\n",
            "[18, 63, 39, 6, 1, 32, 28, 15, 22, 31]\n",
            "nydlycelen\n",
            "[18, 28, 29, 6, 28, 35, 63, 39, 62, 37]\n",
            "beurnarest\n",
            "[51, 63, 14, 5, 37, 49, 17, 62, 46, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:14<00:00, 14.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.183727264404297\n",
            "saisubomen\n",
            "[18, 1, 40, 44, 20, 10, 54, 45, 62, 22]\n",
            "========= Results: epoch 9 of 10 =========\n",
            "train loss: 23.39| valid loss: 23.41\n",
            "\n",
            "========= Epoch 10 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:39,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.200057983398438\n",
            "paansuezop\n",
            "[26, 63, 63, 22, 44, 20, 54, 55, 24, 16]\n",
            "pslerconst\n",
            "[26, 42, 6, 54, 5, 35, 63, 22, 46, 31]\n",
            "quatedreba\n",
            "[44, 20, 49, 31, 54, 29, 41, 1, 51, 49]\n",
            "heventners\n",
            "[18, 1, 34, 54, 22, 31, 0, 54, 5, 42]\n",
            "sysersuftr\n",
            "[18, 28, 42, 54, 5, 44, 20, 11, 31, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 53/208 [00:03<00:10, 14.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.22366714477539\n",
            "amegablami\n",
            "[21, 45, 62, 53, 49, 48, 6, 1, 45, 62]\n",
            "meterttigh\n",
            "[18, 63, 31, 54, 5, 46, 31, 62, 53, 50]\n",
            "sitesshero\n",
            "[18, 63, 31, 54, 5, 43, 50, 54, 41, 24]\n",
            "fatmicocic\n",
            "[18, 1, 46, 45, 62, 35, 63, 46, 62, 46]\n",
            "plesalalin\n",
            "[26, 6, 1, 46, 63, 39, 49, 39, 62, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 103/208 [00:07<00:07, 14.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.44876480102539\n",
            "ricobochce\n",
            "[41, 62, 35, 24, 4, 24, 35, 50, 31, 62]\n",
            "sfosetrica\n",
            "[44, 3, 14, 42, 62, 31, 17, 62, 35, 63]\n",
            "cluifynipp\n",
            "[35, 6, 20, 62, 2, 13, 22, 1, 16, 16]\n",
            "altogronir\n",
            "[21, 39, 31, 54, 32, 41, 63, 22, 62, 22]\n",
            "lutatilapi\n",
            "[18, 1, 31, 49, 31, 62, 39, 49, 31, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▎  | 153/208 [00:10<00:03, 14.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.105030059814453\n",
            "ponoangrop\n",
            "[16, 63, 22, 1, 49, 22, 38, 41, 24, 16]\n",
            "sksteriker\n",
            "[44, 19, 46, 31, 54, 41, 1, 19, 54, 5]\n",
            "paronenkut\n",
            "[26, 63, 39, 24, 45, 63, 22, 31, 14, 31]\n",
            "cassnessay\n",
            "[35, 63, 46, 42, 0, 54, 5, 42, 49, 13]\n",
            "lypicontil\n",
            "[18, 28, 16, 62, 35, 63, 22, 31, 62, 39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 203/208 [00:13<00:00, 13.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.19791603088379\n",
            "muloashopi\n",
            "[18, 14, 39, 24, 49, 46, 50, 24, 16, 62]\n",
            "didgraster\n",
            "[27, 62, 56, 38, 41, 1, 46, 31, 54, 5]\n",
            "sadynatera\n",
            "[18, 1, 56, 13, 37, 49, 31, 54, 41, 1]\n",
            "vondentaNh\n",
            "[25, 63, 22, 38, 54, 22, 31, 49, 43, 50]\n",
            "Gadatitcyc\n",
            "[18, 1, 56, 63, 31, 62, 46, 35, 28, 35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:13<00:00, 14.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.30792999267578\n",
            "unkerslomi\n",
            "[15, 22, 34, 54, 5, 42, 6, 1, 45, 62]\n",
            "========= Results: epoch 10 of 10 =========\n",
            "train loss: 23.35| valid loss: 23.37\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zymBj9QrDHRM"
      },
      "source": [
        "You may wish to try different values of $N$ and see what the impact on sample quality is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auBibYUTtIom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c411c4e-b0a0-48de-fc60-b55f1038e7e5"
      },
      "source": [
        "x = torch.tensor(encode(\"quack\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T))\n",
        "\n",
        "x = torch.tensor(encode(\"quick\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T))\n",
        "\n",
        "x = torch.tensor(encode(\"qurck\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T)) # should have lower probability---in English only vowels follow \"qu\"\n",
        "\n",
        "x = torch.tensor(encode(\"qiick\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T)) # should have lower probability---in English only \"u\" follows \"q\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([[44, 20, 49, 35, 19]], tensor([[-15.1298]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[44, 20, 62, 35, 19]], tensor([[-12.7714]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[44, 20, 10, 35, 19]], tensor([[-14.3361]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[44, 1, 62, 35, 19]], tensor([[-18.7059]], device='cuda:0', grad_fn=<GatherBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eZeQXWjhDev"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "HMMs used to be very popular in natural language processing, but they have largely been overshadowed by neural network models like RNNs and Transformers. Still, it is fun and instructive to study the HMM; some commonly used machine learning techniques like [Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf) are inspired by HMM methods. HMMs are [still used in conjunction with neural networks in speech recognition](https://arxiv.org/abs/1811.07453), where the assumption of a one-hot state makes sense for modelling phonemes, which are spoken one at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXQOBz5zqe10"
      },
      "source": [
        "## Acknowledgments\n",
        "\n",
        "This notebook is based partly on Lawrence Rabiner's excellent article \"[A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf)\", which you may also like to check out. Thanks also to Dima Serdyuk and Kyle Gorman for their feedback on the draft."
      ]
    }
  ]
}