{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HMM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDJIV2EVBuFZ"
      },
      "source": [
        "# Fun with Hidden Markov Models\n",
        "*by Loren Lugosch*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rWFkdjYOlk8"
      },
      "source": [
        "This notebook introduces the Hidden Markov Model (HMM), a simple model for sequential data.\n",
        "\n",
        "We will see:\n",
        "- what an HMM is and when you might want to use it;\n",
        "- the so-called \"three problems\" of an HMM; and \n",
        "- how to implement an HMM in PyTorch.\n",
        "\n",
        "(The code in this notebook can also be found at https://github.com/lorenlugosch/pytorch_HMM.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4g7IG7CBx-Y"
      },
      "source": [
        "## The HMM setup\n",
        "\n",
        "The hypothetical scenario of the friend travelling between cities and sending you selfies can be modeled using an HMM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMPrA6Uv-u-K"
      },
      "source": [
        "An HMM makes two key assumptions:\n",
        "- **Assumption 1:** The state at time $t$ depends *only* on the state at the previous time $t-1$. \n",
        "- **Assumption 2:** The output at time $t$ depends *only* on the state at time $t$.\n",
        "\n",
        "These two assumptions make it possible to efficiently compute certain quantities that we may be interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRNhSK7LgEIS"
      },
      "source": [
        "## Components of an HMM\n",
        "An HMM has three sets of trainable parameters.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pu3zm77vXwp"
      },
      "source": [
        "- The **transition model** is a square matrix $A$, where $A_{s, s'}$ represents $p(z_t = s|z_{t-1} = s')$, the probability of jumping from state $s'$ to state $s$. \n",
        "\n",
        "- The **emission model** $b_s(x_t)$ tells us $p(x_t|z_t = s)$, the probability of generating $x_t$ when the system is in state $s$. For discrete observations, which we will use in this notebook, the emission model is just a lookup table, with one row for each state, and one column for each observation. For real-valued observations, it is common to use a Gaussian mixture model or neural network to implement the emission model. \n",
        "\n",
        "- The **state priors** tell us $p(z_1 = s)$, the probability of starting in state $s$. We use $\\pi$ to denote the vector of state priors, so $\\pi_s$ is the state prior for state $s$.\n",
        "\n",
        "Let's program an HMM class in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as distrib\n",
        "import torch.distributions.transforms as transform\n",
        "# Imports for plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OSjVMTgrK8Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZbW6Pj0og7K"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class HMM(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Hidden Markov Model with discrete observations.\n",
        "  \"\"\"\n",
        "  def __init__(self, N, distributions):\n",
        "    super(HMM, self).__init__()\n",
        "    #self.M = M # number of possible observations\n",
        "    self.N = N # number of states\n",
        "\n",
        "    # A\n",
        "    self.transition_model = TransitionModel(self.N)\n",
        "\n",
        "    # b(x_t)\n",
        "    self.emission_model = EmissionModel(self.N, distributions)\n",
        "\n",
        "    # pi\n",
        "    self.unnormalized_state_priors = torch.nn.Parameter(torch.randn(self.N))\n",
        "\n",
        "    # use the GPU\n",
        "    self.is_cuda = torch.cuda.is_available()\n",
        "    if self.is_cuda: self.cuda()\n",
        "\n",
        "class TransitionModel(torch.nn.Module):\n",
        "  def __init__(self, N):\n",
        "    super(TransitionModel, self).__init__()\n",
        "    self.N = N\n",
        "    self.unnormalized_transition_matrix = torch.nn.Parameter(torch.randn(N,N))\n",
        "\n",
        "class EmissionModel(torch.nn.Module):\n",
        "  def __init__(self, N, distributions):\n",
        "    super(EmissionModel, self).__init__()\n",
        "    self.N = N\n",
        "    self.distributions = distributions ## list of distributions\n",
        "\n",
        "  def pdf(self, hidden_state, observation):\n",
        "    current_distribution = self.distributions[hidden_state]\n",
        "    return torch.exp(current_distribution.log_prob(torch.Tensor(observation)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpgkwNyVwmyM"
      },
      "source": [
        "def sample(self, T=10):\n",
        "  state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
        "  transition_matrix = torch.nn.functional.softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
        "  #emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
        "\n",
        "  # sample initial state\n",
        "  z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
        "  z = []; x = []\n",
        "  z.append(z_t)\n",
        "  for t in range(0,T):\n",
        "    # sample emission\n",
        "    # x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
        "    current_distribution = self.emission_model.distributions[z_t]\n",
        "    x_t = current_distribution.sample()\n",
        "    x.append(x_t)\n",
        "\n",
        "    # sample transition\n",
        "    z_t = torch.distributions.categorical.Categorical(transition_matrix[:,z_t]).sample().item()\n",
        "    if t < T-1: z.append(z_t)\n",
        " \n",
        "  return torch.stack(x), z\n",
        "\n",
        "# Add the sampling method to our HMM class\n",
        "HMM.sample = sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One dimentional sampling test"
      ],
      "metadata": {
        "id": "QgHSpJB-mDtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.Normal(-1,1), distrib.Normal(1, 1)])"
      ],
      "metadata": {
        "id": "fvuibzlKQGbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, z = test_model.sample(20)"
      ],
      "metadata": {
        "id": "f-2ZlXeEQU-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.functional.softmax(test_model.transition_model.unnormalized_transition_matrix, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_JoA2TGQm25",
        "outputId": "1ec5dab8-7a4c-4d15-fc74-e6205f3db16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2265, 0.6390],\n",
              "        [0.7735, 0.3610]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.scatter(list(range(len(x))),x, c=z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ZDqNbbUARKq7",
        "outputId": "7cb056ff-c705-46a4-858b-3a7cd5a82357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ff989bdbfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn2igp0GQe60",
        "outputId": "3a7624e9-f84e-4a31-dbb5-358c4b9d90cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.2457,  1.2421, -1.9351, -1.1116, -0.1787,  2.3537, -0.7931,  0.5367,\n",
              "         -1.7547,  2.4755, -0.7092, -1.3480, -1.0396,  0.5838, -2.3364,  0.3331,\n",
              "         -0.0563,  1.8021,  0.4979,  0.8501]),\n",
              " [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### two dimentional sampling test"
      ],
      "metadata": {
        "id": "1WNbMlwdmKH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.MultivariateNormal(torch.zeros(2),torch.eye(2)), distrib.MultivariateNormal(torch.ones(2)*5,torch.eye(2))])"
      ],
      "metadata": {
        "id": "PeTbT-VWSkSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, z = test_model.sample(200)"
      ],
      "metadata": {
        "id": "7lYjGYu4TLPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.functional.softmax(test_model.transition_model.unnormalized_transition_matrix, dim=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfWtNWpmTOuE",
        "outputId": "d42a0b5c-39d7-43d7-f2de-98c88908a2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2876, 0.0276],\n",
              "        [0.7124, 0.9724]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x[:,0],x[:,1], c=z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "HIudU1RcTRU-",
        "outputId": "c43cce5e-f2a2-4275-8252-b080684cf876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ff98969c090>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohsdYScawkRG"
      },
      "source": [
        "Let's try hard-coding an HMM for generating fake words. (We'll also add some helper functions for encoding and decoding strings.)\n",
        "\n",
        "We will assume that the system has one state for generating vowels and one state for generating consonants, and the transition matrix has 0s on the diagonal---in other words, the system cannot stay in the vowel state or the consonant state for one than one timestep; it has to switch.\n",
        "\n",
        "Since we pass the transition matrix through a softmax, to get 0s we set the unnormalized parameter values to $-\\infty$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_distr = distrib.Normal(5,1)"
      ],
      "metadata": {
        "id": "GO3bHRzkkQkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(test_distr.log_prob(torch.Tensor([4])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nomKg-z9kVZn",
        "outputId": "ddc71202-aa8b-485c-aef5-42cde9df40fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2420])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 10, 1000)\n"
      ],
      "metadata": {
        "id": "ygpIvgX4kz9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q0_density = torch.exp(test_distr.log_prob(torch.Tensor(x))).numpy()\n"
      ],
      "metadata": {
        "id": "vnlFqHsQkk5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(1, 1, sharex=True, figsize=(15, 5))\n",
        "ax1.plot(x, q0_density); ax1.fill_between(x, q0_density, 0, alpha=0.5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "FvZwh66UlAa6",
        "outputId": "22a3b4b0-3236-4885-adc2-99d05d4db0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f265e1eff90>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKzlTlfRgZod"
      },
      "source": [
        "## The Three Problems\n",
        "\n",
        "In a [classic tutorial](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf) on HMMs, Lawrence Rabiner describes \"three problems\" that need to be solved before you can effectively use an HMM. They are:\n",
        "- Problem 1: How do we efficiently compute $p(\\mathbf{x})$?\n",
        "- Problem 2: How do we find the most likely state sequence $\\mathbf{z}$ that could have generated the data? \n",
        "- Problem 3: How do we train the model?\n",
        "\n",
        "In the rest of the notebook, we will see how to solve each problem and implement the solutions in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_RfIAnmN2RZ"
      },
      "source": [
        "### Problem 1: How do we compute $p(\\mathbf{x})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zUUYH0giKV"
      },
      "source": [
        "\n",
        "#### *Why?*\n",
        "Why might we care about computing $p(\\mathbf{x})$? Here's two reasons.\n",
        "* Given two HMMs, $\\theta_1$ and $\\theta_2$, we can compute the likelihood of some data $\\mathbf{x}$ under each model, $p_{\\theta_1}(\\mathbf{x})$ and $p_{\\theta_2}(\\mathbf{x})$, to decide which model is a better fit to the data. \n",
        "\n",
        "  (For example, given an HMM for English speech and an HMM for French speech, we could compute the likelihood given each model, and pick the model with the higher likelihood to infer whether the person is speaking English or French.)\n",
        "* Being able to compute $p(\\mathbf{x})$ gives us a way to train the model, as we will see later.\n",
        "\n",
        "#### *How?*\n",
        "Given that we want $p(\\mathbf{x})$, how do we compute it?\n",
        "\n",
        "We've assumed that the data is generated by visiting some sequence of states $\\mathbf{z}$ and picking an output $x_t$ for each $z_t$ from the emission distribution $p(x_t|z_t)$. So if we knew $\\mathbf{z}$, then the probability of $\\mathbf{x}$ could be computed as follows:\n",
        "\n",
        "$$p(\\mathbf{x}|\\mathbf{z}) = \\prod_{t} p(x_t|z_t) p(z_t|z_{t-1})$$\n",
        "\n",
        "However, we don't know $\\mathbf{z}$; it's hidden. But we do know the probability of any given $\\mathbf{z}$, independent of what we observe. So we could get the probability of $\\mathbf{x}$ by summing over the different possibilities for $\\mathbf{z}$, like this:\n",
        "\n",
        "$$p(\\mathbf{x}) = \\sum_{\\mathbf{z}} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) = \\sum_{\\mathbf{z}} \\prod_{t} p(x_t|z_t) p(z_t|z_{t-1})$$\n",
        "\n",
        "The problem is: if you try to take that sum directly, you will need to compute $N^T$ terms. This is impossible to do for anything but very short sequences. For example, let's say the sequence is of length $T=100$ and there are $N=2$ possible states. Then we would need to check $N^T = 2^{100} \\approx 10^{30}$ different possible state sequences.\n",
        "\n",
        "We need a way to compute $p(\\mathbf{x})$ that doesn't require us to explicitly calculate all $N^T$ terms. For this, we use the forward algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrH0YdUAhS6J"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Forward Algorithm</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\alpha_{s,1} := b_s(x_1) \\cdot \\pi_s$ \n",
        "> \n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "> $\\alpha_{s,t} := b_s(x_t) \\cdot \\underset{s'}{\\sum} A_{s, s'} \\cdot \\alpha_{s',t-1} $\n",
        "> \n",
        "> $p(\\mathbf{x}) := \\underset{s}{\\sum} \\alpha_{s,T}$\\\n",
        "> return $p(\\mathbf{x})$\n",
        "________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAdpwRiMn8Vn"
      },
      "source": [
        "The forward algorithm is much faster than enumerating all $N^T$ possible state sequences: it requires only $O(N^2T)$ operations to run, since each step is mostly multiplying the vector of forward variables by the transition matrix. (And very often we can reduce that complexity even further, if the transition matrix is sparse.)\n",
        "\n",
        "There is one practical problem with the forward algorithm as presented above: it is prone to underflow due to multiplying a long chain of small numbers, since probabilities are always between 0 and 1. Instead, let's do everything in the log domain. In the log domain, a multiplication becomes a sum, and a sum becomes a [logsumexp](https://lorenlugosch.github.io/posts/2020/06/logsumexp/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ8VsLFxA3iT"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Forward Algorithm (Log Domain)</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\text{log }\\alpha_{s,1} := \\text{log }b_s(x_1) + \\text{log }\\pi_s$ \n",
        "> \n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "> $\\text{log }\\alpha_{s,t} := \\text{log }b_s(x_t) +  \\underset{s'}{\\text{logsumexp}} \\left( \\text{log }A_{s, s'} + \\text{log }\\alpha_{s',t-1} \\right)$\n",
        "> \n",
        "> $\\text{log }p(\\mathbf{x}) := \\underset{s}{\\text{logsumexp}} \\left( \\text{log }\\alpha_{s,T} \\right)$\\\n",
        "> return $\\text{log }p(\\mathbf{x})$\n",
        "________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g55ik6ZCEiJU"
      },
      "source": [
        "Now that we have a numerically stable version of the forward algorithm, let's implement it in PyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CMdK1EfE1SJ"
      },
      "source": [
        "\n",
        "def HMM_forward(self, x, T, save_log_alpha=True):\n",
        "  \"\"\"\n",
        "  x : IntTensor of shape (batch size, T_max)\n",
        "  T : IntTensor of shape (batch size)\n",
        "\n",
        "  Compute log p(x) for each example in the batch.\n",
        "  T = length of each example\n",
        "  \"\"\"\n",
        "  if self.is_cuda:\n",
        "  \tx = x.cuda()\n",
        "  \tT = T.cuda()\n",
        "\n",
        "  batch_size = x.shape[0]; T_max = x.shape[1]\n",
        "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "  log_alpha = torch.zeros(batch_size, T_max, self.N) # table (sample, t, state) containing log probability of observations from sample to time t and being in state (in time t)\n",
        "  if self.is_cuda: log_alpha = log_alpha.cuda()\n",
        "\n",
        "  print(log_state_priors)\n",
        "  log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
        "  for t in range(1, T_max):\n",
        "    log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
        "\n",
        "  if save_log_alpha:\n",
        "    self.log_alpha = log_alpha\n",
        "    self.x = x\n",
        "  # Select the sum for the final timestep (each x may have different length).\n",
        "  #print(\"alpha\\n\", log_alpha)\n",
        "  log_sums = log_alpha.logsumexp(dim=2)\n",
        "  #print(\"log_sums\\n\", log_sums)\n",
        "  #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
        "  log_probs = torch.gather(log_sums, 1, T.view(-1,1)-1)\n",
        "  return log_probs\n",
        "\n",
        "def emission_model_forward(self, x_t): ## TODO\n",
        "  #out = self.distributions.log_prob(x_t)\n",
        "  #out = \n",
        "  out  = []\n",
        "  for state in range(self.N):\n",
        "    out.append( self.distributions[state].log_prob(x_t) )\n",
        "  result = torch.stack(out, dim = 1)\n",
        "  #print(\"emission probs\\n\",result)\n",
        "  return result\n",
        "\n",
        "def transition_model_forward(self, log_alpha):\n",
        "  \"\"\"\n",
        "  log_alpha : Tensor of shape (batch size, N)\n",
        "  Multiply previous timestep's alphas by transition matrix (in log domain)\n",
        "  \"\"\"\n",
        "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "  # Matrix multiplication in the log domain\n",
        "  out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
        "  return out\n",
        "\n",
        "def log_domain_matmul(log_A, log_B):\n",
        "\t\"\"\"\n",
        "\tlog_A : m x n\n",
        "\tlog_B : n x p\n",
        "\toutput : m x p matrix\n",
        "\n",
        "\tNormally, a matrix multiplication\n",
        "\tcomputes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
        "\n",
        "\tA log domain matrix multiplication\n",
        "\tcomputes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
        "\t\"\"\"\n",
        "\tm = log_A.shape[0]\n",
        "\tn = log_A.shape[1]\n",
        "\tp = log_B.shape[1]\n",
        "\n",
        "\t# log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
        "\t# log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
        "    # fix for PyTorch > 1.5 by egaznep on Github:\n",
        "\tlog_A_expanded = torch.reshape(log_A, (m,n,1))\n",
        "\tlog_B_expanded = torch.reshape(log_B, (1,n,p))\n",
        "\n",
        "\telementwise_sum = log_A_expanded + log_B_expanded\n",
        "\tout = torch.logsumexp(elementwise_sum, dim=1)\n",
        "\n",
        "\treturn out\n",
        "\n",
        "TransitionModel.forward = transition_model_forward\n",
        "EmissionModel.forward = emission_model_forward\n",
        "HMM.forward = HMM_forward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test forward method"
      ],
      "metadata": {
        "id": "K4mnlI8muU6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.Normal(-1,1), distrib.Normal(1, 1)])"
      ],
      "metadata": {
        "id": "qPLPQRF-uXid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, states = test_model.sample(30)"
      ],
      "metadata": {
        "id": "Xm_JPMKxucOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T =torch.Tensor([10,10,10], dtype= torch.int64)\n",
        "T = torch.ones([1,3], dtype=torch.int64)*10\n",
        "T.view(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtSSX_nuvn-L",
        "outputId": "e0a7c15a-fafb-4c39-fcef-f1ae1b9d5154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10],\n",
              "        [10],\n",
              "        [10]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(torch.reshape(x, (3, 10)), T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaBm7pbou8d2",
        "outputId": "d524e76d-2f4b-497e-d9a5-2b80e4c0556f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1396, -2.0377], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-17.9265],\n",
              "        [-17.2116],\n",
              "        [-16.7105]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBCrFobsEM8X"
      },
      "source": [
        "#### *Side note: deriving the forward algorithm*\n",
        "\n",
        "If you're interested in understanding how the forward algorithm actually computes $p(\\mathbf{x})$, read this section; if not, skip to the next part on \"Problem 2\" (finding the most likely state sequence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpHWWKcxhjkx"
      },
      "source": [
        "\n",
        "\n",
        "To derive the forward algorithm, start by deriving the forward variable:\n",
        "\n",
        "$\n",
        "\\begin{align} \n",
        "    \\alpha_{s,t} &= p(x_1, x_2, \\dots, x_t, z_t=s) \\\\\n",
        "     &= p(x_t | x_1, x_2, \\dots, x_{t-1}, z_t = s) \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_t = s)  \\\\ \n",
        "    &= p(x_t | z_t = s) \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_t = s) \\\\\n",
        "    &= p(x_t | z_t = s) \\cdot \\left( \\sum_{s'} p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s', z_t = s) \\right)\\\\\n",
        "    &= p(x_t | z_t = s) \\cdot \\left( \\sum_{s'} p(z_t = s | x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s') \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s') \\right)\\\\\n",
        "    &= \\underbrace{p(x_t | z_t = s)}_{\\text{emission model}} \\cdot \\left( \\sum_{s'} \\underbrace{p(z_t = s | z_{t-1}=s')}_{\\text{transition model}} \\cdot \\underbrace{p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s')}_{\\text{forward variable for previous timestep}} \\right)\\\\\n",
        "    &= b_s(x_t) \\cdot \\left( \\sum_{s'} A_{s, s'} \\cdot \\alpha_{s',t-1} \\right)\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "I'll explain how to get to each line of this equation from the previous line. \n",
        "\n",
        "Line 1 is the definition of the forward variable $\\alpha_{s,t}$.\n",
        "\n",
        "Line 2 is the chain rule ($p(A,B) = p(A|B) \\cdot p(B)$, where $A$ is $x_t$ and $B$ is all the other variables).\n",
        "\n",
        "In Line 3, we apply Assumption 2: the probability of observation $x_t$ depends only on the current state $z_t$.\n",
        "\n",
        "In Line 4, we marginalize over all the possible states in the previous timestep $t-1$.\n",
        "\n",
        "In Line 5, we apply the chain rule again.\n",
        "\n",
        "In Line 6, we apply Assumption 1: the current state depends only on the previous state.\n",
        "\n",
        "In Line 7, we substitute in the emission probability, the transition probability, and the forward variable for the previous timestep, to get the complete recursion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh1ovNjWDbIA"
      },
      "source": [
        "The formula above can be used for $t = 2 \\rightarrow T$. At $t=1$, there is no previous state, so instead of the transition matrix $A$, we use the state priors $\\pi$, which tell us the probability of starting in each state. Thus for $t=1$, the forward variables are computed as follows:\n",
        "\n",
        "$$\\begin{align} \n",
        "\\alpha_{s,1} &= p(x_1, z_1=s) \\\\\n",
        "  &= p(x_1 | z_1 = s) \\cdot p(z_1 = s)  \\\\ \n",
        "&= b_s(x_1) \\cdot \\pi_s\n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRzSqkRkEWKX"
      },
      "source": [
        "Finally, to compute $p(\\mathbf{x}) = p(x_1, x_2, \\dots, x_T)$, we marginalize over $\\alpha_{s,T}$, the forward variables computed in the last timestep:\n",
        "\n",
        "$$\\begin{align*} \n",
        "p(\\mathbf{x}) &= \\sum_{s} p(x_1, x_2, \\dots, x_T, z_T = s) \\\\ \n",
        "&= \\sum_{s} \\alpha_{s,T}\n",
        "\\end{align*}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLBU8Iu7I5Tb"
      },
      "source": [
        "You can get from this formulation to the log domain formulation by taking the log of the forward variable, and using these identities:\n",
        "- $\\text{log }(a \\cdot b) = \\text{log }a + \\text{log }b$\n",
        "- $\\text{log }(a + b) = \\text{log }(e^{\\text{log }a} + e^{\\text{log }b}) = \\text{logsumexp}(\\text{log }a, \\text{log }b)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxivzF8hgpiW"
      },
      "source": [
        "### Problem 2: How do we compute $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Kv2yyiN7SX"
      },
      "source": [
        "Given an observation sequence $\\mathbf{x}$, we may want to find the most likely sequence of states that could have generated $\\mathbf{x}$. (Given the sequence of selfies, we want to infer what cities the friend visited.) In other words, we want $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x})$.\n",
        "\n",
        "We can use Bayes' rule to rewrite this expression:\n",
        "$$\\begin{align*} \n",
        "    \\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x}) &= \\underset{\\mathbf{z}}{\\text{argmax }} \\frac{p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{p(\\mathbf{x})} \\\\ \n",
        "    &= \\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})\n",
        "\\end{align*}$$\n",
        "\n",
        "Hmm! That last expression, $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})$, looks suspiciously similar to the intractable expression we encountered before introducing the forward algorithm, $\\underset{\\mathbf{z}}{\\sum} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})$.\n",
        "\n",
        "And indeed, just as the intractable *sum* over all $\\mathbf{z}$ can be implemented efficiently using the forward algorithm, so too this intractable *argmax* can be implemented efficiently using a similar divide-and-conquer algorithm: the legendary Viterbi algorithm!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niKZEX5xWeWR"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Viterbi Algorithm</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta_{s,1} := b_s(x_1) \\cdot \\pi_s$\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\psi_{s,1} := 0$\n",
        ">\n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta_{s,t} := b_s(x_t) \\cdot \\left( \\underset{s'}{\\text{max }} A_{s, s'} \\cdot \\delta_{s',t-1} \\right)$\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\psi_{s,t} := \\underset{s'}{\\text{argmax }} A_{s, s'} \\cdot \\delta_{s',t-1}$\n",
        "> \n",
        "> $z_T^* := \\underset{s}{\\text{argmax }} \\delta_{s,T}$\\\n",
        "> for $t = T-1 \\rightarrow 1$:\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_{t}^* := \\psi_{z_{t+1}^*,t+1}$\n",
        "> \n",
        "> $\\mathbf{z}^* := \\{z_{1}^*, \\dots, z_{T}^* \\}$\\\n",
        "return $\\mathbf{z}^*$\n",
        "________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcHVTCucZV6K"
      },
      "source": [
        "The Viterbi algorithm looks somewhat gnarlier than the forward algorithm, but it is essentially the same algorithm, with two tweaks: 1) instead of taking the sum over previous states, we take the max; and 2) we record the argmax of the previous states in a table, and loop back over this table at the end to get $\\mathbf{z}^*$, the most likely state sequence. (And like the forward algorithm, we should run the Viterbi algorithm in the log domain for better numerical stability.) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlN7IY_JZ5A-"
      },
      "source": [
        "Let's add the Viterbi algorithm to our PyTorch model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeDG8DVmZ-P0"
      },
      "source": [
        "def viterbi(self, x, T):\n",
        "  \"\"\"\n",
        "  x : IntTensor of shape (batch size, T_max)\n",
        "  T : IntTensor of shape (batch size)\n",
        "  Find argmax_z log p(x|z) for each (x) in the batch.\n",
        "  \"\"\"\n",
        "  if self.is_cuda:\n",
        "    x = x.cuda()\n",
        "    T = T.cuda()\n",
        "\n",
        "  batch_size = x.shape[0]; T_max = x.shape[1]\n",
        "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "  log_delta = torch.zeros(batch_size, T_max, self.N).float()\n",
        "  psi = torch.zeros(batch_size, T_max, self.N).long()\n",
        "  if self.is_cuda:\n",
        "    log_delta = log_delta.cuda()\n",
        "    psi = psi.cuda()\n",
        "\n",
        "  log_delta[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
        "  for t in range(1, T_max):\n",
        "    max_val, argmax_val = self.transition_model.maxmul(log_delta[:, t-1, :])\n",
        "    log_delta[:, t, :] = self.emission_model(x[:,t]) + max_val\n",
        "    psi[:, t, :] = argmax_val\n",
        "\n",
        "  # Get the log probability of the best path\n",
        "  log_max = log_delta.max(dim=2)[0]\n",
        "  best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
        "\n",
        "  # This next part is a bit tricky to parallelize across the batch,\n",
        "  # so we will do it separately for each example.\n",
        "  z_star = []\n",
        "  for i in range(0, batch_size):\n",
        "    z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
        "    for t in range(T[i] - 1, 0, -1):\n",
        "      z_t = psi[i, t, z_star_i[0]].item()\n",
        "      z_star_i.insert(0, z_t)\n",
        "\n",
        "    z_star.append(z_star_i)\n",
        "\n",
        "  return z_star, best_path_scores # return both the best path and its log probability\n",
        "\n",
        "def transition_model_maxmul(self, log_alpha):\n",
        "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "  out1, out2 = maxmul(log_transition_matrix, log_alpha.transpose(0,1))\n",
        "  return out1.transpose(0,1), out2.transpose(0,1)\n",
        "\n",
        "def maxmul(log_A, log_B):\n",
        "\t\"\"\"\n",
        "\tlog_A : m x n\n",
        "\tlog_B : n x p\n",
        "\toutput : m x p matrix\n",
        "\n",
        "\tSimilar to the log domain matrix multiplication,\n",
        "\tthis computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
        "\t\"\"\"\n",
        "\tm = log_A.shape[0]\n",
        "\tn = log_A.shape[1]\n",
        "\tp = log_B.shape[1]\n",
        "\n",
        "\tlog_A_expanded = torch.stack([log_A] * p, dim=2)\n",
        "\tlog_B_expanded = torch.stack([log_B] * m, dim=0)\n",
        "\n",
        "\telementwise_sum = log_A_expanded + log_B_expanded\n",
        "\tout1,out2 = torch.max(elementwise_sum, dim=1)\n",
        "\n",
        "\treturn out1,out2\n",
        "\n",
        "TransitionModel.maxmul = transition_model_maxmul\n",
        "HMM.viterbi = viterbi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test viterbi"
      ],
      "metadata": {
        "id": "ddLYM5OH8IzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.Normal(-1,1), distrib.Normal(1, 1)])"
      ],
      "metadata": {
        "id": "OLPEYOgY8FZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, states = test_model.sample(10)"
      ],
      "metadata": {
        "id": "G0fI_4vX8QPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgx_mORFC9na",
        "outputId": "14cc7cc8-75c5-4a9f-d5f0-b2c68d19b347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-2.1063,  1.8792, -0.1379, -0.5465, -0.4642, -0.6617,  0.8014,  1.3214,\n",
              "          1.9318,  2.8876]), [0, 1, 1, 0, 1, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = torch.ones([1], dtype=torch.int64)*10\n"
      ],
      "metadata": {
        "id": "wJkumbFN8U25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viterbi_results = test_model.viterbi(torch.reshape(x, (1, 10)), T)"
      ],
      "metadata": {
        "id": "DBxpXgvbDHvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viterbi_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvOSLuk6DmKD",
        "outputId": "0a7b2bd2-43f3-4398-b4eb-ad1d158d8c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[0, 1, 1, 0, 1, 0, 1, 1, 1, 1]],\n",
              " tensor([[-19.6291]], device='cuda:0', grad_fn=<GatherBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.forward(torch.reshape(x, (1, 10)), T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS7w6yLLEKYt",
        "outputId": "e3917f86-0fd7-4947-dcd7-12d5984202c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.0222, -0.4460], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-18.2176]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H1tpPT7DpKO",
        "outputId": "2ab878fd-3c1b-4e0c-fb69-168579b4d93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 1, 0, 1, 0, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InF6PJVOfHwH"
      },
      "source": [
        "The two scores are the same! That's because in this instance there is only one possible path through the HMM, so the probability of the most likely path is the same as the sum of the probabilities of all possible paths.\n",
        "\n",
        "In general, though, the forward score and Viterbi score will always be somewhat close. This is because of a property of the $\\text{logsumexp}$ function: $\\text{logsumexp}(\\mathbf{x}) \\approx \\max (\\mathbf{x})$. ($\\text{logsumexp}$ is sometimes referred to as the \"smooth maximum\" function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x__70tB6gnkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79cf3af7-f75f-43ee-b117-923becdc1095"
      },
      "source": [
        "x = torch.tensor([1., 2., 3.])\n",
        "print(x.max(dim=0)[0])\n",
        "print(x.logsumexp(dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n",
            "tensor(3.4076)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvFtiWhzgy0V"
      },
      "source": [
        "### Problem 3: How do we train the model?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3JaykRalSBZ"
      },
      "source": [
        "Earlier, we hard-coded an HMM to have certain behavior. What we would like to do instead is have the HMM learn to model the data on its own. And while it is possible to use supervised learning with an HMM (by hard-coding the emission model or the transition model) so that the states have a particular interpretation, the really cool thing about HMMs is that they are naturally unsupervised learners, so they can learn to use their different states to represent different patterns in the data, without the programmer needing to indicate what each state means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K471fT4N-PR"
      },
      "source": [
        "Like many machine learning models, an HMM can be trained using maximum likelihood estimation, i.e.:\n",
        "\n",
        "$$\\theta^* = \\underset{\\theta}{\\text{argmin }} -\\sum_{\\mathbf{x}^i}\\text{log }p_{\\theta}(\\mathbf{x}^i)$$\n",
        "\n",
        "where $\\mathbf{x}^1, \\mathbf{x}^2, \\dots$ are training examples. \n",
        "\n",
        "The standard method for doing this is the Expectation-Maximization (EM) algorithm, which for HMMs is also called the \"Baum-Welch\" algorithm. In EM training, we alternate between an \"E-step\", where we estimate the values of the latent variables, and an \"M-step\", where the model parameters are updated given the estimated latent variables. (Think $k$-means, where you guess which cluster each data point belongs to, then reestimate where the clusters are, and repeat.) The EM algorithm has some nice properties: it is guaranteed at each step to decrease the loss function, and the E-step and M-step may have an exact closed form solution, in which case no pesky learning rates are required.\n",
        "\n",
        "But because the HMM forward algorithm is differentiable with respect to all the model parameters, we can also just take advantage of automatic differentiation methods in libraries like PyTorch and try to minimize $-\\text{log }p_{\\theta}(\\mathbf{x})$ directly, by backpropagating through the forward algorithm and running stochastic gradient descent. That means we don't need to write any additional HMM code to implement training: `loss.backward()` is all you need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVh0-369qZDC"
      },
      "source": [
        "Here we will implement SGD training for an HMM in PyTorch. First, some helper classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqiFobGHwdzc"
      },
      "source": [
        "import torch.utils.data\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, lines):\n",
        "    self.lines = lines # list of strings\n",
        "    collate = Collate() # function for generating a minibatch from strings\n",
        "    self.loader = torch.utils.data.DataLoader(self, batch_size=1024, num_workers=1, shuffle=True, collate_fn=collate)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lines)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    line = self.lines[idx].lstrip(\" \").rstrip(\"\\n\").rstrip(\" \").rstrip(\"\\n\")\n",
        "    return line\n",
        "\n",
        "class Collate:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    \"\"\"\n",
        "    Returns a minibatch of strings, padded to have the same length.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    batch_size = len(batch)\n",
        "    for index in range(batch_size):\n",
        "      x_ = batch[index]\n",
        "\n",
        "      # convert letters to integers\n",
        "      x.append(encode(x_))\n",
        "\n",
        "    # pad all sequences with 0 to have same length\n",
        "    x_lengths = [len(x_) for x_ in x]\n",
        "    T = max(x_lengths)\n",
        "    for index in range(batch_size):\n",
        "      x[index] += [0] * (T - len(x[index]))\n",
        "      x[index] = torch.tensor(x[index])\n",
        "\n",
        "    # stack into single tensor\n",
        "    x = torch.stack(x)\n",
        "    x_lengths = torch.tensor(x_lengths)\n",
        "    return (x,x_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpDpwnPnAEA9"
      },
      "source": [
        "Let's load some training/testing data. By default, this will use the unix \"words\" file, but you could also use your own text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52NqFHg8ANsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d367cf-3e66-472b-d8de-0623d986f28d"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/lorenlugosch/pytorch_HMM/master/data/train/training.txt\n",
        "\n",
        "filename = \"training.txt\"\n",
        "\n",
        "with open(filename, \"r\") as f:\n",
        "  lines = f.readlines() # each line of lines will have one word\n",
        "\n",
        "alphabet = list(Counter((\"\".join(lines))).keys())\n",
        "train_lines, valid_lines = train_test_split(lines, test_size=0.1, random_state=42)\n",
        "train_dataset = TextDataset(train_lines)\n",
        "valid_dataset = TextDataset(valid_lines)\n",
        "\n",
        "M = len(alphabet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 08:57:32--  https://raw.githubusercontent.com/lorenlugosch/pytorch_HMM/master/data/train/training.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2493109 (2.4M) [text/plain]\n",
            "Saving to: training.txt\n",
            "\n",
            "training.txt        100%[===================>]   2.38M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-06-12 08:57:33 (70.1 MB/s) - training.txt saved [2493109/2493109]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0AqmyrK7IUn"
      },
      "source": [
        "We will use a Trainer class for training and testing the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iypy_neX9cpq"
      },
      "source": [
        "from tqdm import tqdm # for displaying progress bar\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, lr):\n",
        "    self.model = model\n",
        "    self.lr = lr\n",
        "    self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=0.00001)\n",
        "  \n",
        "  def train(self, dataset):\n",
        "    train_loss = 0\n",
        "    num_samples = 0\n",
        "    self.model.train()\n",
        "    print_interval = 50\n",
        "    for idx, batch in enumerate(tqdm(dataset.loader)):\n",
        "      x,T = batch\n",
        "      batch_size = len(x)\n",
        "      num_samples += batch_size\n",
        "      log_probs = self.model(x,T)\n",
        "      loss = -log_probs.mean()\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      train_loss += loss.cpu().data.numpy().item() * batch_size\n",
        "      if idx % print_interval == 0:\n",
        "        print(\"loss:\", loss.item())\n",
        "        for _ in range(5):\n",
        "          sampled_x, sampled_z = self.model.sample()\n",
        "          print(decode(sampled_x))\n",
        "          print(sampled_z)\n",
        "    train_loss /= num_samples\n",
        "    return train_loss\n",
        "\n",
        "  def test(self, dataset):\n",
        "    test_loss = 0\n",
        "    num_samples = 0\n",
        "    self.model.eval()\n",
        "    print_interval = 50\n",
        "    for idx, batch in enumerate(dataset.loader):\n",
        "      x,T = batch\n",
        "      batch_size = len(x)\n",
        "      num_samples += batch_size\n",
        "      log_probs = self.model(x,T)\n",
        "      loss = -log_probs.mean()\n",
        "      test_loss += loss.cpu().data.numpy().item() * batch_size\n",
        "      if idx % print_interval == 0:\n",
        "        print(\"loss:\", loss.item())\n",
        "        sampled_x, sampled_z = self.model.sample()\n",
        "        print(decode(sampled_x))\n",
        "        print(sampled_z)\n",
        "    test_loss /= num_samples\n",
        "    return test_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUR8qbHm9dMg"
      },
      "source": [
        "Finally, initialize the model and run the main training loop. Every 50 batches, the code will produce a few samples from the model. Over time, these samples should look more and more realistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-NGIK1Q9g2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53092fb7-c32d-4981-c124-f3a1dd5a386a"
      },
      "source": [
        "# Initialize model\n",
        "model = HMM(N=64, M=M)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "trainer = Trainer(model, lr=0.01)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "        print(\"========= Epoch %d of %d =========\" % (epoch+1, num_epochs))\n",
        "        train_loss = trainer.train(train_dataset)\n",
        "        valid_loss = trainer.test(valid_dataset)\n",
        "\n",
        "        print(\"========= Results: epoch %d of %d =========\" % (epoch+1, num_epochs))\n",
        "        print(\"train loss: %.2f| valid loss: %.2f\\n\" % (train_loss, valid_loss) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= Epoch 1 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 38.220359802246094\n",
            "ovS\n",
            "BqfdTS\n",
            "[4, 17, 26, 24, 49, 27, 29, 47, 12, 47]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|         | 3/208 [00:00<00:23,  8.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SzhiieL\n",
            "tZ\n",
            "[32, 36, 55, 47, 35, 57, 14, 60, 59, 38]\n",
            "-OVlKNufaM\n",
            "[60, 18, 60, 50, 32, 57, 20, 51, 33, 58]\n",
            "mJkSOib\n",
            "Kw\n",
            "[18, 39, 60, 17, 38, 55, 51, 47, 32, 50]\n",
            "vsIWdPnBnA\n",
            "[0, 8, 56, 27, 17, 23, 47, 39, 58, 57]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:10, 14.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 32.680179595947266\n",
            "gdiaEscYSs\n",
            "[32, 30, 50, 14, 48, 43, 20, 42, 26, 29]\n",
            "iUvMlHiiwo\n",
            "[29, 43, 30, 14, 48, 3, 46, 59, 29, 40]\n",
            "SfatoBrGIK\n",
            "[20, 53, 2, 46, 2, 57, 41, 11, 40, 24]\n",
            "mMilVb\n",
            "Yxx\n",
            "[44, 19, 40, 40, 60, 27, 58, 50, 0, 13]\n",
            "sUhYqrlIMx\n",
            "[33, 27, 52, 54, 39, 41, 48, 63, 22, 38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:07<00:07, 13.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 30.349931716918945\n",
            "pRrsinkell\n",
            "[50, 62, 12, 45, 62, 22, 58, 38, 59, 15]\n",
            "ihLhnbisud\n",
            "[6, 54, 51, 16, 31, 36, 54, 34, 16, 39]\n",
            "oraonioiqs\n",
            "[0, 41, 17, 52, 0, 59, 63, 22, 0, 14]\n",
            "juciweufcn\n",
            "[44, 62, 49, 1, 50, 0, 43, 47, 35, 20]\n",
            "sicfadnxrd\n",
            "[44, 19, 49, 49, 1, 46, 22, 48, 3, 38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:10<00:03, 15.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 28.456218719482422\n",
            "viycelurTb\n",
            "[44, 62, 12, 14, 45, 16, 14, 41, 63, 39]\n",
            "oiomomyorb\n",
            "[18, 62, 24, 45, 14, 45, 15, 24, 41, 39]\n",
            "ecSlqvFnag\n",
            "[44, 49, 31, 59, 13, 51, 15, 22, 63, 53]\n",
            "hrsoniedil\n",
            "[18, 12, 31, 24, 0, 47, 12, 39, 62, 22]\n",
            "penfniTlru\n",
            "[51, 54, 63, 33, 22, 50, 49, 31, 37, 15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 14.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.394794464111328\n",
            "dascidomyH\n",
            "[34, 14, 5, 46, 62, 34, 24, 45, 24, 45]\n",
            "tFshXdrduV\n",
            "[44, 15, 22, 31, 54, 34, 54, 34, 40, 57]\n",
            "twitcgoesr\n",
            "[44, 21, 43, 12, 5, 58, 31, 54, 5, 39]\n",
            "cudu\n",
            "ltilr\n",
            "[44, 62, 55, 14, 5, 9, 46, 62, 44, 41]\n",
            "ramsVfroag\n",
            "[18, 24, 45, 24, 45, 24, 31, 62, 35, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:14<00:00, 14.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.797958374023438\n",
            "bessrinive\n",
            "[44, 63, 22, 49, 31, 62, 44, 62, 53, 54]\n",
            "========= Results: epoch 1 of 10 =========\n",
            "train loss: 30.79| valid loss: 26.61\n",
            "\n",
            "========= Epoch 2 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:40,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.685302734375\n",
            "Marusfliih\n",
            "[44, 63, 41, 62, 35, 63, 39, 59, 63, 46]\n",
            "ateemmcala\n",
            "[14, 0, 54, 24, 45, 62, 35, 63, 39, 63]\n",
            "ufKsllpXti\n",
            "[14, 45, 62, 46, 53, 0, 26, 62, 46, 62]\n",
            "eOmogoates\n",
            "[33, 14, 45, 24, 35, 20, 49, 31, 54, 5]\n",
            "-lianhertr\n",
            "[18, 39, 59, 63, 22, 31, 54, 22, 49, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:10, 15.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.105327606201172\n",
            "rietreeguc\n",
            "[41, 62, 49, 31, 41, 62, 63, 35, 62, 35]\n",
            "rinnibuDrE\n",
            "[41, 62, 5, 45, 62, 44, 14, 1, 41, 62]\n",
            "srarcronha\n",
            "[44, 41, 63, 22, 35, 41, 63, 22, 50, 63]\n",
            "hotingifal\n",
            "[18, 63, 31, 62, 22, 38, 62, 34, 63, 39]\n",
            "pIbcantres\n",
            "[16, 15, 22, 35, 63, 22, 31, 41, 63, 46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:06<00:07, 14.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 25.067440032958984\n",
            "radatepnty\n",
            "[41, 63, 39, 63, 31, 62, 46, 22, 31, 62]\n",
            "iasiaslial\n",
            "[62, 63, 45, 62, 63, 46, 6, 54, 63, 53]\n",
            "ancigeroab\n",
            "[63, 22, 31, 62, 35, 54, 41, 24, 14, 39]\n",
            "pedigaceXa\n",
            "[16, 54, 34, 62, 35, 49, 35, 63, 34, 63]\n",
            "socuschewe\n",
            "[48, 63, 35, 14, 45, 46, 50, 63, 39, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:10<00:03, 13.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.631916046142578\n",
            "giTmmcroov\n",
            "[18, 63, 61, 45, 45, 35, 41, 24, 47, 34]\n",
            "ultoupipia\n",
            "[15, 22, 31, 54, 5, 45, 62, 39, 62, 63]\n",
            "macencrepl\n",
            "[18, 63, 38, 63, 22, 35, 17, 12, 16, 6]\n",
            "phocarnoer\n",
            "[16, 6, 63, 22, 63, 22, 38, 6, 54, 41]\n",
            "racotlipre\n",
            "[41, 63, 35, 63, 31, 41, 62, 35, 41, 54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 14.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.934513092041016\n",
            "sireinapet\n",
            "[44, 15, 22, 38, 62, 22, 24, 45, 49, 31]\n",
            "unocismomr\n",
            "[15, 22, 63, 46, 62, 46, 50, 24, 45, 17]\n",
            "udsliglant\n",
            "[15, 22, 44, 6, 62, 35, 6, 63, 22, 31]\n",
            "tribineaai\n",
            "[33, 41, 62, 39, 62, 31, 54, 5, 43, 62]\n",
            "dumaPtesen\n",
            "[32, 14, 45, 63, 46, 31, 54, 45, 62, 22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:13<00:00, 14.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.86551284790039\n",
            "couchide-u\n",
            "[35, 63, 14, 46, 50, 62, 34, 54, 5, 52]\n",
            "========= Results: epoch 2 of 10 =========\n",
            "train loss: 25.27| valid loss: 24.60\n",
            "\n",
            "========= Epoch 3 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.48318099975586\n",
            "Anunbofich\n",
            "[21, 0, 15, 22, 38, 63, 22, 62, 46, 50]\n",
            "broteeyoca\n",
            "[26, 41, 63, 31, 59, 49, 25, 24, 35, 63]\n",
            "pmflousiim\n",
            "[18, 36, 44, 6, 24, 14, 46, 50, 62, 31]\n",
            "pqygiaRica\n",
            "[26, 6, 28, 38, 59, 63, 22, 62, 35, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|         | 3/208 [00:00<00:20, 10.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "viotederee\n",
            "[34, 62, 63, 31, 62, 34, 54, 41, 19, 49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 51/208 [00:04<00:15, 10.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.424537658691406\n",
            "cpanUidado\n",
            "[46, 31, 63, 22, 24, 47, 34, 63, 38, 54]\n",
            "rilitrodin\n",
            "[41, 63, 39, 62, 31, 41, 63, 34, 62, 35]\n",
            "croncKrous\n",
            "[35, 41, 63, 22, 35, 54, 41, 24, 14, 46]\n",
            "onlersnong\n",
            "[63, 22, 31, 54, 5, 44, 6, 63, 22, 38]\n",
            "calosmianD\n",
            "[18, 63, 39, 63, 5, 45, 62, 63, 22, 49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:07<00:07, 14.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.104280471801758\n",
            "Qandeblver\n",
            "[18, 63, 22, 38, 63, 39, 6, 0, 54, 41]\n",
            "ssheruoian\n",
            "[46, 31, 50, 54, 41, 14, 31, 62, 63, 45]\n",
            "inuuisiote\n",
            "[15, 22, 24, 17, 62, 45, 62, 63, 31, 62]\n",
            "tlDritrito\n",
            "[26, 6, 1, 41, 62, 31, 41, 62, 31, 24]\n",
            "priprighei\n",
            "[26, 41, 62, 35, 41, 62, 35, 50, 62, 46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:11<00:03, 14.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.04503631591797\n",
            "Dystrinlem\n",
            "[18, 1, 46, 31, 41, 62, 0, 43, 1, 45]\n",
            "anurateaef\n",
            "[63, 22, 62, 34, 63, 31, 54, 17, 62, 2]\n",
            "sentialcce\n",
            "[44, 63, 22, 31, 62, 63, 39, 49, 31, 62]\n",
            "cholyemall\n",
            "[35, 50, 63, 39, 62, 63, 45, 63, 39, 6]\n",
            "cortailexl\n",
            "[35, 63, 22, 31, 63, 39, 6, 62, 39, 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:14<00:00, 13.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.293285369873047\n",
            "allewovong\n",
            "[63, 39, 6, 54, 41, 63, 34, 63, 22, 38]\n",
            "wanvorNima\n",
            "[18, 63, 22, 34, 54, 5, 44, 62, 45, 63]\n",
            "phiturride\n",
            "[26, 50, 62, 31, 54, 5, 41, 62, 34, 54]\n",
            "mastideoch\n",
            "[18, 63, 22, 31, 62, 34, 62, 63, 35, 50]\n",
            "ffionadept\n",
            "[60, 2, 62, 63, 34, 63, 22, 62, 22, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:14<00:00, 13.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.761920928955078\n",
            "ofeadogidt\n",
            "[47, 34, 54, 63, 39, 24, 32, 62, 34, 54]\n",
            "========= Results: epoch 3 of 10 =========\n",
            "train loss: 24.22| valid loss: 24.04\n",
            "\n",
            "========= Epoch 4 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:38,  5.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.110631942749023\n",
            "haditonlyd\n",
            "[18, 63, 34, 62, 31, 63, 39, 6, 28, 38]\n",
            "thounulahe\n",
            "[26, 50, 24, 14, 31, 63, 39, 49, 31, 62]\n",
            "pritiotere\n",
            "[26, 41, 62, 31, 62, 63, 31, 54, 41, 62]\n",
            "lireormoge\n",
            "[34, 54, 41, 62, 63, 5, 45, 63, 39, 62]\n",
            "abolicapop\n",
            "[63, 39, 63, 39, 62, 35, 63, 39, 24, 16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:10, 14.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.95294189453125\n",
            "divabtiala\n",
            "[34, 62, 34, 63, 39, 31, 62, 63, 39, 49]\n",
            "nanelnevcu\n",
            "[18, 63, 22, 63, 39, 34, 62, 46, 35, 14]\n",
            "sidiUmalel\n",
            "[44, 62, 34, 62, 63, 45, 63, 39, 62, 39]\n",
            "nalitickmp\n",
            "[18, 63, 39, 62, 31, 62, 35, 19, 45, 16]\n",
            "bousistore\n",
            "[18, 24, 14, 46, 62, 46, 31, 54, 41, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:06<00:07, 13.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.891273498535156\n",
            "phtflatial\n",
            "[26, 50, 31, 2, 6, 49, 31, 62, 63, 39]\n",
            "oncreliaph\n",
            "[63, 22, 35, 41, 63, 39, 62, 24, 16, 50]\n",
            "Thotwusheu\n",
            "[26, 50, 63, 31, 37, 62, 46, 50, 63, 22]\n",
            "tysmizusSr\n",
            "[31, 54, 5, 45, 62, 35, 14, 46, 26, 41]\n",
            "scelathyme\n",
            "[44, 35, 63, 39, 49, 31, 50, 28, 45, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|  | 151/208 [00:10<00:04, 12.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.020217895507812\n",
            "Runtearent\n",
            "[18, 15, 22, 31, 54, 49, 41, 63, 22, 38]\n",
            "eundanevil\n",
            "[51, 15, 22, 38, 63, 34, 62, 34, 63, 39]\n",
            "outthredie\n",
            "[24, 14, 46, 31, 50, 41, 62, 34, 62, 63]\n",
            "rurdisnern\n",
            "[41, 14, 39, 34, 62, 46, 0, 54, 5, 0]\n",
            "Mativavnfe\n",
            "[18, 63, 31, 62, 34, 63, 34, 0, 2, 54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 13.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.61150360107422\n",
            "kuoprymoku\n",
            "[34, 54, 24, 16, 41, 62, 34, 63, 31, 14]\n",
            "Atollesaro\n",
            "[21, 31, 63, 39, 6, 54, 45, 49, 41, 24]\n",
            "covoKolype\n",
            "[35, 63, 34, 63, 39, 24, 32, 28, 16, 54]\n",
            "teroninwys\n",
            "[18, 54, 41, 24, 45, 62, 22, 31, 28, 46]\n",
            "ghoussthor\n",
            "[26, 50, 24, 14, 5, 46, 31, 50, 54, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:14<00:00, 14.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.6522216796875\n",
            "vedgydrero\n",
            "[34, 54, 5, 32, 28, 29, 41, 54, 41, 24]\n",
            "========= Results: epoch 4 of 10 =========\n",
            "train loss: 23.89| valid loss: 23.85\n",
            "\n",
            "========= Epoch 5 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:38,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.433408737182617\n",
            "borsumphal\n",
            "[18, 63, 5, 43, 14, 45, 16, 50, 63, 22]\n",
            "hancorhate\n",
            "[18, 63, 22, 35, 63, 31, 50, 63, 46, 62]\n",
            "bastymangi\n",
            "[48, 49, 46, 31, 28, 45, 63, 22, 38, 62]\n",
            "barnonsour\n",
            "[18, 63, 22, 34, 63, 22, 31, 24, 14, 41]\n",
            "tistJuildo\n",
            "[27, 62, 46, 31, 41, 20, 62, 39, 55, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:10, 15.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.753307342529297\n",
            "umayistedy\n",
            "[15, 45, 49, 37, 62, 46, 31, 54, 38, 28]\n",
            "poloteatyp\n",
            "[18, 63, 39, 63, 31, 62, 46, 31, 28, 16]\n",
            "inralearir\n",
            "[15, 22, 41, 63, 39, 62, 63, 22, 62, 46]\n",
            "rasciveopy\n",
            "[18, 63, 46, 31, 62, 34, 62, 24, 16, 28]\n",
            "Maumeniabr\n",
            "[18, 63, 15, 45, 63, 22, 62, 63, 48, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:07<00:07, 13.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.89297866821289\n",
            "ochigousta\n",
            "[63, 35, 50, 62, 53, 24, 14, 46, 31, 63]\n",
            "Aspresledn\n",
            "[21, 44, 16, 41, 54, 5, 6, 54, 29, 0]\n",
            "vursanuido\n",
            "[34, 54, 5, 44, 63, 31, 20, 62, 34, 63]\n",
            "subemohrea\n",
            "[44, 20, 10, 54, 45, 63, 50, 41, 62, 63]\n",
            "sulogryort\n",
            "[44, 20, 39, 24, 32, 41, 62, 63, 22, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:10<00:03, 14.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.550996780395508\n",
            "ceustibral\n",
            "[35, 63, 14, 46, 31, 62, 48, 41, 63, 39]\n",
            "bortabastu\n",
            "[48, 63, 22, 31, 63, 48, 63, 46, 31, 12]\n",
            "elfucthteo\n",
            "[63, 39, 3, 14, 46, 31, 50, 31, 62, 63]\n",
            "inendesmic\n",
            "[15, 22, 62, 22, 34, 54, 5, 45, 62, 35]\n",
            "alanepyler\n",
            "[63, 39, 63, 34, 63, 16, 28, 6, 54, 5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 14.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.570589065551758\n",
            "wyencrivil\n",
            "[18, 61, 63, 22, 35, 41, 62, 34, 62, 6]\n",
            "psedmegdsi\n",
            "[26, 42, 54, 5, 45, 62, 53, 56, 30, 62]\n",
            "untesIidan\n",
            "[15, 22, 31, 54, 5, 18, 62, 34, 63, 22]\n",
            "pofficalle\n",
            "[18, 63, 11, 2, 62, 35, 63, 39, 6, 54]\n",
            "proorinaom\n",
            "[26, 41, 24, 47, 34, 62, 22, 63, 1, 45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:13<00:00, 14.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.552005767822266\n",
            "weckraisne\n",
            "[41, 62, 35, 19, 41, 63, 62, 46, 0, 54]\n",
            "========= Results: epoch 5 of 10 =========\n",
            "train loss: 23.75| valid loss: 23.74\n",
            "\n",
            "========= Epoch 6 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:38,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.48934555053711\n",
            "dioserespe\n",
            "[27, 62, 24, 42, 54, 41, 62, 46, 31, 54]\n",
            "rospkitest\n",
            "[41, 24, 44, 16, 31, 62, 31, 62, 46, 31]\n",
            "tiitaliDet\n",
            "[26, 50, 62, 35, 63, 39, 62, 34, 62, 31]\n",
            "cruamosril\n",
            "[26, 41, 20, 63, 45, 63, 31, 41, 62, 39]\n",
            "toledetrim\n",
            "[26, 63, 39, 62, 34, 49, 31, 41, 62, 45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:10, 14.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.70694351196289\n",
            "aptupeedis\n",
            "[63, 16, 31, 20, 10, 0, 54, 56, 62, 46]\n",
            "otretionem\n",
            "[63, 31, 41, 62, 31, 62, 63, 22, 1, 45]\n",
            "dremoustis\n",
            "[26, 41, 1, 45, 24, 14, 46, 31, 62, 46]\n",
            "Isleropham\n",
            "[21, 46, 6, 54, 41, 24, 16, 6, 12, 45]\n",
            "relsesdicu\n",
            "[41, 62, 39, 42, 54, 5, 34, 62, 35, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:06<00:07, 14.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.88528823852539\n",
            "ingifelasi\n",
            "[15, 22, 38, 62, 2, 54, 6, 63, 46, 62]\n",
            "soerealise\n",
            "[18, 63, 54, 22, 31, 63, 39, 62, 46, 54]\n",
            "Tinchiollo\n",
            "[26, 15, 22, 35, 50, 62, 63, 39, 6, 24]\n",
            "phelluletr\n",
            "[26, 50, 62, 39, 6, 20, 39, 62, 31, 41]\n",
            "dirarraust\n",
            "[27, 62, 34, 63, 5, 41, 63, 14, 46, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:10<00:03, 14.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.371965408325195\n",
            "ennineacle\n",
            "[15, 22, 34, 62, 22, 1, 49, 46, 6, 62]\n",
            "rachersane\n",
            "[18, 63, 35, 50, 54, 5, 44, 63, 34, 54]\n",
            "untidatoph\n",
            "[15, 22, 31, 62, 34, 49, 31, 63, 16, 50]\n",
            "imalyshins\n",
            "[15, 45, 49, 17, 62, 46, 50, 62, 22, 46]\n",
            "tretreicac\n",
            "[26, 41, 62, 31, 41, 1, 62, 35, 63, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 14.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.569175720214844\n",
            "whraunxier\n",
            "[26, 50, 41, 63, 15, 22, 45, 62, 63, 5]\n",
            "untmuskiva\n",
            "[15, 22, 31, 24, 14, 46, 31, 62, 34, 49]\n",
            "nycatelyla\n",
            "[18, 28, 35, 63, 31, 54, 6, 28, 39, 49]\n",
            "frassaster\n",
            "[26, 41, 63, 5, 44, 1, 46, 31, 54, 5]\n",
            "Sitiausklo\n",
            "[18, 62, 31, 62, 63, 14, 46, 31, 6, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:13<00:00, 14.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.102989196777344\n",
            "hanalobNus\n",
            "[18, 63, 22, 63, 39, 63, 10, 9, 14, 46]\n",
            "========= Results: epoch 6 of 10 =========\n",
            "train loss: 23.65| valid loss: 23.65\n",
            "\n",
            "========= Epoch 7 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:39,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.449460983276367\n",
            "anroidienr\n",
            "[15, 22, 41, 24, 47, 56, 62, 63, 22, 41]\n",
            "privesmyti\n",
            "[26, 41, 62, 34, 54, 5, 45, 62, 31, 62]\n",
            "biphopterp\n",
            "[48, 62, 16, 50, 63, 16, 31, 54, 5, 16]\n",
            "aridurwhad\n",
            "[63, 39, 62, 34, 54, 5, 43, 50, 63, 34]\n",
            "paterbalal\n",
            "[26, 63, 31, 54, 5, 25, 63, 39, 63, 39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:11, 13.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.892345428466797\n",
            "tripessuti\n",
            "[26, 41, 1, 16, 54, 5, 44, 14, 31, 62]\n",
            "seagesichr\n",
            "[44, 1, 49, 38, 54, 5, 62, 35, 50, 41]\n",
            "Cssomatrei\n",
            "[63, 5, 44, 24, 45, 49, 31, 41, 1, 62]\n",
            "messiayyio\n",
            "[18, 1, 46, 31, 62, 63, 23, 36, 59, 12]\n",
            "eperivossc\n",
            "[1, 51, 54, 5, 62, 34, 54, 5, 46, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:07<00:07, 14.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.258758544921875\n",
            "psereenusm\n",
            "[26, 42, 54, 41, 1, 63, 22, 24, 46, 45]\n",
            "peatolerst\n",
            "[18, 1, 63, 31, 54, 6, 54, 5, 46, 31]\n",
            "phaloulous\n",
            "[26, 50, 63, 39, 24, 14, 39, 24, 14, 46]\n",
            "suilineshi\n",
            "[44, 20, 62, 39, 62, 22, 62, 46, 50, 62]\n",
            "acksthalil\n",
            "[63, 35, 19, 46, 31, 50, 63, 39, 62, 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:10<00:04, 13.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.47913932800293\n",
            "ingcreessl\n",
            "[15, 22, 38, 54, 41, 1, 54, 5, 44, 6]\n",
            "ormphypler\n",
            "[63, 5, 45, 16, 50, 28, 16, 6, 54, 5]\n",
            "ivicogypus\n",
            "[47, 34, 62, 35, 24, 32, 28, 16, 14, 46]\n",
            "susridiner\n",
            "[44, 20, 5, 41, 62, 34, 62, 34, 54, 5]\n",
            "stridolric\n",
            "[46, 31, 41, 62, 56, 63, 39, 41, 62, 35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 14.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.940227508544922\n",
            "liristodit\n",
            "[18, 62, 39, 62, 46, 31, 63, 39, 62, 31]\n",
            "trouscless\n",
            "[26, 41, 24, 14, 46, 35, 6, 54, 5, 44]\n",
            "Uutympangl\n",
            "[21, 14, 31, 12, 45, 16, 63, 22, 38, 6]\n",
            "bigalpKync\n",
            "[44, 62, 53, 63, 39, 16, 50, 62, 22, 35]\n",
            "q-erseguia\n",
            "[44, 6, 54, 5, 44, 1, 53, 20, 62, 63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:14<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.485172271728516\n",
            "bilouleble\n",
            "[48, 62, 39, 24, 14, 39, 62, 48, 6, 54]\n",
            "========= Results: epoch 7 of 10 =========\n",
            "train loss: 23.55| valid loss: 23.55\n",
            "\n",
            "========= Epoch 8 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.490684509277344\n",
            "rivistevio\n",
            "[17, 62, 34, 62, 46, 31, 62, 34, 62, 63]\n",
            "proosteoce\n",
            "[26, 41, 24, 14, 46, 31, 62, 63, 35, 63]\n",
            "pantatesti\n",
            "[18, 63, 22, 31, 49, 31, 63, 46, 31, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|         | 3/208 [00:00<00:20,  9.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deywhonbuc\n",
            "[38, 54, 7, 43, 50, 63, 22, 51, 14, 46]\n",
            "enentlembr\n",
            "[21, 34, 54, 22, 31, 6, 54, 45, 51, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:10, 14.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.670957565307617\n",
            "blivobersh\n",
            "[48, 6, 62, 34, 54, 10, 54, 5, 43, 50]\n",
            "tryoullear\n",
            "[26, 41, 28, 24, 14, 39, 6, 1, 63, 5]\n",
            "Trocionerc\n",
            "[26, 41, 1, 35, 62, 24, 0, 54, 5, 31]\n",
            "Pohimurgss\n",
            "[26, 63, 50, 62, 45, 14, 22, 38, 46, 31]\n",
            "ragharsedl\n",
            "[41, 49, 38, 50, 63, 5, 45, 54, 56, 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:07<00:07, 14.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.539636611938477\n",
            "sednatkess\n",
            "[46, 54, 29, 0, 24, 35, 19, 54, 5, 45]\n",
            "vilictaaid\n",
            "[34, 62, 17, 62, 46, 31, 49, 17, 62, 56]\n",
            "isterricab\n",
            "[21, 46, 31, 54, 5, 41, 62, 35, 63, 4]\n",
            "Crapeloror\n",
            "[26, 41, 1, 18, 63, 39, 24, 41, 24, 41]\n",
            "sangsumosi\n",
            "[44, 63, 22, 38, 46, 12, 45, 24, 42, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:10<00:03, 14.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.555219650268555\n",
            "odcancuram\n",
            "[47, 56, 35, 63, 22, 31, 54, 41, 62, 45]\n",
            "sthistonio\n",
            "[46, 31, 50, 62, 46, 31, 63, 37, 62, 63]\n",
            "cochoginip\n",
            "[35, 24, 35, 50, 24, 32, 62, 22, 1, 16]\n",
            "stethority\n",
            "[46, 31, 62, 31, 50, 63, 5, 62, 31, 23]\n",
            "luperseuth\n",
            "[6, 1, 16, 54, 5, 42, 54, 5, 46, 50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 14.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.617158889770508\n",
            "peniollala\n",
            "[26, 63, 22, 62, 63, 39, 6, 63, 39, 1]\n",
            "medlsousol\n",
            "[18, 1, 56, 59, 42, 24, 14, 46, 63, 39]\n",
            "acanguisti\n",
            "[21, 35, 63, 22, 38, 20, 62, 46, 31, 62]\n",
            "grartiaxsu\n",
            "[26, 41, 63, 5, 31, 62, 63, 22, 44, 20]\n",
            "Jonissluda\n",
            "[18, 24, 37, 62, 46, 44, 6, 12, 56, 59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:14<00:00, 14.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.595508575439453\n",
            "Flintroplo\n",
            "[26, 6, 62, 22, 31, 41, 24, 16, 6, 24]\n",
            "========= Results: epoch 8 of 10 =========\n",
            "train loss: 23.46| valid loss: 23.47\n",
            "\n",
            "========= Epoch 9 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.8101749420166\n",
            "ecosterioE\n",
            "[21, 35, 63, 46, 31, 54, 41, 62, 63, 15]\n",
            "tantedrahv\n",
            "[26, 63, 22, 31, 49, 29, 41, 63, 39, 34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|         | 3/208 [00:00<00:21,  9.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oxhichocyl\n",
            "[15, 22, 50, 62, 35, 50, 24, 35, 28, 39]\n",
            "cksugricom\n",
            "[35, 19, 44, 20, 53, 41, 1, 35, 24, 45]\n",
            "braposomer\n",
            "[26, 41, 1, 16, 24, 44, 24, 45, 54, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:10, 14.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.47339630126953\n",
            "Momanwlvea\n",
            "[18, 24, 45, 63, 22, 25, 39, 34, 54, 49]\n",
            "honstednes\n",
            "[18, 63, 22, 46, 31, 54, 29, 0, 54, 5]\n",
            "glodronous\n",
            "[38, 6, 24, 32, 41, 63, 22, 24, 14, 42]\n",
            "centylynet\n",
            "[35, 63, 22, 31, 23, 6, 13, 37, 49, 31]\n",
            "apbrosthae\n",
            "[21, 10, 51, 41, 63, 46, 31, 41, 63, 40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:07<00:07, 14.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.555282592773438\n",
            "phaanachel\n",
            "[26, 50, 63, 15, 22, 1, 35, 50, 62, 6]\n",
            "rQoniferai\n",
            "[10, 55, 24, 45, 62, 2, 54, 41, 1, 62]\n",
            "Dousereste\n",
            "[18, 24, 14, 42, 54, 41, 1, 46, 31, 54]\n",
            "gethterede\n",
            "[18, 1, 31, 50, 31, 54, 5, 54, 56, 63]\n",
            "sphaincone\n",
            "[44, 16, 50, 63, 15, 22, 35, 63, 37, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:10<00:03, 14.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.41158103942871\n",
            "unilallyhe\n",
            "[15, 22, 14, 39, 49, 39, 6, 28, 50, 54]\n",
            "unajuiatin\n",
            "[15, 22, 1, 44, 20, 62, 49, 31, 62, 22]\n",
            "eizictatri\n",
            "[40, 47, 34, 62, 46, 31, 49, 31, 41, 1]\n",
            "nepoldicur\n",
            "[18, 1, 16, 63, 39, 56, 62, 35, 63, 22]\n",
            "Ulisiveran\n",
            "[21, 39, 62, 46, 62, 34, 54, 41, 63, 22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 14.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.691513061523438\n",
            "eblifermpc\n",
            "[21, 39, 6, 62, 34, 54, 5, 45, 16, 31]\n",
            "chronicart\n",
            "[35, 50, 41, 63, 37, 62, 35, 63, 39, 31]\n",
            "dalledyunt\n",
            "[18, 63, 39, 6, 1, 32, 28, 15, 22, 31]\n",
            "nydlycelen\n",
            "[18, 28, 29, 6, 28, 35, 63, 39, 62, 37]\n",
            "beurnarest\n",
            "[51, 63, 14, 5, 37, 49, 17, 62, 46, 31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:14<00:00, 14.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.183727264404297\n",
            "saisubomen\n",
            "[18, 1, 40, 44, 20, 10, 54, 45, 62, 22]\n",
            "========= Results: epoch 9 of 10 =========\n",
            "train loss: 23.39| valid loss: 23.41\n",
            "\n",
            "========= Epoch 10 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:39,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.200057983398438\n",
            "paansuezop\n",
            "[26, 63, 63, 22, 44, 20, 54, 55, 24, 16]\n",
            "pslerconst\n",
            "[26, 42, 6, 54, 5, 35, 63, 22, 46, 31]\n",
            "quatedreba\n",
            "[44, 20, 49, 31, 54, 29, 41, 1, 51, 49]\n",
            "heventners\n",
            "[18, 1, 34, 54, 22, 31, 0, 54, 5, 42]\n",
            "sysersuftr\n",
            "[18, 28, 42, 54, 5, 44, 20, 11, 31, 41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|       | 53/208 [00:03<00:10, 14.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.22366714477539\n",
            "amegablami\n",
            "[21, 45, 62, 53, 49, 48, 6, 1, 45, 62]\n",
            "meterttigh\n",
            "[18, 63, 31, 54, 5, 46, 31, 62, 53, 50]\n",
            "sitesshero\n",
            "[18, 63, 31, 54, 5, 43, 50, 54, 41, 24]\n",
            "fatmicocic\n",
            "[18, 1, 46, 45, 62, 35, 63, 46, 62, 46]\n",
            "plesalalin\n",
            "[26, 6, 1, 46, 63, 39, 49, 39, 62, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|     | 103/208 [00:07<00:07, 14.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.44876480102539\n",
            "ricobochce\n",
            "[41, 62, 35, 24, 4, 24, 35, 50, 31, 62]\n",
            "sfosetrica\n",
            "[44, 3, 14, 42, 62, 31, 17, 62, 35, 63]\n",
            "cluifynipp\n",
            "[35, 6, 20, 62, 2, 13, 22, 1, 16, 16]\n",
            "altogronir\n",
            "[21, 39, 31, 54, 32, 41, 63, 22, 62, 22]\n",
            "lutatilapi\n",
            "[18, 1, 31, 49, 31, 62, 39, 49, 31, 62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|  | 153/208 [00:10<00:03, 14.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.105030059814453\n",
            "ponoangrop\n",
            "[16, 63, 22, 1, 49, 22, 38, 41, 24, 16]\n",
            "sksteriker\n",
            "[44, 19, 46, 31, 54, 41, 1, 19, 54, 5]\n",
            "paronenkut\n",
            "[26, 63, 39, 24, 45, 63, 22, 31, 14, 31]\n",
            "cassnessay\n",
            "[35, 63, 46, 42, 0, 54, 5, 42, 49, 13]\n",
            "lypicontil\n",
            "[18, 28, 16, 62, 35, 63, 22, 31, 62, 39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|| 203/208 [00:13<00:00, 13.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.19791603088379\n",
            "muloashopi\n",
            "[18, 14, 39, 24, 49, 46, 50, 24, 16, 62]\n",
            "didgraster\n",
            "[27, 62, 56, 38, 41, 1, 46, 31, 54, 5]\n",
            "sadynatera\n",
            "[18, 1, 56, 13, 37, 49, 31, 54, 41, 1]\n",
            "vondentaNh\n",
            "[25, 63, 22, 38, 54, 22, 31, 49, 43, 50]\n",
            "Gadatitcyc\n",
            "[18, 1, 56, 63, 31, 62, 46, 35, 28, 35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 208/208 [00:13<00:00, 14.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.30792999267578\n",
            "unkerslomi\n",
            "[15, 22, 34, 54, 5, 42, 6, 1, 45, 62]\n",
            "========= Results: epoch 10 of 10 =========\n",
            "train loss: 23.35| valid loss: 23.37\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zymBj9QrDHRM"
      },
      "source": [
        "You may wish to try different values of $N$ and see what the impact on sample quality is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auBibYUTtIom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c411c4e-b0a0-48de-fc60-b55f1038e7e5"
      },
      "source": [
        "x = torch.tensor(encode(\"quack\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T))\n",
        "\n",
        "x = torch.tensor(encode(\"quick\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T))\n",
        "\n",
        "x = torch.tensor(encode(\"qurck\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T)) # should have lower probability---in English only vowels follow \"qu\"\n",
        "\n",
        "x = torch.tensor(encode(\"qiick\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T)) # should have lower probability---in English only \"u\" follows \"q\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([[44, 20, 49, 35, 19]], tensor([[-15.1298]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[44, 20, 62, 35, 19]], tensor([[-12.7714]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[44, 20, 10, 35, 19]], tensor([[-14.3361]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[44, 1, 62, 35, 19]], tensor([[-18.7059]], device='cuda:0', grad_fn=<GatherBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward-Backward Algoritm"
      ],
      "metadata": {
        "id": "aaYiIO8AgCLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HMM_backward(self, x, T, save_log_beta=True):\n",
        "  \"\"\"\n",
        "  x : IntTensor of shape (batch size, T_max)\n",
        "  T : IntTensor of shape (batch size)\n",
        "\n",
        "  Compute backward log p(x) for each example in the batch.\n",
        "  T = length of each example\n",
        "  \"\"\"\n",
        "  if self.is_cuda:\n",
        "  \tx = x.cuda()\n",
        "  \tT = T.cuda()\n",
        "\n",
        "  batch_size = x.shape[0]; T_max = x.shape[1] - 1\n",
        "  gather_indexes = torch.zeros((batch_size,1), dtype=torch.int64)\n",
        "  if self.is_cuda:\n",
        "    gather_indexes = gather_indexes.cuda()\n",
        "  #log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "  log_beta = torch.zeros(batch_size, T_max+1, self.N) # table (sample, t, state) containing log probability of observations from sample from time t+1 to T_max and being in state (in time t)\n",
        "  if self.is_cuda: log_beta = log_beta.cuda()\n",
        "\n",
        "  log_beta[:, T_max, :] = 1 #self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
        "  for t in range(T_max-1, 0-1, -1):\n",
        "    log_beta[:, t, :] = self.emission_model(x[:,t+1]) + self.transition_model.back(log_beta[:, t+1, :])\n",
        "\n",
        "  if save_log_beta:\n",
        "    self.log_beta = log_beta\n",
        "    self.x = x\n",
        "\n",
        "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "  termination = self.emission_model(x[:,0]) + log_state_priors + log_beta[:, 0, :]\n",
        "\n",
        "  log_sums = log_beta.logsumexp(dim=2)\n",
        "  #print(\"log_sums\\n\", log_sums)\n",
        "  #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
        "  #log_probs = torch.gather(log_sums, 1, gather_indexes)\n",
        "  log_probs = termination.logsumexp(dim=1)\n",
        "  return log_probs\n",
        "\n",
        "def transition_model_backward(self, log_beta):\n",
        "  \"\"\"\n",
        "  log_alpha : Tensor of shape (batch size, N)\n",
        "  Multiply previous timestep's alphas by transition matrix (in log domain)\n",
        "  \"\"\"\n",
        "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "  # Matrix multiplication in the log domain\n",
        "  #out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
        "  out = log_domain_matmul(log_transition_matrix.transpose(0,1), log_beta.transpose(0,1)).transpose(0,1)\n",
        "  return out\n",
        "\n",
        "TransitionModel.back = transition_model_backward\n",
        "HMM.back = HMM_backward\n"
      ],
      "metadata": {
        "id": "Oef1rllHhFRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = HMM(2, [distrib.Normal(-1,1), distrib.Normal(1, 1)])"
      ],
      "metadata": {
        "id": "OJ-JENA0QaLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, states = test_model.sample(30)"
      ],
      "metadata": {
        "id": "_TVMkxUpRWj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T =torch.Tensor([10,10,10], dtype= torch.int64)\n",
        "T = torch.ones([1,3], dtype=torch.int64)*10\n",
        "T.view(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdpQj4ZpRZSe",
        "outputId": "4678a3f8-de0b-4f54-d65a-489ab4351d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10],\n",
              "        [10],\n",
              "        [10]])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(torch.reshape(x, (3, 10)), T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh8th1MWRba2",
        "outputId": "1609ccea-1d76-448b-d0a5-2bae8ff5b652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.5572, -0.2366], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-13.8154],\n",
              "        [-18.5195],\n",
              "        [-14.0782]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.back(torch.reshape(x, (3, 10)), T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQq71SzLRdTW",
        "outputId": "5fb7de16-bdfd-420f-9fd7-47bcbf6e17d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-12.6863, -17.7177, -12.9610], device='cuda:0',\n",
              "       grad_fn=<LogsumexpBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward(self, x, T):\n",
        "    pass"
      ],
      "metadata": {
        "id": "I8koB5argH3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eZeQXWjhDev"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "HMMs used to be very popular in natural language processing, but they have largely been overshadowed by neural network models like RNNs and Transformers. Still, it is fun and instructive to study the HMM; some commonly used machine learning techniques like [Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf) are inspired by HMM methods. HMMs are [still used in conjunction with neural networks in speech recognition](https://arxiv.org/abs/1811.07453), where the assumption of a one-hot state makes sense for modelling phonemes, which are spoken one at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXQOBz5zqe10"
      },
      "source": [
        "## Acknowledgments\n",
        "\n",
        "This notebook is based partly on Lawrence Rabiner's excellent article \"[A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf)\", which you may also like to check out. Thanks also to Dima Serdyuk and Kyle Gorman for their feedback on the draft."
      ]
    }
  ]
}