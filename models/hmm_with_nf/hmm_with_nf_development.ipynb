{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b7f017",
   "metadata": {
    "id": "55b7f017"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd5e3a",
   "metadata": {
    "id": "e5cd5e3a"
   },
   "source": [
    "## Normalizing Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135f759d",
   "metadata": {
    "id": "135f759d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distrib\n",
    "import torch.distributions.transforms as transform\n",
    "from torch.distributions import constraints\n",
    "import torch.optim as optim\n",
    "import torch.utils.data \n",
    "\n",
    "# Imports for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Flow(nn.Module):\n",
    "\n",
    "    def invert(self, y):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # def log_abs_det_jacobian(self, x, y):\n",
    "    #     raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x, log_det_jacob):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __init__(self, event_dim = 1):\n",
    "        # transform.Transform.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "        self._event_dim = event_dim\n",
    "\n",
    "    # Init all parameters\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "    # Hacky hash bypass\n",
    "    def __hash__(self):\n",
    "        return nn.Module.__hash__(self)\n",
    "\n",
    "    @property\n",
    "    def event_dim(self):\n",
    "        return self._event_dim\n",
    "\n",
    "    @constraints.dependent_property(is_discrete=False)\n",
    "    def domain(self):\n",
    "        if self.event_dim == 0:\n",
    "            return constraints.real\n",
    "        return constraints.independent(constraints.real, self.event_dim)\n",
    "\n",
    "    @constraints.dependent_property(is_discrete=False)\n",
    "    def codomain(self):\n",
    "        if self.event_dim == 0:\n",
    "            return constraints.real\n",
    "        return constraints.independent(constraints.real, self.event_dim)\n",
    "\n",
    "class NormalizingFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, blocks, flow_length, base_distrib, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        biject = []\n",
    "        for f in range(flow_length):\n",
    "            for b_flow in blocks:\n",
    "                biject.append(b_flow(dim, self.device))\n",
    "        # self.transforms = transform.ComposeTransform(biject)\n",
    "        self.bijectors = nn.ModuleList(biject)\n",
    "        self.base_distrib = base_distrib\n",
    "        # self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
    "        # self.log_det = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applies series of flows\n",
    "        log_det_jacob = torch.zeros((x.shape[0]), device = self.device, dtype = torch.float32)\n",
    "        for b in range(len(self.bijectors)):\n",
    "            x, log_det_jacob = self.bijectors[b](x, log_det_jacob)\n",
    "        return x, log_det_jacob\n",
    "\n",
    "    def invert(self, z):\n",
    "        for layer in reversed(self.bijectors):\n",
    "            z = layer.invert(z)\n",
    "        return z\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        y, log_det_jacob = self(x)\n",
    "        return self.base_distrib.log_prob(y) + log_det_jacob\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        z = self.base_distrib.sample(sample_shape)\n",
    "        z = z.to(device)\n",
    "        return torch.squeeze(self.invert(z[None,:]), dim=0)\n",
    "\n",
    "    # def sample(self):\n",
    "    #     # dodać cude\n",
    "    #     with torch.no_grad():\n",
    "    #         base_dens_samples = self.base_density.sample()\n",
    "    #         out_samples, _ = self.forward(base_dens_samples)\n",
    "    #     return out_samples\n",
    "\n",
    "    # def log_prob(self, y):\n",
    "    #     #dodać cude\n",
    "    #     log_det_reversed_order = []\n",
    "    #     for b in reversed(range(len(self.bijectors))):\n",
    "    #         log_det_reversed_order.append(self.bijectors[b].log_abs_det_jacobian(y))\n",
    "    #         y = self.bijectors[b]._inverse(y)\n",
    "    #         print(y)\n",
    "    #     log_prob_base = self.base_density.log_prob(y)\n",
    "    #     jacobian_part = torch.sum(torch.stack(log_det_reversed_order))\n",
    "    #     return log_prob_base - jacobian_part\n",
    "\n",
    "class CouplingFlow(Flow):\n",
    "    def __init__(self, dim, device, n_hidden=128, n_layers=3, activation=nn.ReLU, last_s_activation = nn.Tanh):\n",
    "        super(CouplingFlow, self).__init__()\n",
    "        self.k = dim // 2\n",
    "        self.t = self.t_transform_net(self.k, self.k, n_hidden, n_layers, activation)\n",
    "        self.s = self.s_transform_net(self.k, self.k, n_hidden, n_layers, activation, last_s_activation)\n",
    "        self.device = device\n",
    "        self.dim = dim\n",
    "        # self.register_buffer(\"mask\",torch.cat((torch.ones(self.k), torch.zeros(self.dim - self.k))).detach())\n",
    "        self.init_parameters()\n",
    "        # self.bijective = True\n",
    "\n",
    "\n",
    "    def t_transform_net(self, n_in, n_out, n_hidden, n_layer, activation):\n",
    "        net = nn.ModuleList()\n",
    "        for l in range(n_layer):\n",
    "            module = nn.Linear(l == 0 and n_in or n_hidden, n_hidden)\n",
    "            # module.weight.data.uniform_(-1, 1)\n",
    "            net.append(module)\n",
    "            net.append(activation())\n",
    "            if l == n_layer -1:\n",
    "                module = nn.Linear(n_hidden, n_out)\n",
    "                net.append(module)\n",
    "        return nn.Sequential(*net)\n",
    "\n",
    "    def s_transform_net(self, n_in, n_out, n_hidden, n_layer, activation, last_s_activation):\n",
    "        net = nn.ModuleList()\n",
    "        for l in range(n_layer):\n",
    "            module = nn.Linear((l == 0 and n_in) or n_hidden, l == n_layer - 1 and n_out or n_hidden)\n",
    "            # module.weight.data.uniform_(-1, 1)\n",
    "            net.append(module)\n",
    "            net.append((l == n_layer - 1 and last_s_activation()) or activation())\n",
    "        return nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x, log_det_jacob):\n",
    "        x_k = x[:, 0:self.k]\n",
    "        xp_D = x[:, self.k:self.dim] * torch.exp(self.s(x_k)) + self.t(x_k)\n",
    "        # print(x_k.shape)\n",
    "        log_det_jacob += torch.sum(torch.abs(self.s(x_k)), dim = 1)\n",
    "        # xp_D = x * self.g_sig(x_k) + self.g_mu(x_k)\n",
    "\n",
    "        return torch.cat((x_k, xp_D), dim=1), log_det_jacob\n",
    "\n",
    "\n",
    "    def invert(self, y):\n",
    "        yp_k = y[:, 0:self.k]\n",
    "        y_D = (y[:, self.k:self.dim] - self.t(yp_k)) * torch.exp(-self.s(yp_k))\n",
    "        # y_D = (((1 - self.mask) * y) - (1 - self.mask) * (self.g_mu(yp_k)) / self.g_sig(yp_k))\n",
    "\n",
    "        return torch.cat((yp_k, y_D), dim=1)\n",
    "\n",
    "\n",
    "class ReverseFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim, device):\n",
    "        super(ReverseFlow, self).__init__()\n",
    "        # k = dim // 2\n",
    "        # self.permute = torch.cat((torch.arange(dim, k, -1),torch.arange(1, k+1, 1)))\n",
    "        self.permute = torch.arange(dim-1, -1, -1)\n",
    "        self.inverse = torch.argsort(self.permute)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, log_det_jacob):\n",
    "        return x[:, self.permute] , log_det_jacob\n",
    "\n",
    "    def invert(self, z):\n",
    "        return z[:, self.inverse]\n",
    "\n",
    "\n",
    "def nll_loss(y, log_det_jacob, base_distrib):\n",
    "    log_likelihood = base_distrib.log_prob(y) + log_det_jacob\n",
    "    return -torch.mean(log_likelihood)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "def nll_loss(y, log_det_jacob, base_distrib):\n",
    "    log_likelihood = base_distrib.log_prob(y) + log_det_jacob\n",
    "    return -torch.mean(log_likelihood)\n",
    "\n",
    "def train_flow(flow, data_loader, loss, optimizer, scheduler, device, epochs=10001, plot_it=1000, batch_size = 64):\n",
    "    base_distrib = flow.base_distrib\n",
    "    flow.to(device)\n",
    "    # ims = []\n",
    "    #fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 7))\n",
    "    # fig = plt.figure(figsize=(10, 10))\n",
    "    # id_figure=2\n",
    "    # plt.figure(figsize=(16, 18))\n",
    "    # subplot_num = epochs // plot_it\n",
    "    # plt.subplot((subplot_num//4) + 1,4,1)\n",
    "    # plt.hexbin(z[:,0], z[:,1], C=target_density(torch.Tensor(z)).numpy().squeeze(), cmap='rainbow')\n",
    "    # plt.title('Target density', fontsize=15);\n",
    "    # Main optimization loop\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Training epoch {epoch + 1} ...\")\n",
    "        loss_acc = 0.0\n",
    "        flow.train()\n",
    "\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            batch = batch.to(device)\n",
    "            zk, log_det_jacob = flow(batch)\n",
    "            loss_n = loss(zk, log_det_jacob, base_distrib)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_n.backward()\n",
    "            # Do the step of optimizer\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # Gather data and report\n",
    "            # loss_acc += loss_n.item() * len(batch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            flow.eval()\n",
    "            with torch.no_grad():\n",
    "                z = base_distrib.sample((1000,))\n",
    "                x = flow.invert(z).cpu()\n",
    "                plt.scatter(x[:, 0], x[:, 1])\n",
    "                plt.xlim(-5, 5)\n",
    "                plt.ylim(-5, 5)\n",
    "                plt.title(f\"Epoch: {epoch + 1} nll loss: {loss_n:.4f}\")\n",
    "                plt.show()\n",
    "\n",
    "# dataset\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "batch_size=256\n",
    "\n",
    "class MoonDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, device, lenght=1000):\n",
    "        self.device = device\n",
    "        self.lenght = lenght\n",
    "        self.generate_moons(lenght)\n",
    "\n",
    "    def generate_moons(self, lenght):\n",
    "        moons_data = make_moons(lenght, noise=0.05)[0].astype(\"float32\")\n",
    "        self.moons = moons_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.lenght\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t = torch.tensor(self.moons[index], device = device)\n",
    "        return (t)\n",
    "        \n",
    "my_moons = MoonDataset(device, 5000)\n",
    "\n",
    "moons_dataloader = torch.utils.data.DataLoader(my_moons, batch_size=batch_size,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2ed13",
   "metadata": {
    "id": "49f2ed13"
   },
   "source": [
    "## Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c7dd531",
   "metadata": {
    "id": "4c7dd531"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class HMM(torch.nn.Module):\n",
    "#   \"\"\"\n",
    "#   Hidden Markov Model with discrete observations.\n",
    "#   \"\"\"\n",
    "    def __init__(self, N, distributions, transition_matrix=None, state_priors='uniform', device=device):\n",
    "        super(HMM, self).__init__()\n",
    "        #self.M = M # number of possible observations\n",
    "        self.N = N # number of states\n",
    "\n",
    "        # A\n",
    "        self.transition_model = TransitionModel(self.N, transition_matrix)\n",
    "\n",
    "        # b(x_t)\n",
    "        self.emission_model = EmissionModel(self.N, distributions)\n",
    "\n",
    "        # pi # CHECK\n",
    "        if state_priors==\"uniform\":\n",
    "            self.unnormalized_state_priors = torch.ones(self.N, device=device)/self.N#torch.nn.Parameter(torch.randn(self.N))#torch.randn(self.N)#\n",
    "            self.normalized_state_priors = self.unnormalized_state_priors\n",
    "            self.log_normalized_state_priors = torch.log(self.unnormalized_state_priors)\n",
    "        elif state_priors==\"random\":\n",
    "            self.unnormalized_state_priors = torch.randn(self.N).to(device)\n",
    "            self.normalized_state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
    "            self.log_normalized_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "        elif torch.is_tensor(state_priors):\n",
    "            self.unnormalized_state_priors = state_priors\n",
    "            self.normalized_state_priors = torch.nn.functional.normalize(self.unnormalized_transition_matrix, p=1, dim=0)\n",
    "            self.log_normalized_state_priors = torch.log(self.normalized_state_priors)\n",
    "        else:\n",
    "            raise ValueError(\"state_priors must be 'uniform', 'random' or torch tensor\")\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.transition_model = self.transition_model.to(device)\n",
    "        self.emission_model = self.emission_model.to(device)\n",
    "        \n",
    "        return self\n",
    "\n",
    "class TransitionModel(torch.nn.Module):\n",
    "    def __init__(self, N, transition_matrix=None, device=device):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.N = N\n",
    "        if transition_matrix is None:\n",
    "            self.unnormalized_transition_matrix = torch.nn.functional.softmax(torch.randn(N,N), dim=1).to(device)#torch.nn.Parameter(torch.randn(N,N))# CHECK\n",
    "        else:\n",
    "            self.unnormalized_transition_matrix = transition_matrix.to(device)\n",
    "            \n",
    "        self.device = device\n",
    "            \n",
    "    def normalized_transition_matrix(self):\n",
    "        #return torch.nn.functional.softmax(self.unnormalized_transition_matrix, dim=1) ## CHECK # original dim=0\n",
    "        return torch.nn.functional.normalize(self.unnormalized_transition_matrix, p=1, dim=1)\n",
    "    def log_normalized_transition_matrix(self):\n",
    "        #return torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=1) ## CHECK \n",
    "        return torch.log(self.normalized_transition_matrix())\n",
    "\n",
    "class EmissionModel(torch.nn.Module):\n",
    "    def __init__(self, N, distributions, device=device):\n",
    "        super(EmissionModel, self).__init__()\n",
    "        self.N = N\n",
    "        self.distributions = distributions ## list of distributions\n",
    "        self.device = device\n",
    "\n",
    "    def pdf(self, hidden_state, observation):\n",
    "        current_distribution = self.distributions[hidden_state]\n",
    "        return torch.exp(current_distribution.log_prob(torch.Tensor(observation, device=self.device)))\n",
    "\n",
    "def sample(self, T=10):\n",
    "    state_priors = self.normalized_state_priors#torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
    "    transition_matrix = self.transition_model.normalized_transition_matrix()\n",
    "    #emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
    "\n",
    "    # sample initial state\n",
    "    z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
    "    z = []; x = []\n",
    "    z.append(z_t)\n",
    "    for t in range(0,T):\n",
    "        # sample emission\n",
    "        # x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
    "        current_distribution = self.emission_model.distributions[z_t]\n",
    "        x_t = current_distribution.sample()\n",
    "        x.append(x_t)\n",
    "\n",
    "        # sample transition\n",
    "        z_t = torch.distributions.categorical.Categorical(transition_matrix[z_t, :]).sample().item() # CHECK # original [:, z_t]\n",
    "        if t < T-1: z.append(z_t)\n",
    " \n",
    "    return torch.stack(x).to(self.device), z\n",
    "\n",
    "# Add the sampling method to our HMM class\n",
    "HMM.sample = sample\n",
    "\n",
    "def HMM_forward(self, x, T, save_log_alpha=True):\n",
    "    \"\"\"\n",
    "    x : IntTensor of shape (batch size, T_max)\n",
    "    T : IntTensor of shape (batch size)\n",
    "\n",
    "    Compute log p(x) for each example in the batch.\n",
    "    T = length of each example\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "    #log_state_priors = torch.log(self.unnormalized_state_priors)  # TODO #torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    #log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    log_state_priors = self.log_normalized_state_priors\n",
    "    log_alpha = torch.zeros(batch_size, T_max, self.N) # table (sample, t, state) containing log probability of observations from sample to time t and being in state (in time t)\n",
    "\n",
    "    log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
    "    for t in range(1, T_max):\n",
    "    #print(f\"t={t} \", self.emission_model(x[:,t]), self.transition_model(log_alpha[:, t-1, :]))\n",
    "        log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
    "\n",
    "    if save_log_alpha:\n",
    "        self.log_alpha = log_alpha.to(self.device)\n",
    "        self.x = x\n",
    "    # Select the sum for the final timestep (each x may have different length).\n",
    "    #print(\"alpha\\n\", log_alpha)\n",
    "    log_sums = log_alpha.logsumexp(dim=2).to(self.device)\n",
    "    #print(\"log_sums\\n\", log_sums)\n",
    "    #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
    "    log_probs = torch.gather(log_sums, 1, T.view(-1,1)-1)\n",
    "    return log_probs\n",
    "\n",
    "def emission_model_forward(self, x_t): ## TODO\n",
    "    #out = self.distributions.log_prob(x_t)\n",
    "    #out = \n",
    "    out  = []\n",
    "    for state in range(self.N):\n",
    "        out.append( self.distributions[state].log_prob(x_t) )\n",
    "    result = torch.stack(out, dim = 1).to(device)\n",
    "    #print(\"emission probs\\n\",result)\n",
    "    return result\n",
    "\n",
    "def transition_model_forward(self, log_alpha):\n",
    "    \"\"\"\n",
    "    log_alpha : Tensor of shape (batch size, N)\n",
    "    Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "    \"\"\"\n",
    "    log_transition_matrix = self.log_normalized_transition_matrix()\n",
    "\n",
    "    # Matrix multiplication in the log domain\n",
    "    out = log_domain_matmul(log_transition_matrix.transpose(0,1), log_alpha.transpose(0,1).to(device)).transpose(0,1) # CHECK # original log_transition_matrix\n",
    "    return out.to(device)\n",
    "\n",
    "def log_domain_matmul(log_A, log_B):\n",
    "    \"\"\"\n",
    "    log_A : m x n\n",
    "    log_B : n x p\n",
    "    output : m x p matrix\n",
    "\n",
    "    Normally, a matrix multiplication\n",
    "    computes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
    "\n",
    "    A log domain matrix multiplication\n",
    "    computes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
    "    \"\"\"\n",
    "    m = log_A.shape[0]#; print(log_A.shape, log_B.shape)\n",
    "    n = log_A.shape[1]\n",
    "    p = log_B.shape[1]\n",
    "    #print(log_A.shape, log_B.shape)\n",
    "    # log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "    # log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "    # fix for PyTorch > 1.5 by egaznep on Github:\n",
    "    log_A_expanded = torch.reshape(log_A, (m,n,1))#; print(log_A_expanded.shape)\n",
    "    log_B_expanded = torch.reshape(log_B, (1,n,p))#; print(log_B_expanded.shape)\n",
    "\n",
    "    elementwise_sum = log_A_expanded + log_B_expanded #; print(\"hello\")\n",
    "    out = torch.logsumexp(elementwise_sum, dim=1)#;print(out.shape)\n",
    "\n",
    "    return out\n",
    "\n",
    "TransitionModel.forward = transition_model_forward\n",
    "EmissionModel.forward = emission_model_forward\n",
    "HMM.forward = HMM_forward\n",
    "\n",
    "def viterbi(self, x, T):\n",
    "    \"\"\"\n",
    "    x : IntTensor of shape (batch size, T_max)\n",
    "    T : IntTensor of shape (batch size)\n",
    "    Find argmax_z log p(x|z) for each (x) in the batch.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "    log_state_priors = self.log_normalized_state_priors#torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    log_delta = torch.zeros(batch_size, T_max, self.N).float()\n",
    "    psi = torch.zeros(batch_size, T_max, self.N).long()\n",
    "\n",
    "    log_delta[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
    "    for t in range(1, T_max):\n",
    "        max_val, argmax_val = self.transition_model.maxmul(log_delta[:, t-1, :])\n",
    "        log_delta[:, t, :] = self.emission_model(x[:,t]) + max_val\n",
    "        psi[:, t, :] = argmax_val\n",
    "\n",
    "    # Get the log probability of the best path\n",
    "    log_max = log_delta.max(dim=2)[0]\n",
    "    best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
    "\n",
    "    # This next part is a bit tricky to parallelize across the batch,\n",
    "    # so we will do it separately for each example.\n",
    "    z_star = []\n",
    "    for i in range(0, batch_size):\n",
    "        z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
    "        for t in range(T[i] - 1, 0, -1):\n",
    "            z_t = psi[i, t, z_star_i[0]].item()\n",
    "            z_star_i.insert(0, z_t)\n",
    "\n",
    "        z_star.append(z_star_i)\n",
    "\n",
    "    return z_star, best_path_scores # return both the best path and its log probability\n",
    "\n",
    "def transition_model_maxmul(self, log_alpha):\n",
    "    log_transition_matrix = self.log_normalized_transition_matrix()#torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "    out1, out2 = maxmul(log_transition_matrix.transpose(0,1), log_alpha.transpose(0,1)) # CHECK # original log_transition_matrix\n",
    "    return out1.transpose(0,1), out2.transpose(0,1)\n",
    "\n",
    "def maxmul(log_A, log_B):\n",
    "    \"\"\"\n",
    "    log_A : m x n\n",
    "    log_B : n x p\n",
    "    output : m x p matrix\n",
    "\n",
    "    Similar to the log domain matrix multiplication,\n",
    "    this computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
    "    \"\"\"\n",
    "    m = log_A.shape[0]\n",
    "    n = log_A.shape[1]\n",
    "    p = log_B.shape[1]\n",
    "\n",
    "    log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "    log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "\n",
    "    elementwise_sum = log_A_expanded + log_B_expanded\n",
    "    out1,out2 = torch.max(elementwise_sum, dim=1)\n",
    "\n",
    "    return out1,out2\n",
    "\n",
    "TransitionModel.maxmul = transition_model_maxmul\n",
    "HMM.viterbi = viterbi\n",
    "\n",
    "def HMM_backward(self, x, T, save_log_beta=True):\n",
    "    \"\"\"\n",
    "    x : IntTensor of shape (batch size, T_max)\n",
    "    T : IntTensor of shape (batch size)\n",
    "\n",
    "    Compute backward log p(x) for each example in the batch.\n",
    "    T = length of each example\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = x.shape[0]; T_max = x.shape[1] - 1\n",
    "    gather_indexes = torch.zeros((batch_size,1), dtype=torch.int64)\n",
    "\n",
    "    #log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    log_beta = torch.zeros(batch_size, T_max+1, self.N, device=self.device) # table (sample, t, state) containing log probability of observations from sample from time t+1 to T_max and being in state (in time t)\n",
    "\n",
    "    log_transition_matrix = self.transition_model.log_normalized_transition_matrix() #torch.nn.functional.log_softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "    log_beta[:, T_max, :] = 0 #1 #self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
    "    for t in range(T_max-1, 0-1, -1):\n",
    "        suma = (self.emission_model(x[:,t+1])+log_beta[:, t+1, :]).transpose(1,0)\n",
    "        #print(suma.shape)\n",
    "        #suma = suma.unsqueeze(1)\n",
    "        #print(suma.shape)\n",
    "        out = log_domain_matmul(log_transition_matrix, suma).transpose(1,0)# CHECK # original log_transition_matrix\n",
    "        #print(out.shape, log_beta[:, t, :].shape)\n",
    "        log_beta[:, t, :] = out\n",
    "\n",
    "    if save_log_beta:\n",
    "        self.log_beta = log_beta.to(self.device)\n",
    "        self.x = x\n",
    "\n",
    "    log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    termination = self.emission_model(x[:,0]) + log_state_priors + log_beta[:, 0, :]\n",
    "\n",
    "    log_sums = log_beta.logsumexp(dim=2)\n",
    "    #print(\"log_sums\\n\", log_sums)\n",
    "    #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
    "    #log_probs = torch.gather(log_sums, 1, gather_indexes)\n",
    "    log_probs = termination.logsumexp(dim=1)\n",
    "    return log_probs\n",
    "\n",
    "# def transition_model_backward(self, log_beta):\n",
    "#   \"\"\"\n",
    "#   log_alpha : Tensor of shape (batch size, N)\n",
    "#   Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "#   \"\"\"\n",
    "#   log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "#   # Matrix multiplication in the log domain\n",
    "#   #out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
    "#   out = log_domain_matmul(log_transition_matrix.transpose(0,1), log_beta.transpose(0,1)).transpose(0,1)\n",
    "#   return out\n",
    "\n",
    "#TransitionModel.back = transition_model_backward\n",
    "HMM.back = HMM_backward\n",
    "\n",
    "def forward_backward_step(self, x, T):\n",
    "    T_max=10\n",
    "    forward_result = self.forward(x, T, save_log_alpha=True)\n",
    "    backward_result = self.back(x, T, save_log_beta=True)\n",
    "    #log_beta and alpha have shape: (batch_size, T_max, self.N)\n",
    "    denominator_sum = (self.log_alpha+self.log_beta).logsumexp(dim=2)[:, 0:T_max-1]\n",
    "    log_alpha = self.log_alpha[:, 0:(T_max-1),:].unsqueeze(3)\n",
    "    log_beta = self.log_beta[:, 1:T_max, :].unsqueeze(2)\n",
    "    dim1,dim2,dim3 = x.shape\n",
    "    log_b = torch.zeros(((dim1,dim3,dim2)))\n",
    "    for i in range(dim1):\n",
    "        log_b[i, :] = self.emission_model(x[i,:]).transpose(0,1)\n",
    "    log_b = log_b.transpose(1,2)[:, 1:T_max, :].unsqueeze(2)\n",
    "\n",
    "    log_transition_matrix = self.transition_model.log_normalized_transition_matrix()#torch.nn.functional.log_softmax(self.transition_model.unnormalized_transition_matrix, dim=0).unsqueeze(0).unsqueeze(1)\n",
    "    #print(log_transition_matrix.shape, log_b.shape, log_transition_matrix.shape, log_alpha.shape)\n",
    "\n",
    "    nominator = log_alpha.to(device)+log_transition_matrix.to(device)+log_beta.to(device)+log_b.to(device) # CHECK\n",
    "    #print(nominator.shape)\n",
    "    log_ksi = nominator - denominator_sum[:, 0:T_max-1].unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "\n",
    "    approx_log_A = log_ksi.logsumexp(dim=(0,1)) \n",
    "    approx_log_A = approx_log_A - log_ksi.logsumexp(dim=(0,1,3)).unsqueeze(1)\n",
    "    approx_A = torch.exp(approx_log_A)#.transpose(0,1)#### Czy chcemy ten transpose CHECK originaly no transpose\n",
    "    self.transition_model.unnormalized_transition_matrix = approx_A\n",
    "\n",
    "    return approx_A\n",
    "\n",
    "\n",
    "HMM.forward_backward_step = forward_backward_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e98a31",
   "metadata": {
    "id": "22e98a31"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb2710",
   "metadata": {
    "id": "ebeb2710"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56933972",
   "metadata": {
    "id": "56933972"
   },
   "source": [
    "## Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0fcc0d41",
   "metadata": {
    "id": "0fcc0d41"
   },
   "outputs": [],
   "source": [
    "class CircleDistribution(distrib.uniform.Uniform):\n",
    "    def __init__(self, radious, center, noise_sd=0.01, validate_args=None):\n",
    "        super().__init__(0, 2 * np.pi, validate_args)\n",
    "        self.noise_distribution = distrib.Normal(0,noise_sd)\n",
    "        self.radious = radious\n",
    "        self.center = center\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        line = super().sample(sample_shape)\n",
    "        x = self.radious * torch.cos(line) + self.center[0]\n",
    "        y = self.radious * torch.sin(line) + self.center[1]\n",
    "\n",
    "        return torch.stack([x,y], dim=-1) + torch.stack([self.noise_distribution.sample(sample_shape), \n",
    "                                                         self.noise_distribution.sample(sample_shape)], dim=-1)\n",
    "\n",
    "    def plot(self, T=400):\n",
    "        samples = self.sample([T])\n",
    "        plt.scatter(samples[:,0], samples[:,1])\n",
    "\n",
    "class TriangleDistribution(distrib.uniform.Uniform):\n",
    "    def __init__(self, noise_sd=0.01, validate_args=None):\n",
    "        super().__init__(0, 2*(1 + 2**0.5), validate_args)\n",
    "        self.noise_distribution = distrib.Normal(0,noise_sd)\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        line = super().sample(sample_shape)\n",
    "        base = line <= 2\n",
    "        left_arm = (line <= 2 + 2**0.5) * (line > 2)\n",
    "        right_arm = line > 2 + 2**0.5\n",
    "        triangle = torch.stack([line * base, torch.zeros(sample_shape)], dim=-1)\n",
    "        left_arm_coord = left_arm * (line - 2.) / 2**0.5\n",
    "        triangle += torch.stack([left_arm_coord, left_arm_coord], dim=-1)\n",
    "        right_arm_coord = right_arm * ((line - (2 + 2**0.5)) / 2**0.5 + 1)\n",
    "        right_arm_coord_y = right_arm * (1 - (line - (2 + 2**0.5)) / 2**0.5)\n",
    "        triangle += torch.stack([right_arm_coord, right_arm_coord_y], dim=-1)\n",
    "        return triangle + torch.stack([self.noise_distribution.sample(sample_shape), self.noise_distribution.sample(sample_shape)], dim=-1)\n",
    "\n",
    "    def plot(self, T=400):\n",
    "        samples = self.sample([T])\n",
    "        plt.scatter(samples[:,0], samples[:,1])\n",
    "        \n",
    "\n",
    "\n",
    "class ArtificialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, distributions, device, T=1000, sequence_length=100,transition_matrix=None):\n",
    "        self.device = device\n",
    "        self.sequence_length = sequence_length\n",
    "        self.T = T\n",
    "        self.MyHMM = HMM(len(distributions), distributions,transition_matrix=transition_matrix)\n",
    "        self.data_dim = distributions[0].sample().shape[0]\n",
    "        self.generate_sequence()\n",
    "\n",
    "    def generate_sequence(self): # TODO zmienic! - chcemy mieć T sekwencji wygenerowanych za pomocą HMM.sample(self.sequence_length)\n",
    "        \n",
    "        self.train_X = torch.zeros((self.T, self.sequence_length, self.data_dim))\n",
    "        self.train_Z = torch.zeros((self.T, self.sequence_length))\n",
    "        for i in range(self.T):\n",
    "            x, z = self.MyHMM.sample(self.sequence_length)\n",
    "            self.train_X[i, :, :] = x\n",
    "            self.train_Z[i, :] = torch.tensor(z, dtype=torch.int)\n",
    "\n",
    "        self.test_T = int(0.2*self.T)\n",
    "        self.test_X = torch.zeros((self.test_T, self.sequence_length, self.data_dim))\n",
    "        self.test_Z = torch.zeros((self.test_T, self.sequence_length))\n",
    "        for i in range(self.test_T):\n",
    "            x, z = self.MyHMM.sample(self.sequence_length)\n",
    "            self.test_X[i, :, :] = x\n",
    "            self.test_Z[i, :] = torch.tensor(z, dtype=torch.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.T\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t = self.train_X[index]\n",
    "        t = t.to(device)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "252ad3ec",
   "metadata": {
    "id": "252ad3ec"
   },
   "outputs": [],
   "source": [
    "my_dataset = ArtificialDataset(\n",
    "    [TriangleDistribution(noise_sd=0.05), CircleDistribution(torch.tensor(0.5), torch.tensor([3.5,3.5]), noise_sd=0.05)],\n",
    "    device, \n",
    "    T=512, sequence_length = 256,\n",
    "    transition_matrix=torch.Tensor([[0.1,0.9], [0.7, 0.3]]))\n",
    "samples = my_dataset[1:len(my_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "71e5ee33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71e5ee33",
    "outputId": "3bd503dd-0bee-4209-95ac-08fc5373c656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset.data_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7204b2fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "7204b2fa",
    "outputId": "caf4b6e5-255c-433a-90c8-86c48c11aefe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff4a4d6b5b0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(samples[0,:,0].cpu(), samples[0,:,1].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a1168",
   "metadata": {
    "id": "b06a1168"
   },
   "source": [
    "# Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df4315a1",
   "metadata": {
    "id": "df4315a1"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, dim, blocks, flow_length, base_distributions, transition_matrix=None, device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flows = []\n",
    "        for d in base_distributions:\n",
    "            self.flows.append(NormalizingFlow(dim=dim,device=device,blocks=blocks,flow_length=flow_length,\n",
    "                                              base_distrib=d))    \n",
    "        \n",
    "        self.device = device\n",
    "        self.hmm = HMM(len(self.flows),self.flows,transition_matrix=transition_matrix)\n",
    "    \n",
    "    def forward(self, x, T):\n",
    "        return self.hmm(x, T)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return chain(*[f.parameters() for f in self.flows])\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.hmm = self.hmm.to(device)\n",
    "\n",
    "        self.flows = [f.to(device) for f in self.flows]\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "296b541e",
   "metadata": {
    "id": "296b541e"
   },
   "outputs": [],
   "source": [
    "def hmm_step(hmm_object, x, T):\n",
    "    with torch.no_grad():\n",
    "        hmm_object.forward_backward_step(x, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "846aa12d",
   "metadata": {
    "id": "846aa12d"
   },
   "outputs": [],
   "source": [
    "def nf_step(model, x, T, optimizer, scheduler):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = - torch.mean(model(x, T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f2b7f33a",
   "metadata": {
    "id": "f2b7f33a"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_MainModel(model, dataset, batch_size, epochs, optimizer, scheduler):\n",
    "    model.train()\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        turn = epoch % 2\n",
    "        # turn = - turn + 1\n",
    "        \n",
    "        for batch, x in enumerate(dataloader):\n",
    "            loc_batch_size = x.shape[0]\n",
    "            T = torch.ones([batch_size,1], dtype=torch.int64, device=device)*my_dataset.sequence_length\n",
    "            if turn==1:\n",
    "                hmm_step(model.hmm, x, T)\n",
    "            else:\n",
    "                loss = nf_step(model, x, T, optimizer, scheduler)\n",
    "            turn = - turn + 1\n",
    "            \n",
    "        if epoch % 6 == 1:\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b033750c",
   "metadata": {
    "id": "b033750c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████████▍                                                                                                                                     | 2/10 [01:27<05:50, 43.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370.210693359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 8/10 [06:00<01:31, 45.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364.9915466308594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [07:30<00:00, 45.06s/it]\n"
     ]
    }
   ],
   "source": [
    "block = [CouplingFlow, ReverseFlow]\n",
    "ref_distrib1 = distrib.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "ref_distrib2 = distrib.MultivariateNormal(torch.ones(2).to(device), torch.eye(2).to(device))\n",
    "main_model = MainModel(2,block,4,[ref_distrib1, ref_distrib2],device=device)\n",
    "main_model = main_model.to(device)\n",
    "flow_optimizer = optim.Adam(main_model.parameters(), lr=0.0003, weight_decay=0.001)\n",
    "flow_scheduler = optim.lr_scheduler.ExponentialLR(flow_optimizer, 0.99995)\n",
    "\n",
    "train_MainModel(main_model,my_dataset,32,10,flow_optimizer,flow_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "80c3b7c5",
   "metadata": {
    "id": "80c3b7c5"
   },
   "outputs": [],
   "source": [
    "x, states = main_model.hmm.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2c2d2e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff4953dc3a0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x[:,0].detach().cpu().numpy(), x[:,1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "13c2ab44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.8774e-33],\n",
       "        [1.0000e+00, 0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_model.hmm.transition_model.normalized_transition_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c805c4d",
   "metadata": {
    "id": "1c805c4d"
   },
   "outputs": [],
   "source": [
    "ref_distrib = distrib.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "block = [CouplingFlow, ReverseFlow]\n",
    "my_flow = NormalizingFlow(dim = 2, device = device, blocks = block, flow_length = 8, base_distrib = ref_distrib)\n",
    "# Create optimizer algorithm\n",
    "flow_optimizer = optim.Adam(flow.parameters(), lr=0.0003, weight_decay=0.001)\n",
    "# Add learning rate scheduler\n",
    "flow_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e309f86",
   "metadata": {
    "id": "8e309f86"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hmm_with_nf_development.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
