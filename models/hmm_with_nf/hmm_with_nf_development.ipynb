{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "55b7f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd5e3a",
   "metadata": {},
   "source": [
    "## Normalizing Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "135f759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distrib\n",
    "import torch.distributions.transforms as transform\n",
    "from torch.distributions import constraints\n",
    "import torch.optim as optim\n",
    "import torch.utils.data \n",
    "\n",
    "# Imports for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Flow(nn.Module):\n",
    "\n",
    "    def invert(self, y):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # def log_abs_det_jacobian(self, x, y):\n",
    "    #     raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x, log_det_jacob):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __init__(self, event_dim = 1):\n",
    "        # transform.Transform.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "        self._event_dim = event_dim\n",
    "\n",
    "    # Init all parameters\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "    # Hacky hash bypass\n",
    "    def __hash__(self):\n",
    "        return nn.Module.__hash__(self)\n",
    "\n",
    "    @property\n",
    "    def event_dim(self):\n",
    "        return self._event_dim\n",
    "\n",
    "    @constraints.dependent_property(is_discrete=False)\n",
    "    def domain(self):\n",
    "        if self.event_dim == 0:\n",
    "            return constraints.real\n",
    "        return constraints.independent(constraints.real, self.event_dim)\n",
    "\n",
    "    @constraints.dependent_property(is_discrete=False)\n",
    "    def codomain(self):\n",
    "        if self.event_dim == 0:\n",
    "            return constraints.real\n",
    "        return constraints.independent(constraints.real, self.event_dim)\n",
    "\n",
    "class NormalizingFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, blocks, flow_length, base_distrib, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        biject = []\n",
    "        for f in range(flow_length):\n",
    "            for b_flow in blocks:\n",
    "                biject.append(b_flow(dim, self.device))\n",
    "        # self.transforms = transform.ComposeTransform(biject)\n",
    "        self.bijectors = nn.ModuleList(biject)\n",
    "        self.base_distrib = base_distrib\n",
    "        # self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
    "        # self.log_det = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applies series of flows\n",
    "        log_det_jacob = torch.zeros((x.shape[0]), device = self.device, dtype = torch.float32)\n",
    "        for b in range(len(self.bijectors)):\n",
    "            x, log_det_jacob = self.bijectors[b](x, log_det_jacob)\n",
    "        return x, log_det_jacob\n",
    "\n",
    "    def invert(self, z):\n",
    "        for layer in reversed(self.bijectors):\n",
    "            z = layer.invert(z)\n",
    "        return z\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        y, log_det_jacob = self(x)\n",
    "        return self.base_distrib.log_prob(y) + log_det_jacob\n",
    "\n",
    "    # def sample(self):\n",
    "    #     # dodać cude\n",
    "    #     with torch.no_grad():\n",
    "    #         base_dens_samples = self.base_density.sample()\n",
    "    #         out_samples, _ = self.forward(base_dens_samples)\n",
    "    #     return out_samples\n",
    "\n",
    "    # def log_prob(self, y):\n",
    "    #     #dodać cude\n",
    "    #     log_det_reversed_order = []\n",
    "    #     for b in reversed(range(len(self.bijectors))):\n",
    "    #         log_det_reversed_order.append(self.bijectors[b].log_abs_det_jacobian(y))\n",
    "    #         y = self.bijectors[b]._inverse(y)\n",
    "    #         print(y)\n",
    "    #     log_prob_base = self.base_density.log_prob(y)\n",
    "    #     jacobian_part = torch.sum(torch.stack(log_det_reversed_order))\n",
    "    #     return log_prob_base - jacobian_part\n",
    "\n",
    "class CouplingFlow(Flow):\n",
    "    def __init__(self, dim, device, n_hidden=128, n_layers=3, activation=nn.ReLU, last_s_activation = nn.Tanh):\n",
    "        super(CouplingFlow, self).__init__()\n",
    "        self.k = dim // 2\n",
    "        self.t = self.t_transform_net(self.k, self.k, n_hidden, n_layers, activation)\n",
    "        self.s = self.s_transform_net(self.k, self.k, n_hidden, n_layers, activation, last_s_activation)\n",
    "        self.device = device\n",
    "        self.dim = dim\n",
    "        # self.register_buffer(\"mask\",torch.cat((torch.ones(self.k), torch.zeros(self.dim - self.k))).detach())\n",
    "        self.init_parameters()\n",
    "        # self.bijective = True\n",
    "\n",
    "\n",
    "    def t_transform_net(self, n_in, n_out, n_hidden, n_layer, activation):\n",
    "        net = nn.ModuleList()\n",
    "        for l in range(n_layer):\n",
    "            module = nn.Linear(l == 0 and n_in or n_hidden, n_hidden)\n",
    "            # module.weight.data.uniform_(-1, 1)\n",
    "            net.append(module)\n",
    "            net.append(activation())\n",
    "            if l == n_layer -1:\n",
    "                module = nn.Linear(n_hidden, n_out)\n",
    "                net.append(module)\n",
    "        return nn.Sequential(*net)\n",
    "\n",
    "    def s_transform_net(self, n_in, n_out, n_hidden, n_layer, activation, last_s_activation):\n",
    "        net = nn.ModuleList()\n",
    "        for l in range(n_layer):\n",
    "            module = nn.Linear((l == 0 and n_in) or n_hidden, l == n_layer - 1 and n_out or n_hidden)\n",
    "            # module.weight.data.uniform_(-1, 1)\n",
    "            net.append(module)\n",
    "            net.append((l == n_layer - 1 and last_s_activation()) or activation())\n",
    "        return nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x, log_det_jacob):\n",
    "        x_k = x[:, 0:self.k]\n",
    "        xp_D = x[:, self.k:self.dim] * torch.exp(self.s(x_k)) + self.t(x_k)\n",
    "        # print(x_k.shape)\n",
    "        log_det_jacob += torch.sum(torch.abs(self.s(x_k)), dim = 1)\n",
    "        # xp_D = x * self.g_sig(x_k) + self.g_mu(x_k)\n",
    "\n",
    "        return torch.cat((x_k, xp_D), dim=1), log_det_jacob\n",
    "\n",
    "\n",
    "    def invert(self, y):\n",
    "        yp_k = y[:, 0:self.k]\n",
    "        y_D = (y[:, self.k:self.dim] - self.t(yp_k)) * torch.exp(-self.s(yp_k))\n",
    "        # y_D = (((1 - self.mask) * y) - (1 - self.mask) * (self.g_mu(yp_k)) / self.g_sig(yp_k))\n",
    "\n",
    "        return torch.cat((yp_k, y_D), dim=1)\n",
    "\n",
    "\n",
    "class ReverseFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim, device):\n",
    "        super(ReverseFlow, self).__init__()\n",
    "        # k = dim // 2\n",
    "        # self.permute = torch.cat((torch.arange(dim, k, -1),torch.arange(1, k+1, 1)))\n",
    "        self.permute = torch.arange(dim-1, -1, -1)\n",
    "        self.inverse = torch.argsort(self.permute)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x, log_det_jacob):\n",
    "        return x[:, self.permute] , log_det_jacob\n",
    "\n",
    "    def invert(self, z):\n",
    "        return z[:, self.inverse]\n",
    "\n",
    "\n",
    "def nll_loss(y, log_det_jacob, base_distrib):\n",
    "    log_likelihood = base_distrib.log_prob(y) + log_det_jacob\n",
    "    return -torch.mean(log_likelihood)\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "def nll_loss(y, log_det_jacob, base_distrib):\n",
    "    log_likelihood = base_distrib.log_prob(y) + log_det_jacob\n",
    "    return -torch.mean(log_likelihood)\n",
    "\n",
    "def train_flow(flow, data_loader, loss, optimizer, scheduler, device, epochs=10001, plot_it=1000, batch_size = 64):\n",
    "    base_distrib = flow.base_distrib\n",
    "    flow.to(device)\n",
    "    # ims = []\n",
    "    #fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 7))\n",
    "    # fig = plt.figure(figsize=(10, 10))\n",
    "    # id_figure=2\n",
    "    # plt.figure(figsize=(16, 18))\n",
    "    # subplot_num = epochs // plot_it\n",
    "    # plt.subplot((subplot_num//4) + 1,4,1)\n",
    "    # plt.hexbin(z[:,0], z[:,1], C=target_density(torch.Tensor(z)).numpy().squeeze(), cmap='rainbow')\n",
    "    # plt.title('Target density', fontsize=15);\n",
    "    # Main optimization loop\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Training epoch {epoch + 1} ...\")\n",
    "        loss_acc = 0.0\n",
    "        flow.train()\n",
    "\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            batch = batch.to(device)\n",
    "            zk, log_det_jacob = flow(batch)\n",
    "            loss_n = loss(zk, log_det_jacob, base_distrib)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_n.backward()\n",
    "            # Do the step of optimizer\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # Gather data and report\n",
    "            # loss_acc += loss_n.item() * len(batch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            flow.eval()\n",
    "            with torch.no_grad():\n",
    "                z = base_distrib.sample((1000,))\n",
    "                x = flow.invert(z).cpu()\n",
    "                plt.scatter(x[:, 0], x[:, 1])\n",
    "                plt.xlim(-5, 5)\n",
    "                plt.ylim(-5, 5)\n",
    "                plt.title(f\"Epoch: {epoch + 1} nll loss: {loss_n:.4f}\")\n",
    "                plt.show()\n",
    "\n",
    "# dataset\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "batch_size=256\n",
    "\n",
    "class MoonDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, device, lenght=1000):\n",
    "        self.device = device\n",
    "        self.lenght = lenght\n",
    "        self.generate_moons(lenght)\n",
    "\n",
    "    def generate_moons(self, lenght):\n",
    "        moons_data = make_moons(lenght, noise=0.05)[0].astype(\"float32\")\n",
    "        self.moons = moons_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.lenght\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t = torch.tensor(self.moons[index], device = device)\n",
    "        return (t)\n",
    "        \n",
    "my_moons = MoonDataset(device, 5000)\n",
    "\n",
    "moons_dataloader = torch.utils.data.DataLoader(my_moons, batch_size=batch_size,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2ed13",
   "metadata": {},
   "source": [
    "## Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4c7dd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class HMM(torch.nn.Module):\n",
    "#   \"\"\"\n",
    "#   Hidden Markov Model with discrete observations.\n",
    "#   \"\"\"\n",
    "    def __init__(self, N, distributions, transition_matrix=None, state_priors='uniform'):\n",
    "        super(HMM, self).__init__()\n",
    "        #self.M = M # number of possible observations\n",
    "        self.N = N # number of states\n",
    "\n",
    "        # A\n",
    "        self.transition_model = TransitionModel(self.N, transition_matrix)\n",
    "\n",
    "        # b(x_t)\n",
    "        self.emission_model = EmissionModel(self.N, distributions)\n",
    "\n",
    "        # pi # CHECK\n",
    "        if state_priors==\"uniform\":\n",
    "            self.unnormalized_state_priors = torch.ones(self.N)/self.N#torch.nn.Parameter(torch.randn(self.N))#torch.randn(self.N)#\n",
    "            self.normalized_state_priors = self.unnormalized_state_priors\n",
    "            self.log_normalized_state_priors = torch.log(self.unnormalized_state_priors)\n",
    "        elif state_priors==\"random\":\n",
    "            self.unnormalized_state_priors = torch.randn(self.N)\n",
    "            self.normalized_state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
    "            self.log_normalized_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "        elif torch.is_tensor(state_priors):\n",
    "            self.unnormalized_state_priors = state_priors\n",
    "            self.normalized_state_priors = torch.nn.functional.normalize(self.unnormalized_transition_matrix, p=1, dim=0)\n",
    "            self.log_normalized_state_priors = torch.log(self.normalized_state_priors)\n",
    "        else:\n",
    "            raise ValueError(\"state_priors must be 'uniform', 'random' or torch tensor\")\n",
    "\n",
    "\n",
    "        # use the GPU\n",
    "        self.is_cuda = torch.cuda.is_available()\n",
    "        if self.is_cuda: self.cuda()\n",
    "\n",
    "class TransitionModel(torch.nn.Module):\n",
    "    def __init__(self, N, transition_matrix=None):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.N = N\n",
    "        if transition_matrix is None:\n",
    "            self.unnormalized_transition_matrix = torch.nn.functional.softmax(torch.randn(N,N), dim=1)#torch.nn.Parameter(torch.randn(N,N))# CHECK\n",
    "        else:\n",
    "            self.unnormalized_transition_matrix = transition_matrix\n",
    "            \n",
    "    def normalized_transition_matrix(self):\n",
    "        #return torch.nn.functional.softmax(self.unnormalized_transition_matrix, dim=1) ## CHECK # original dim=0\n",
    "        return torch.nn.functional.normalize(self.unnormalized_transition_matrix, p=1, dim=1)\n",
    "    def log_normalized_transition_matrix(self):\n",
    "        #return torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=1) ## CHECK \n",
    "        return torch.log(self.normalized_transition_matrix())\n",
    "\n",
    "class EmissionModel(torch.nn.Module):\n",
    "    def __init__(self, N, distributions):\n",
    "        super(EmissionModel, self).__init__()\n",
    "        self.N = N\n",
    "        self.distributions = distributions ## list of distributions\n",
    "\n",
    "    def pdf(self, hidden_state, observation):\n",
    "        current_distribution = self.distributions[hidden_state]\n",
    "        return torch.exp(current_distribution.log_prob(torch.Tensor(observation)))\n",
    "\n",
    "def sample(self, T=10):\n",
    "    state_priors = self.normalized_state_priors#torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
    "    transition_matrix = self.transition_model.normalized_transition_matrix()\n",
    "    #emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
    "\n",
    "    # sample initial state\n",
    "    z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
    "    z = []; x = []\n",
    "    z.append(z_t)\n",
    "    for t in range(0,T):\n",
    "        # sample emission\n",
    "        # x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
    "        current_distribution = self.emission_model.distributions[z_t]\n",
    "        x_t = current_distribution.sample()\n",
    "        x.append(x_t)\n",
    "\n",
    "        # sample transition\n",
    "        z_t = torch.distributions.categorical.Categorical(transition_matrix[z_t, :]).sample().item() # CHECK # original [:, z_t]\n",
    "        if t < T-1: z.append(z_t)\n",
    " \n",
    "    return torch.stack(x), z\n",
    "\n",
    "# Add the sampling method to our HMM class\n",
    "HMM.sample = sample\n",
    "\n",
    "def HMM_forward(self, x, T, save_log_alpha=True):\n",
    "    \"\"\"\n",
    "    x : IntTensor of shape (batch size, T_max)\n",
    "    T : IntTensor of shape (batch size)\n",
    "\n",
    "    Compute log p(x) for each example in the batch.\n",
    "    T = length of each example\n",
    "    \"\"\"\n",
    "    if self.is_cuda:\n",
    "        x = x.cuda()\n",
    "        T = T.cuda()\n",
    "\n",
    "    batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "    #log_state_priors = torch.log(self.unnormalized_state_priors)  # TODO #torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    #log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    log_state_priors = self.log_normalized_state_priors\n",
    "    log_alpha = torch.zeros(batch_size, T_max, self.N) # table (sample, t, state) containing log probability of observations from sample to time t and being in state (in time t)\n",
    "    if self.is_cuda: log_alpha = log_alpha.cuda()\n",
    "\n",
    "    log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
    "    for t in range(1, T_max):\n",
    "    #print(f\"t={t} \", self.emission_model(x[:,t]), self.transition_model(log_alpha[:, t-1, :]))\n",
    "        log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
    "\n",
    "    if save_log_alpha:\n",
    "        self.log_alpha = log_alpha\n",
    "        self.x = x\n",
    "    # Select the sum for the final timestep (each x may have different length).\n",
    "    #print(\"alpha\\n\", log_alpha)\n",
    "    log_sums = log_alpha.logsumexp(dim=2)\n",
    "    #print(\"log_sums\\n\", log_sums)\n",
    "    #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
    "    log_probs = torch.gather(log_sums, 1, T.view(-1,1)-1)\n",
    "    return log_probs\n",
    "\n",
    "def emission_model_forward(self, x_t): ## TODO\n",
    "    #out = self.distributions.log_prob(x_t)\n",
    "    #out = \n",
    "    out  = []\n",
    "    for state in range(self.N):\n",
    "        out.append( self.distributions[state].log_prob(x_t) )\n",
    "    result = torch.stack(out, dim = 1)\n",
    "    #print(\"emission probs\\n\",result)\n",
    "    return result\n",
    "\n",
    "def transition_model_forward(self, log_alpha):\n",
    "    \"\"\"\n",
    "    log_alpha : Tensor of shape (batch size, N)\n",
    "    Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "    \"\"\"\n",
    "    log_transition_matrix = self.log_normalized_transition_matrix()\n",
    "\n",
    "    # Matrix multiplication in the log domain\n",
    "    out = log_domain_matmul(log_transition_matrix.transpose(0,1), log_alpha.transpose(0,1)).transpose(0,1) # CHECK # original log_transition_matrix\n",
    "    return out\n",
    "\n",
    "def log_domain_matmul(log_A, log_B):\n",
    "    \"\"\"\n",
    "    log_A : m x n\n",
    "    log_B : n x p\n",
    "    output : m x p matrix\n",
    "\n",
    "    Normally, a matrix multiplication\n",
    "    computes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
    "\n",
    "    A log domain matrix multiplication\n",
    "    computes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
    "    \"\"\"\n",
    "    m = log_A.shape[0]#; print(log_A.shape, log_B.shape)\n",
    "    n = log_A.shape[1]\n",
    "    p = log_B.shape[1]\n",
    "    #print(log_A.shape, log_B.shape)\n",
    "    # log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "    # log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "    # fix for PyTorch > 1.5 by egaznep on Github:\n",
    "    log_A_expanded = torch.reshape(log_A, (m,n,1))#; print(log_A_expanded.shape)\n",
    "    log_B_expanded = torch.reshape(log_B, (1,n,p))#; print(log_B_expanded.shape)\n",
    "\n",
    "    elementwise_sum = log_A_expanded + log_B_expanded #; print(\"hello\")\n",
    "    out = torch.logsumexp(elementwise_sum, dim=1)#;print(out.shape)\n",
    "\n",
    "    return out\n",
    "\n",
    "TransitionModel.forward = transition_model_forward\n",
    "EmissionModel.forward = emission_model_forward\n",
    "HMM.forward = HMM_forward\n",
    "\n",
    "def viterbi(self, x, T):\n",
    "    \"\"\"\n",
    "    x : IntTensor of shape (batch size, T_max)\n",
    "    T : IntTensor of shape (batch size)\n",
    "    Find argmax_z log p(x|z) for each (x) in the batch.\n",
    "    \"\"\"\n",
    "    if self.is_cuda:\n",
    "        x = x.cuda()\n",
    "        T = T.cuda()\n",
    "\n",
    "    batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "    log_state_priors = self.log_normalized_state_priors#torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    log_delta = torch.zeros(batch_size, T_max, self.N).float()\n",
    "    psi = torch.zeros(batch_size, T_max, self.N).long()\n",
    "    if self.is_cuda:\n",
    "        log_delta = log_delta.cuda()\n",
    "        psi = psi.cuda()\n",
    "\n",
    "    log_delta[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
    "    for t in range(1, T_max):\n",
    "        max_val, argmax_val = self.transition_model.maxmul(log_delta[:, t-1, :])\n",
    "        log_delta[:, t, :] = self.emission_model(x[:,t]) + max_val\n",
    "        psi[:, t, :] = argmax_val\n",
    "\n",
    "    # Get the log probability of the best path\n",
    "    log_max = log_delta.max(dim=2)[0]\n",
    "    best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
    "\n",
    "    # This next part is a bit tricky to parallelize across the batch,\n",
    "    # so we will do it separately for each example.\n",
    "    z_star = []\n",
    "    for i in range(0, batch_size):\n",
    "        z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
    "        for t in range(T[i] - 1, 0, -1):\n",
    "            z_t = psi[i, t, z_star_i[0]].item()\n",
    "            z_star_i.insert(0, z_t)\n",
    "\n",
    "        z_star.append(z_star_i)\n",
    "\n",
    "    return z_star, best_path_scores # return both the best path and its log probability\n",
    "\n",
    "def transition_model_maxmul(self, log_alpha):\n",
    "    log_transition_matrix = self.log_normalized_transition_matrix()#torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "    out1, out2 = maxmul(log_transition_matrix.transpose(0,1), log_alpha.transpose(0,1)) # CHECK # original log_transition_matrix\n",
    "    return out1.transpose(0,1), out2.transpose(0,1)\n",
    "\n",
    "def maxmul(log_A, log_B):\n",
    "    \"\"\"\n",
    "    log_A : m x n\n",
    "    log_B : n x p\n",
    "    output : m x p matrix\n",
    "\n",
    "    Similar to the log domain matrix multiplication,\n",
    "    this computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
    "    \"\"\"\n",
    "    m = log_A.shape[0]\n",
    "    n = log_A.shape[1]\n",
    "    p = log_B.shape[1]\n",
    "\n",
    "    log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "    log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "\n",
    "    elementwise_sum = log_A_expanded + log_B_expanded\n",
    "    out1,out2 = torch.max(elementwise_sum, dim=1)\n",
    "\n",
    "    return out1,out2\n",
    "\n",
    "TransitionModel.maxmul = transition_model_maxmul\n",
    "HMM.viterbi = viterbi\n",
    "\n",
    "def HMM_backward(self, x, T, save_log_beta=True):\n",
    "    \"\"\"\n",
    "    x : IntTensor of shape (batch size, T_max)\n",
    "    T : IntTensor of shape (batch size)\n",
    "\n",
    "    Compute backward log p(x) for each example in the batch.\n",
    "    T = length of each example\n",
    "    \"\"\"\n",
    "    if self.is_cuda:\n",
    "        x = x.cuda()\n",
    "        T = T.cuda()\n",
    "\n",
    "    batch_size = x.shape[0]; T_max = x.shape[1] - 1\n",
    "    gather_indexes = torch.zeros((batch_size,1), dtype=torch.int64)\n",
    "    if self.is_cuda:\n",
    "        gather_indexes = gather_indexes.cuda()\n",
    "    #log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    log_beta = torch.zeros(batch_size, T_max+1, self.N) # table (sample, t, state) containing log probability of observations from sample from time t+1 to T_max and being in state (in time t)\n",
    "    if self.is_cuda: log_beta = log_beta.cuda()\n",
    "\n",
    "    log_transition_matrix = self.transition_model.log_normalized_transition_matrix() #torch.nn.functional.log_softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "    log_beta[:, T_max, :] = 0 #1 #self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
    "    for t in range(T_max-1, 0-1, -1):\n",
    "        suma = (self.emission_model(x[:,t+1])+log_beta[:, t+1, :]).transpose(1,0)\n",
    "        #print(suma.shape)\n",
    "        #suma = suma.unsqueeze(1)\n",
    "        #print(suma.shape)\n",
    "        out = log_domain_matmul(log_transition_matrix, suma).transpose(1,0)# CHECK # original log_transition_matrix\n",
    "        #print(out.shape, log_beta[:, t, :].shape)\n",
    "        log_beta[:, t, :] = out\n",
    "\n",
    "    if save_log_beta:\n",
    "        self.log_beta = log_beta\n",
    "        self.x = x\n",
    "\n",
    "    log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    termination = self.emission_model(x[:,0]) + log_state_priors + log_beta[:, 0, :]\n",
    "\n",
    "    log_sums = log_beta.logsumexp(dim=2)\n",
    "    #print(\"log_sums\\n\", log_sums)\n",
    "    #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
    "    #log_probs = torch.gather(log_sums, 1, gather_indexes)\n",
    "    log_probs = termination.logsumexp(dim=1)\n",
    "    return log_probs\n",
    "\n",
    "# def transition_model_backward(self, log_beta):\n",
    "#   \"\"\"\n",
    "#   log_alpha : Tensor of shape (batch size, N)\n",
    "#   Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "#   \"\"\"\n",
    "#   log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "#   # Matrix multiplication in the log domain\n",
    "#   #out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
    "#   out = log_domain_matmul(log_transition_matrix.transpose(0,1), log_beta.transpose(0,1)).transpose(0,1)\n",
    "#   return out\n",
    "\n",
    "#TransitionModel.back = transition_model_backward\n",
    "HMM.back = HMM_backward\n",
    "\n",
    "def forward_backward_step(self, x, T):\n",
    "    T_max=10\n",
    "    forward_result = self.forward(x, T, save_log_alpha=True)\n",
    "    backward_result = self.back(x, T, save_log_beta=True)\n",
    "    #log_beta and alpha have shape: (batch_size, T_max, self.N)\n",
    "    denominator_sum = (self.log_alpha+self.log_beta).logsumexp(dim=2)[:, 0:T_max-1]\n",
    "    log_alpha = self.log_alpha[:, 0:(T_max-1),:].unsqueeze(3)\n",
    "    log_beta = self.log_beta[:, 1:T_max, :].unsqueeze(2)\n",
    "    log_b = self.emission_model(x)\n",
    "    #print(log_b.shape)\n",
    "    log_b = log_b.transpose(1,2)[:, 1:T_max, :].unsqueeze(2)\n",
    "    log_transition_matrix = self.transition_model.log_normalized_transition_matrix()#torch.nn.functional.log_softmax(self.transition_model.unnormalized_transition_matrix, dim=0).unsqueeze(0).unsqueeze(1)\n",
    "    #print(log_transition_matrix.shape, log_b.shape, log_transition_matrix.shape, log_alpha.shape)\n",
    "\n",
    "    nominator = log_alpha+log_transition_matrix+log_beta+log_b # CHECK\n",
    "    #print(nominator.shape)\n",
    "    log_ksi = nominator - denominator_sum[:, 0:T_max-1].unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "\n",
    "    approx_log_A = log_ksi.logsumexp(dim=(0,1)) \n",
    "    approx_log_A = approx_log_A - log_ksi.logsumexp(dim=(0,1,3)).unsqueeze(1)\n",
    "    approx_A = torch.exp(approx_log_A)#.transpose(0,1)#### Czy chcemy ten transpose CHECK originaly no transpose\n",
    "    self.transition_model.unnormalized_transition_matrix = approx_A\n",
    "\n",
    "    return approx_A\n",
    "\n",
    "\n",
    "HMM.forward_backward_step = forward_backward_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e98a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb2710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56933972",
   "metadata": {},
   "source": [
    "## Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0fcc0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleDistribution(distrib.uniform.Uniform):\n",
    "    def __init__(self, radious, center, noise_sd=0.01, validate_args=None):\n",
    "        super().__init__(0, 2 * np.pi, validate_args)\n",
    "        self.noise_distribution = distrib.Normal(0,noise_sd)\n",
    "        self.radious = radious\n",
    "        self.center = center\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        line = super().sample(sample_shape)\n",
    "        x = self.radious * torch.cos(line) + self.center[0]\n",
    "        y = self.radious * torch.sin(line) + self.center[1]\n",
    "\n",
    "        return torch.stack([x,y], dim=-1) + torch.stack([self.noise_distribution.sample(sample_shape), \n",
    "                                                         self.noise_distribution.sample(sample_shape)], dim=-1)\n",
    "\n",
    "    def plot(self, T=400):\n",
    "        samples = self.sample([T])\n",
    "        plt.scatter(samples[:,0], samples[:,1])\n",
    "\n",
    "class TriangleDistribution(distrib.uniform.Uniform):\n",
    "    def __init__(self, noise_sd=0.01, validate_args=None):\n",
    "        super().__init__(0, 2*(1 + 2**0.5), validate_args)\n",
    "        self.noise_distribution = distrib.Normal(0,noise_sd)\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        line = super().sample(sample_shape)\n",
    "        base = line <= 2\n",
    "        left_arm = (line <= 2 + 2**0.5) * (line > 2)\n",
    "        right_arm = line > 2 + 2**0.5\n",
    "        triangle = torch.stack([line * base, torch.zeros(sample_shape)], dim=-1)\n",
    "        left_arm_coord = left_arm * (line - 2.) / 2**0.5\n",
    "        triangle += torch.stack([left_arm_coord, left_arm_coord], dim=-1)\n",
    "        right_arm_coord = right_arm * ((line - (2 + 2**0.5)) / 2**0.5 + 1)\n",
    "        right_arm_coord_y = right_arm * (1 - (line - (2 + 2**0.5)) / 2**0.5)\n",
    "        triangle += torch.stack([right_arm_coord, right_arm_coord_y], dim=-1)\n",
    "        return triangle + torch.stack([self.noise_distribution.sample(sample_shape), self.noise_distribution.sample(sample_shape)], dim=-1)\n",
    "\n",
    "    def plot(self, T=400):\n",
    "        samples = self.sample([T])\n",
    "        plt.scatter(samples[:,0], samples[:,1])\n",
    "        \n",
    "\n",
    "\n",
    "class ArtificialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, distributions, device, T=1000, sequence_length=100,transition_matrix=None):\n",
    "        self.device = device\n",
    "        self.sequence_length = sequence_length\n",
    "        self.T = T\n",
    "        self.MyHMM = HMM(len(distributions), distributions,transition_matrix=transition_matrix)\n",
    "        self.data_dim = distributions[0].sample().shape[0]\n",
    "        self.generate_sequence()\n",
    "\n",
    "    def generate_sequence(self): # TODO zmienic! - chcemy mieć T sekwencji wygenerowanych za pomocą HMM.sample(self.sequence_length)\n",
    "        \n",
    "        self.train_X = torch.zeros((self.T, self.sequence_length, self.data_dim))\n",
    "        self.train_Z = torch.zeros((self.T, self.sequence_length))\n",
    "        for i in range(self.T):\n",
    "            x, z = self.MyHMM.sample(self.sequence_length)\n",
    "            self.train_X[i, :, :] = x\n",
    "            self.train_Z[i, :] = torch.tensor(z, dtype=torch.int)\n",
    "\n",
    "        self.test_T = int(0.2*self.T)\n",
    "        self.test_X = torch.zeros((self.test_T, self.sequence_length, self.data_dim))\n",
    "        self.test_Z = torch.zeros((self.test_T, self.sequence_length))\n",
    "        for i in range(self.test_T):\n",
    "            x, z = self.MyHMM.sample(self.sequence_length)\n",
    "            self.test_X[i, :, :] = x\n",
    "            self.test_Z[i, :] = torch.tensor(z, dtype=torch.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.T\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t = self.train_X[index]\n",
    "        t.to(device)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "252ad3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = ArtificialDataset(\n",
    "    [TriangleDistribution(noise_sd=0.05), CircleDistribution(torch.tensor(0.5), torch.tensor([3.5,3.5]), noise_sd=0.05)],\n",
    "    device, \n",
    "    T=100, sequence_length = 256,\n",
    "    transition_matrix=torch.Tensor([[0.1,0.9], [0.7, 0.3]]))\n",
    "samples = my_dataset[1:len(my_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "71e5ee33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset.data_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7204b2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2e11b0d040>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(samples[0,:,0], samples[0,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a1168",
   "metadata": {},
   "source": [
    "# Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "df4315a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, dim, blocks, flow_length, base_distributions, transition_matrix=None, device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flows = []\n",
    "        for d in base_distributions:\n",
    "            self.flows.append(NormalizingFlow(dim=dim,device=device,blocks=blocks,flow_length=flow_length,\n",
    "                                              base_distrib=d))    \n",
    "        \n",
    "        self.device = device\n",
    "        self.hmm = HMM(len(self.flows),self.flows,transition_matrix=transition_matrix)\n",
    "    \n",
    "    def forward(self, x, T):\n",
    "        return self.hmm(x, T)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return chain(*[f.parameters() for f in self.flows])\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.hmm.to(device)\n",
    "\n",
    "        for f in self.flows:\n",
    "            f.to(device)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "296b541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_step(hmm_object, x, T):\n",
    "    hmm_object.forward_backward_step(x, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "846aa12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_step(model, x, T, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = - model(x, T)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f2b7f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_MainModel(model, dataset, batch_size, epochs, optimizer):\n",
    "    model.train()\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        turn = epoch % 2\n",
    "        \n",
    "        for batch, x in enumerate(dataloader):\n",
    "            loc_batch_size = x.shape[0]\n",
    "            T = torch.ones([batch_size,1], dtype=torch.int64)*my_dataset.sequence_length\n",
    "            if turn==1:\n",
    "                hmm_step(model.hmm, x, T)\n",
    "            else:\n",
    "                nf_step(model, x, T, optimizer)\n",
    "            turn = - turn + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "36bb40e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmissionModel()"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_model.hmm.emission_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7a44778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6931, -0.6931])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_model.hmm.log_normalized_state_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b033750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmissionModel() -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [140]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m main_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m flow_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(main_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0003\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_MainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmy_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mflow_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [135]\u001b[0m, in \u001b[0;36mtrain_MainModel\u001b[0;34m(model, dataset, batch_size, epochs, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m     hmm_step(model\u001b[38;5;241m.\u001b[39mhmm, x, T)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mnf_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m turn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m turn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [134]\u001b[0m, in \u001b[0;36mnf_step\u001b[0;34m(model, x, T, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnf_step\u001b[39m(model, x, T, optimizer):\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [137]\u001b[0m, in \u001b[0;36mMainModel.forward\u001b[0;34m(self, x, T)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, T):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36mHMM_forward\u001b[0;34m(self, x, T, save_log_alpha)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda: log_alpha \u001b[38;5;241m=\u001b[39m log_alpha\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memission_model, log_state_priors\u001b[38;5;241m.\u001b[39mget_device())\n\u001b[0;32m--> 111\u001b[0m log_alpha[:, \u001b[38;5;241m0\u001b[39m, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memission_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog_state_priors\u001b[49m \u001b[38;5;66;03m# emission_model - log prob for each distr\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, T_max):\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m#print(f\"t={t} \", self.emission_model(x[:,t]), self.transition_model(log_alpha[:, t-1, :]))\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     log_alpha[:, t, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memission_model(x[:,t]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_model(log_alpha[:, t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "block = [CouplingFlow, ReverseFlow]\n",
    "ref_distrib = distrib.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "main_model = MainModel(2,block,8,[ref_distrib]*2,device=device)\n",
    "main_model.to(device)\n",
    "flow_optimizer = optim.Adam(main_model.parameters(), lr=0.0003, weight_decay=0.001)\n",
    "\n",
    "train_MainModel(main_model,my_dataset,5,10,flow_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5eef00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c805c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ref_distrib = distrib.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "block = [CouplingFlow, ReverseFlow]\n",
    "my_flow = NormalizingFlow(dim = 2, device = device, blocks = block, flow_length = 8, base_distrib = ref_distrib)\n",
    "# Create optimizer algorithm\n",
    "flow_optimizer = optim.Adam(flow.parameters(), lr=0.0003, weight_decay=0.001)\n",
    "# Add learning rate scheduler\n",
    "flow_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e309f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
