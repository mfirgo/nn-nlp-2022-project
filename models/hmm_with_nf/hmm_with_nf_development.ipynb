{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "55b7f017",
      "metadata": {
        "id": "55b7f017"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5cd5e3a",
      "metadata": {
        "id": "e5cd5e3a"
      },
      "source": [
        "## Normalizing Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "135f759d",
      "metadata": {
        "id": "135f759d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as distrib\n",
        "import torch.distributions.transforms as transform\n",
        "from torch.distributions import constraints\n",
        "import torch.optim as optim\n",
        "import torch.utils.data \n",
        "\n",
        "# Imports for plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "class Flow(nn.Module):\n",
        "\n",
        "    def invert(self, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    # def log_abs_det_jacobian(self, x, y):\n",
        "    #     raise NotImplementedError()\n",
        "\n",
        "    def forward(self, x, log_det_jacob):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __init__(self, event_dim = 1):\n",
        "        # transform.Transform.__init__(self)\n",
        "        nn.Module.__init__(self)\n",
        "        self._event_dim = event_dim\n",
        "\n",
        "    # Init all parameters\n",
        "    def init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.uniform_(-0.01, 0.01)\n",
        "\n",
        "    # Hacky hash bypass\n",
        "    def __hash__(self):\n",
        "        return nn.Module.__hash__(self)\n",
        "\n",
        "    @property\n",
        "    def event_dim(self):\n",
        "        return self._event_dim\n",
        "\n",
        "    @constraints.dependent_property(is_discrete=False)\n",
        "    def domain(self):\n",
        "        if self.event_dim == 0:\n",
        "            return constraints.real\n",
        "        return constraints.independent(constraints.real, self.event_dim)\n",
        "\n",
        "    @constraints.dependent_property(is_discrete=False)\n",
        "    def codomain(self):\n",
        "        if self.event_dim == 0:\n",
        "            return constraints.real\n",
        "        return constraints.independent(constraints.real, self.event_dim)\n",
        "\n",
        "class NormalizingFlow(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, blocks, flow_length, base_distrib, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        biject = []\n",
        "        for f in range(flow_length):\n",
        "            for b_flow in blocks:\n",
        "                biject.append(b_flow(dim, self.device))\n",
        "        # self.transforms = transform.ComposeTransform(biject)\n",
        "        self.bijectors = nn.ModuleList(biject)\n",
        "        self.base_distrib = base_distrib\n",
        "        # self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
        "        # self.log_det = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Applies series of flows\n",
        "        log_det_jacob = torch.zeros((x.shape[0]), device = self.device, dtype = torch.float32)\n",
        "        for b in range(len(self.bijectors)):\n",
        "            x, log_det_jacob = self.bijectors[b](x, log_det_jacob)\n",
        "        return x, log_det_jacob\n",
        "\n",
        "    def invert(self, z):\n",
        "        for layer in reversed(self.bijectors):\n",
        "            z = layer.invert(z)\n",
        "        return z\n",
        "    \n",
        "    def log_prob(self, x):\n",
        "        y, log_det_jacob = self(x)\n",
        "        return self.base_distrib.log_prob(y) + log_det_jacob\n",
        "\n",
        "    def sample(self, sample_shape=torch.Size([])):\n",
        "        z = self.base_distrib.sample(sample_shape)\n",
        "        return torch.squeeze(self.invert(z[None,:]), dim=0)\n",
        "\n",
        "    # def sample(self):\n",
        "    #     # dodać cude\n",
        "    #     with torch.no_grad():\n",
        "    #         base_dens_samples = self.base_density.sample()\n",
        "    #         out_samples, _ = self.forward(base_dens_samples)\n",
        "    #     return out_samples\n",
        "\n",
        "    # def log_prob(self, y):\n",
        "    #     #dodać cude\n",
        "    #     log_det_reversed_order = []\n",
        "    #     for b in reversed(range(len(self.bijectors))):\n",
        "    #         log_det_reversed_order.append(self.bijectors[b].log_abs_det_jacobian(y))\n",
        "    #         y = self.bijectors[b]._inverse(y)\n",
        "    #         print(y)\n",
        "    #     log_prob_base = self.base_density.log_prob(y)\n",
        "    #     jacobian_part = torch.sum(torch.stack(log_det_reversed_order))\n",
        "    #     return log_prob_base - jacobian_part\n",
        "\n",
        "class CouplingFlow(Flow):\n",
        "    def __init__(self, dim, device, n_hidden=128, n_layers=3, activation=nn.ReLU, last_s_activation = nn.Tanh):\n",
        "        super(CouplingFlow, self).__init__()\n",
        "        self.k = dim // 2\n",
        "        self.t = self.t_transform_net(self.k, self.k, n_hidden, n_layers, activation)\n",
        "        self.s = self.s_transform_net(self.k, self.k, n_hidden, n_layers, activation, last_s_activation)\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "        # self.register_buffer(\"mask\",torch.cat((torch.ones(self.k), torch.zeros(self.dim - self.k))).detach())\n",
        "        self.init_parameters()\n",
        "        # self.bijective = True\n",
        "\n",
        "\n",
        "    def t_transform_net(self, n_in, n_out, n_hidden, n_layer, activation):\n",
        "        net = nn.ModuleList()\n",
        "        for l in range(n_layer):\n",
        "            module = nn.Linear(l == 0 and n_in or n_hidden, n_hidden)\n",
        "            # module.weight.data.uniform_(-1, 1)\n",
        "            net.append(module)\n",
        "            net.append(activation())\n",
        "            if l == n_layer -1:\n",
        "                module = nn.Linear(n_hidden, n_out)\n",
        "                net.append(module)\n",
        "        return nn.Sequential(*net)\n",
        "\n",
        "    def s_transform_net(self, n_in, n_out, n_hidden, n_layer, activation, last_s_activation):\n",
        "        net = nn.ModuleList()\n",
        "        for l in range(n_layer):\n",
        "            module = nn.Linear((l == 0 and n_in) or n_hidden, l == n_layer - 1 and n_out or n_hidden)\n",
        "            # module.weight.data.uniform_(-1, 1)\n",
        "            net.append(module)\n",
        "            net.append((l == n_layer - 1 and last_s_activation()) or activation())\n",
        "        return nn.Sequential(*net)\n",
        "\n",
        "    def forward(self, x, log_det_jacob):\n",
        "        x_k = x[:, 0:self.k]\n",
        "        xp_D = x[:, self.k:self.dim] * torch.exp(self.s(x_k)) + self.t(x_k)\n",
        "        # print(x_k.shape)\n",
        "        log_det_jacob += torch.sum(torch.abs(self.s(x_k)), dim = 1)\n",
        "        # xp_D = x * self.g_sig(x_k) + self.g_mu(x_k)\n",
        "\n",
        "        return torch.cat((x_k, xp_D), dim=1), log_det_jacob\n",
        "\n",
        "\n",
        "    def invert(self, y):\n",
        "        yp_k = y[:, 0:self.k]\n",
        "        y_D = (y[:, self.k:self.dim] - self.t(yp_k)) * torch.exp(-self.s(yp_k))\n",
        "        # y_D = (((1 - self.mask) * y) - (1 - self.mask) * (self.g_mu(yp_k)) / self.g_sig(yp_k))\n",
        "\n",
        "        return torch.cat((yp_k, y_D), dim=1)\n",
        "\n",
        "\n",
        "class ReverseFlow(Flow):\n",
        "\n",
        "    def __init__(self, dim, device):\n",
        "        super(ReverseFlow, self).__init__()\n",
        "        # k = dim // 2\n",
        "        # self.permute = torch.cat((torch.arange(dim, k, -1),torch.arange(1, k+1, 1)))\n",
        "        self.permute = torch.arange(dim-1, -1, -1)\n",
        "        self.inverse = torch.argsort(self.permute)\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, x, log_det_jacob):\n",
        "        return x[:, self.permute] , log_det_jacob\n",
        "\n",
        "    def invert(self, z):\n",
        "        return z[:, self.inverse]\n",
        "\n",
        "\n",
        "def nll_loss(y, log_det_jacob, base_distrib):\n",
        "    log_likelihood = base_distrib.log_prob(y) + log_det_jacob\n",
        "    return -torch.mean(log_likelihood)\n",
        "\n",
        "\n",
        "# training\n",
        "\n",
        "def nll_loss(y, log_det_jacob, base_distrib):\n",
        "    log_likelihood = base_distrib.log_prob(y) + log_det_jacob\n",
        "    return -torch.mean(log_likelihood)\n",
        "\n",
        "def train_flow(flow, data_loader, loss, optimizer, scheduler, device, epochs=10001, plot_it=1000, batch_size = 64):\n",
        "    base_distrib = flow.base_distrib\n",
        "    flow.to(device)\n",
        "    # ims = []\n",
        "    #fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 7))\n",
        "    # fig = plt.figure(figsize=(10, 10))\n",
        "    # id_figure=2\n",
        "    # plt.figure(figsize=(16, 18))\n",
        "    # subplot_num = epochs // plot_it\n",
        "    # plt.subplot((subplot_num//4) + 1,4,1)\n",
        "    # plt.hexbin(z[:,0], z[:,1], C=target_density(torch.Tensor(z)).numpy().squeeze(), cmap='rainbow')\n",
        "    # plt.title('Target density', fontsize=15);\n",
        "    # Main optimization loop\n",
        "    for epoch in range(epochs):\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Training epoch {epoch + 1} ...\")\n",
        "        loss_acc = 0.0\n",
        "        flow.train()\n",
        "\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            batch = batch.to(device)\n",
        "            zk, log_det_jacob = flow(batch)\n",
        "            loss_n = loss(zk, log_det_jacob, base_distrib)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss_n.backward()\n",
        "            # Do the step of optimizer\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # Gather data and report\n",
        "            # loss_acc += loss_n.item() * len(batch)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            flow.eval()\n",
        "            with torch.no_grad():\n",
        "                z = base_distrib.sample((1000,))\n",
        "                x = flow.invert(z).cpu()\n",
        "                plt.scatter(x[:, 0], x[:, 1])\n",
        "                plt.xlim(-5, 5)\n",
        "                plt.ylim(-5, 5)\n",
        "                plt.title(f\"Epoch: {epoch + 1} nll loss: {loss_n:.4f}\")\n",
        "                plt.show()\n",
        "\n",
        "# dataset\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "batch_size=256\n",
        "\n",
        "class MoonDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, device, lenght=1000):\n",
        "        self.device = device\n",
        "        self.lenght = lenght\n",
        "        self.generate_moons(lenght)\n",
        "\n",
        "    def generate_moons(self, lenght):\n",
        "        moons_data = make_moons(lenght, noise=0.05)[0].astype(\"float32\")\n",
        "        self.moons = moons_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.lenght\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        t = torch.tensor(self.moons[index], device = device)\n",
        "        return (t)\n",
        "        \n",
        "my_moons = MoonDataset(device, 5000)\n",
        "\n",
        "moons_dataloader = torch.utils.data.DataLoader(my_moons, batch_size=batch_size,\n",
        "                        shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f2ed13",
      "metadata": {
        "id": "49f2ed13"
      },
      "source": [
        "## Hidden Markov Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4c7dd531",
      "metadata": {
        "id": "4c7dd531"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class HMM(torch.nn.Module):\n",
        "#   \"\"\"\n",
        "#   Hidden Markov Model with discrete observations.\n",
        "#   \"\"\"\n",
        "    def __init__(self, N, distributions, transition_matrix=None, state_priors='uniform'):\n",
        "        super(HMM, self).__init__()\n",
        "        #self.M = M # number of possible observations\n",
        "        self.N = N # number of states\n",
        "\n",
        "        # A\n",
        "        self.transition_model = TransitionModel(self.N, transition_matrix)\n",
        "\n",
        "        # b(x_t)\n",
        "        self.emission_model = EmissionModel(self.N, distributions)\n",
        "\n",
        "        # pi # CHECK\n",
        "        if state_priors==\"uniform\":\n",
        "            self.unnormalized_state_priors = torch.ones(self.N)/self.N#torch.nn.Parameter(torch.randn(self.N))#torch.randn(self.N)#\n",
        "            self.normalized_state_priors = self.unnormalized_state_priors\n",
        "            self.log_normalized_state_priors = torch.log(self.unnormalized_state_priors)\n",
        "        elif state_priors==\"random\":\n",
        "            self.unnormalized_state_priors = torch.randn(self.N)\n",
        "            self.normalized_state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
        "            self.log_normalized_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "        elif torch.is_tensor(state_priors):\n",
        "            self.unnormalized_state_priors = state_priors\n",
        "            self.normalized_state_priors = torch.nn.functional.normalize(self.unnormalized_transition_matrix, p=1, dim=0)\n",
        "            self.log_normalized_state_priors = torch.log(self.normalized_state_priors)\n",
        "        else:\n",
        "            raise ValueError(\"state_priors must be 'uniform', 'random' or torch tensor\")\n",
        "\n",
        "\n",
        "        # use the GPU\n",
        "        self.is_cuda = torch.cuda.is_available()\n",
        "        if self.is_cuda: self.cuda()\n",
        "\n",
        "class TransitionModel(torch.nn.Module):\n",
        "    def __init__(self, N, transition_matrix=None):\n",
        "        super(TransitionModel, self).__init__()\n",
        "        self.N = N\n",
        "        if transition_matrix is None:\n",
        "            self.unnormalized_transition_matrix = torch.nn.functional.softmax(torch.randn(N,N), dim=1)#torch.nn.Parameter(torch.randn(N,N))# CHECK\n",
        "        else:\n",
        "            self.unnormalized_transition_matrix = transition_matrix\n",
        "            \n",
        "    def normalized_transition_matrix(self):\n",
        "        #return torch.nn.functional.softmax(self.unnormalized_transition_matrix, dim=1) ## CHECK # original dim=0\n",
        "        return torch.nn.functional.normalize(self.unnormalized_transition_matrix, p=1, dim=1)\n",
        "    def log_normalized_transition_matrix(self):\n",
        "        #return torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=1) ## CHECK \n",
        "        return torch.log(self.normalized_transition_matrix())\n",
        "\n",
        "class EmissionModel(torch.nn.Module):\n",
        "    def __init__(self, N, distributions):\n",
        "        super(EmissionModel, self).__init__()\n",
        "        self.N = N\n",
        "        self.distributions = distributions ## list of distributions\n",
        "\n",
        "    def pdf(self, hidden_state, observation):\n",
        "        current_distribution = self.distributions[hidden_state]\n",
        "        return torch.exp(current_distribution.log_prob(torch.Tensor(observation)))\n",
        "\n",
        "def sample(self, T=10):\n",
        "    state_priors = self.normalized_state_priors#torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
        "    transition_matrix = self.transition_model.normalized_transition_matrix()\n",
        "    #emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
        "\n",
        "    # sample initial state\n",
        "    z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
        "    z = []; x = []\n",
        "    z.append(z_t)\n",
        "    for t in range(0,T):\n",
        "        # sample emission\n",
        "        # x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
        "        current_distribution = self.emission_model.distributions[z_t]\n",
        "        x_t = current_distribution.sample()\n",
        "        x.append(x_t)\n",
        "\n",
        "        # sample transition\n",
        "        z_t = torch.distributions.categorical.Categorical(transition_matrix[z_t, :]).sample().item() # CHECK # original [:, z_t]\n",
        "        if t < T-1: z.append(z_t)\n",
        " \n",
        "    return torch.stack(x), z\n",
        "\n",
        "# Add the sampling method to our HMM class\n",
        "HMM.sample = sample\n",
        "\n",
        "def HMM_forward(self, x, T, save_log_alpha=True):\n",
        "    \"\"\"\n",
        "    x : IntTensor of shape (batch size, T_max)\n",
        "    T : IntTensor of shape (batch size)\n",
        "\n",
        "    Compute log p(x) for each example in the batch.\n",
        "    T = length of each example\n",
        "    \"\"\"\n",
        "    if self.is_cuda:\n",
        "        x = x.cuda()\n",
        "        T = T.cuda()\n",
        "\n",
        "    batch_size = x.shape[0]; T_max = x.shape[1]\n",
        "    #log_state_priors = torch.log(self.unnormalized_state_priors)  # TODO #torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "    #log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "    log_state_priors = self.log_normalized_state_priors\n",
        "    log_alpha = torch.zeros(batch_size, T_max, self.N) # table (sample, t, state) containing log probability of observations from sample to time t and being in state (in time t)\n",
        "    if self.is_cuda: log_alpha = log_alpha.cuda()\n",
        "\n",
        "    log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
        "    for t in range(1, T_max):\n",
        "    #print(f\"t={t} \", self.emission_model(x[:,t]), self.transition_model(log_alpha[:, t-1, :]))\n",
        "        log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
        "\n",
        "    if save_log_alpha:\n",
        "        self.log_alpha = log_alpha\n",
        "        self.x = x\n",
        "    # Select the sum for the final timestep (each x may have different length).\n",
        "    #print(\"alpha\\n\", log_alpha)\n",
        "    log_sums = log_alpha.logsumexp(dim=2)\n",
        "    #print(\"log_sums\\n\", log_sums)\n",
        "    #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
        "    log_probs = torch.gather(log_sums, 1, T.view(-1,1)-1)\n",
        "    return log_probs\n",
        "\n",
        "def emission_model_forward(self, x_t): ## TODO\n",
        "    #out = self.distributions.log_prob(x_t)\n",
        "    #out = \n",
        "    out  = []\n",
        "    for state in range(self.N):\n",
        "        out.append( self.distributions[state].log_prob(x_t) )\n",
        "    result = torch.stack(out, dim = 1)\n",
        "    #print(\"emission probs\\n\",result)\n",
        "    return result\n",
        "\n",
        "def transition_model_forward(self, log_alpha):\n",
        "    \"\"\"\n",
        "    log_alpha : Tensor of shape (batch size, N)\n",
        "    Multiply previous timestep's alphas by transition matrix (in log domain)\n",
        "    \"\"\"\n",
        "    log_transition_matrix = self.log_normalized_transition_matrix()\n",
        "\n",
        "    # Matrix multiplication in the log domain\n",
        "    out = log_domain_matmul(log_transition_matrix.transpose(0,1), log_alpha.transpose(0,1)).transpose(0,1) # CHECK # original log_transition_matrix\n",
        "    return out\n",
        "\n",
        "def log_domain_matmul(log_A, log_B):\n",
        "    \"\"\"\n",
        "    log_A : m x n\n",
        "    log_B : n x p\n",
        "    output : m x p matrix\n",
        "\n",
        "    Normally, a matrix multiplication\n",
        "    computes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
        "\n",
        "    A log domain matrix multiplication\n",
        "    computes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
        "    \"\"\"\n",
        "    m = log_A.shape[0]#; print(log_A.shape, log_B.shape)\n",
        "    n = log_A.shape[1]\n",
        "    p = log_B.shape[1]\n",
        "    #print(log_A.shape, log_B.shape)\n",
        "    # log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
        "    # log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
        "    # fix for PyTorch > 1.5 by egaznep on Github:\n",
        "    log_A_expanded = torch.reshape(log_A, (m,n,1))#; print(log_A_expanded.shape)\n",
        "    log_B_expanded = torch.reshape(log_B, (1,n,p))#; print(log_B_expanded.shape)\n",
        "\n",
        "    elementwise_sum = log_A_expanded + log_B_expanded #; print(\"hello\")\n",
        "    out = torch.logsumexp(elementwise_sum, dim=1)#;print(out.shape)\n",
        "\n",
        "    return out\n",
        "\n",
        "TransitionModel.forward = transition_model_forward\n",
        "EmissionModel.forward = emission_model_forward\n",
        "HMM.forward = HMM_forward\n",
        "\n",
        "def viterbi(self, x, T):\n",
        "    \"\"\"\n",
        "    x : IntTensor of shape (batch size, T_max)\n",
        "    T : IntTensor of shape (batch size)\n",
        "    Find argmax_z log p(x|z) for each (x) in the batch.\n",
        "    \"\"\"\n",
        "    if self.is_cuda:\n",
        "        x = x.cuda()\n",
        "        T = T.cuda()\n",
        "\n",
        "    batch_size = x.shape[0]; T_max = x.shape[1]\n",
        "    log_state_priors = self.log_normalized_state_priors#torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "    log_delta = torch.zeros(batch_size, T_max, self.N).float()\n",
        "    psi = torch.zeros(batch_size, T_max, self.N).long()\n",
        "    if self.is_cuda:\n",
        "        log_delta = log_delta.cuda()\n",
        "        psi = psi.cuda()\n",
        "\n",
        "    log_delta[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
        "    for t in range(1, T_max):\n",
        "        max_val, argmax_val = self.transition_model.maxmul(log_delta[:, t-1, :])\n",
        "        log_delta[:, t, :] = self.emission_model(x[:,t]) + max_val\n",
        "        psi[:, t, :] = argmax_val\n",
        "\n",
        "    # Get the log probability of the best path\n",
        "    log_max = log_delta.max(dim=2)[0]\n",
        "    best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
        "\n",
        "    # This next part is a bit tricky to parallelize across the batch,\n",
        "    # so we will do it separately for each example.\n",
        "    z_star = []\n",
        "    for i in range(0, batch_size):\n",
        "        z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
        "        for t in range(T[i] - 1, 0, -1):\n",
        "            z_t = psi[i, t, z_star_i[0]].item()\n",
        "            z_star_i.insert(0, z_t)\n",
        "\n",
        "        z_star.append(z_star_i)\n",
        "\n",
        "    return z_star, best_path_scores # return both the best path and its log probability\n",
        "\n",
        "def transition_model_maxmul(self, log_alpha):\n",
        "    log_transition_matrix = self.log_normalized_transition_matrix()#torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "    out1, out2 = maxmul(log_transition_matrix.transpose(0,1), log_alpha.transpose(0,1)) # CHECK # original log_transition_matrix\n",
        "    return out1.transpose(0,1), out2.transpose(0,1)\n",
        "\n",
        "def maxmul(log_A, log_B):\n",
        "    \"\"\"\n",
        "    log_A : m x n\n",
        "    log_B : n x p\n",
        "    output : m x p matrix\n",
        "\n",
        "    Similar to the log domain matrix multiplication,\n",
        "    this computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
        "    \"\"\"\n",
        "    m = log_A.shape[0]\n",
        "    n = log_A.shape[1]\n",
        "    p = log_B.shape[1]\n",
        "\n",
        "    log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
        "    log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
        "\n",
        "    elementwise_sum = log_A_expanded + log_B_expanded\n",
        "    out1,out2 = torch.max(elementwise_sum, dim=1)\n",
        "\n",
        "    return out1,out2\n",
        "\n",
        "TransitionModel.maxmul = transition_model_maxmul\n",
        "HMM.viterbi = viterbi\n",
        "\n",
        "def HMM_backward(self, x, T, save_log_beta=True):\n",
        "    \"\"\"\n",
        "    x : IntTensor of shape (batch size, T_max)\n",
        "    T : IntTensor of shape (batch size)\n",
        "\n",
        "    Compute backward log p(x) for each example in the batch.\n",
        "    T = length of each example\n",
        "    \"\"\"\n",
        "    if self.is_cuda:\n",
        "        x = x.cuda()\n",
        "        T = T.cuda()\n",
        "\n",
        "    batch_size = x.shape[0]; T_max = x.shape[1] - 1\n",
        "    gather_indexes = torch.zeros((batch_size,1), dtype=torch.int64)\n",
        "    if self.is_cuda:\n",
        "        gather_indexes = gather_indexes.cuda()\n",
        "    #log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "    log_beta = torch.zeros(batch_size, T_max+1, self.N) # table (sample, t, state) containing log probability of observations from sample from time t+1 to T_max and being in state (in time t)\n",
        "    if self.is_cuda: log_beta = log_beta.cuda()\n",
        "\n",
        "    log_transition_matrix = self.transition_model.log_normalized_transition_matrix() #torch.nn.functional.log_softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "    log_beta[:, T_max, :] = 0 #1 #self.emission_model(x[:,0]) + log_state_priors # emission_model - log prob for each distr\n",
        "    for t in range(T_max-1, 0-1, -1):\n",
        "        suma = (self.emission_model(x[:,t+1])+log_beta[:, t+1, :]).transpose(1,0)\n",
        "        #print(suma.shape)\n",
        "        #suma = suma.unsqueeze(1)\n",
        "        #print(suma.shape)\n",
        "        out = log_domain_matmul(log_transition_matrix, suma).transpose(1,0)# CHECK # original log_transition_matrix\n",
        "        #print(out.shape, log_beta[:, t, :].shape)\n",
        "        log_beta[:, t, :] = out\n",
        "\n",
        "    if save_log_beta:\n",
        "        self.log_beta = log_beta\n",
        "        self.x = x\n",
        "\n",
        "    log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "    termination = self.emission_model(x[:,0]) + log_state_priors + log_beta[:, 0, :]\n",
        "\n",
        "    log_sums = log_beta.logsumexp(dim=2)\n",
        "    #print(\"log_sums\\n\", log_sums)\n",
        "    #log_probs = torch.gather(log_sums, 1, T.view(1,-1))\n",
        "    #log_probs = torch.gather(log_sums, 1, gather_indexes)\n",
        "    log_probs = termination.logsumexp(dim=1)\n",
        "    return log_probs\n",
        "\n",
        "# def transition_model_backward(self, log_beta):\n",
        "#   \"\"\"\n",
        "#   log_alpha : Tensor of shape (batch size, N)\n",
        "#   Multiply previous timestep's alphas by transition matrix (in log domain)\n",
        "#   \"\"\"\n",
        "#   log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "#   # Matrix multiplication in the log domain\n",
        "#   #out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
        "#   out = log_domain_matmul(log_transition_matrix.transpose(0,1), log_beta.transpose(0,1)).transpose(0,1)\n",
        "#   return out\n",
        "\n",
        "#TransitionModel.back = transition_model_backward\n",
        "HMM.back = HMM_backward\n",
        "\n",
        "def forward_backward_step(self, x, T):\n",
        "    T_max=10\n",
        "    forward_result = self.forward(x, T, save_log_alpha=True)\n",
        "    backward_result = self.back(x, T, save_log_beta=True)\n",
        "    #log_beta and alpha have shape: (batch_size, T_max, self.N)\n",
        "    denominator_sum = (self.log_alpha+self.log_beta).logsumexp(dim=2)[:, 0:T_max-1]\n",
        "    log_alpha = self.log_alpha[:, 0:(T_max-1),:].unsqueeze(3)\n",
        "    log_beta = self.log_beta[:, 1:T_max, :].unsqueeze(2)\n",
        "    dim1,dim2,dim3 = x.shape\n",
        "    log_b = torch.zeros(((dim1,dim3,dim2)))\n",
        "    for i in range(dim1):\n",
        "        log_b[i, :] = self.emission_model(x[i,:]).transpose(0,1)\n",
        "    log_b = log_b.transpose(1,2)[:, 1:T_max, :].unsqueeze(2)\n",
        "\n",
        "    log_transition_matrix = self.transition_model.log_normalized_transition_matrix()#torch.nn.functional.log_softmax(self.transition_model.unnormalized_transition_matrix, dim=0).unsqueeze(0).unsqueeze(1)\n",
        "    #print(log_transition_matrix.shape, log_b.shape, log_transition_matrix.shape, log_alpha.shape)\n",
        "\n",
        "    nominator = log_alpha+log_transition_matrix+log_beta+log_b # CHECK\n",
        "    #print(nominator.shape)\n",
        "    log_ksi = nominator - denominator_sum[:, 0:T_max-1].unsqueeze(2).unsqueeze(3)\n",
        "    \n",
        "\n",
        "    approx_log_A = log_ksi.logsumexp(dim=(0,1)) \n",
        "    approx_log_A = approx_log_A - log_ksi.logsumexp(dim=(0,1,3)).unsqueeze(1)\n",
        "    approx_A = torch.exp(approx_log_A)#.transpose(0,1)#### Czy chcemy ten transpose CHECK originaly no transpose\n",
        "    self.transition_model.unnormalized_transition_matrix = approx_A\n",
        "\n",
        "    return approx_A\n",
        "\n",
        "\n",
        "HMM.forward_backward_step = forward_backward_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "22e98a31",
      "metadata": {
        "id": "22e98a31"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ebeb2710",
      "metadata": {
        "id": "ebeb2710"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56933972",
      "metadata": {
        "id": "56933972"
      },
      "source": [
        "## Training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0fcc0d41",
      "metadata": {
        "id": "0fcc0d41"
      },
      "outputs": [],
      "source": [
        "class CircleDistribution(distrib.uniform.Uniform):\n",
        "    def __init__(self, radious, center, noise_sd=0.01, validate_args=None):\n",
        "        super().__init__(0, 2 * np.pi, validate_args)\n",
        "        self.noise_distribution = distrib.Normal(0,noise_sd)\n",
        "        self.radious = radious\n",
        "        self.center = center\n",
        "\n",
        "    def sample(self, sample_shape=torch.Size([])):\n",
        "        line = super().sample(sample_shape)\n",
        "        x = self.radious * torch.cos(line) + self.center[0]\n",
        "        y = self.radious * torch.sin(line) + self.center[1]\n",
        "\n",
        "        return torch.stack([x,y], dim=-1) + torch.stack([self.noise_distribution.sample(sample_shape), \n",
        "                                                         self.noise_distribution.sample(sample_shape)], dim=-1)\n",
        "\n",
        "    def plot(self, T=400):\n",
        "        samples = self.sample([T])\n",
        "        plt.scatter(samples[:,0], samples[:,1])\n",
        "\n",
        "class TriangleDistribution(distrib.uniform.Uniform):\n",
        "    def __init__(self, noise_sd=0.01, validate_args=None):\n",
        "        super().__init__(0, 2*(1 + 2**0.5), validate_args)\n",
        "        self.noise_distribution = distrib.Normal(0,noise_sd)\n",
        "\n",
        "    def sample(self, sample_shape=torch.Size([])):\n",
        "        line = super().sample(sample_shape)\n",
        "        base = line <= 2\n",
        "        left_arm = (line <= 2 + 2**0.5) * (line > 2)\n",
        "        right_arm = line > 2 + 2**0.5\n",
        "        triangle = torch.stack([line * base, torch.zeros(sample_shape)], dim=-1)\n",
        "        left_arm_coord = left_arm * (line - 2.) / 2**0.5\n",
        "        triangle += torch.stack([left_arm_coord, left_arm_coord], dim=-1)\n",
        "        right_arm_coord = right_arm * ((line - (2 + 2**0.5)) / 2**0.5 + 1)\n",
        "        right_arm_coord_y = right_arm * (1 - (line - (2 + 2**0.5)) / 2**0.5)\n",
        "        triangle += torch.stack([right_arm_coord, right_arm_coord_y], dim=-1)\n",
        "        return triangle + torch.stack([self.noise_distribution.sample(sample_shape), self.noise_distribution.sample(sample_shape)], dim=-1)\n",
        "\n",
        "    def plot(self, T=400):\n",
        "        samples = self.sample([T])\n",
        "        plt.scatter(samples[:,0], samples[:,1])\n",
        "        \n",
        "\n",
        "\n",
        "class ArtificialDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, distributions, device, T=1000, sequence_length=100,transition_matrix=None):\n",
        "        self.device = device\n",
        "        self.sequence_length = sequence_length\n",
        "        self.T = T\n",
        "        self.MyHMM = HMM(len(distributions), distributions,transition_matrix=transition_matrix)\n",
        "        self.data_dim = distributions[0].sample().shape[0]\n",
        "        self.generate_sequence()\n",
        "\n",
        "    def generate_sequence(self): # TODO zmienic! - chcemy mieć T sekwencji wygenerowanych za pomocą HMM.sample(self.sequence_length)\n",
        "        \n",
        "        self.train_X = torch.zeros((self.T, self.sequence_length, self.data_dim))\n",
        "        self.train_Z = torch.zeros((self.T, self.sequence_length))\n",
        "        for i in range(self.T):\n",
        "            x, z = self.MyHMM.sample(self.sequence_length)\n",
        "            self.train_X[i, :, :] = x\n",
        "            self.train_Z[i, :] = torch.tensor(z, dtype=torch.int)\n",
        "\n",
        "        self.test_T = int(0.2*self.T)\n",
        "        self.test_X = torch.zeros((self.test_T, self.sequence_length, self.data_dim))\n",
        "        self.test_Z = torch.zeros((self.test_T, self.sequence_length))\n",
        "        for i in range(self.test_T):\n",
        "            x, z = self.MyHMM.sample(self.sequence_length)\n",
        "            self.test_X[i, :, :] = x\n",
        "            self.test_Z[i, :] = torch.tensor(z, dtype=torch.int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.T\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        t = self.train_X[index]\n",
        "        t.to(device)\n",
        "        return t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "252ad3ec",
      "metadata": {
        "id": "252ad3ec"
      },
      "outputs": [],
      "source": [
        "my_dataset = ArtificialDataset(\n",
        "    [TriangleDistribution(noise_sd=0.05), CircleDistribution(torch.tensor(0.5), torch.tensor([3.5,3.5]), noise_sd=0.05)],\n",
        "    device, \n",
        "    T=10, sequence_length = 15,\n",
        "    transition_matrix=torch.Tensor([[0.1,0.9], [0.7, 0.3]]))\n",
        "samples = my_dataset[1:len(my_dataset)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "71e5ee33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71e5ee33",
        "outputId": "3bd503dd-0bee-4209-95ac-08fc5373c656"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "my_dataset.data_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7204b2fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "7204b2fa",
        "outputId": "caf4b6e5-255c-433a-90c8-86c48c11aefe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f4b0ebfffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.scatter(samples[0,:,0], samples[0,:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06a1168",
      "metadata": {
        "id": "b06a1168"
      },
      "source": [
        "# Main model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "df4315a1",
      "metadata": {
        "id": "df4315a1"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, dim, blocks, flow_length, base_distributions, transition_matrix=None, device='cpu'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.flows = []\n",
        "        for d in base_distributions:\n",
        "            self.flows.append(NormalizingFlow(dim=dim,device=device,blocks=blocks,flow_length=flow_length,\n",
        "                                              base_distrib=d))    \n",
        "        \n",
        "        self.device = device\n",
        "        self.hmm = HMM(len(self.flows),self.flows,transition_matrix=transition_matrix)\n",
        "    \n",
        "    def forward(self, x, T):\n",
        "        return self.hmm(x, T)\n",
        "    \n",
        "    def parameters(self):\n",
        "        return chain(*[f.parameters() for f in self.flows])\n",
        "    \n",
        "    def to(self, device):\n",
        "        super().to(device)\n",
        "        self.hmm.to(device)\n",
        "\n",
        "        for f in self.flows:\n",
        "            f.to(device)\n",
        "            \n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "296b541e",
      "metadata": {
        "id": "296b541e"
      },
      "outputs": [],
      "source": [
        "def hmm_step(hmm_object, x, T):\n",
        "    with torch.no_grad():\n",
        "        hmm_object.forward_backward_step(x, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "846aa12d",
      "metadata": {
        "id": "846aa12d"
      },
      "outputs": [],
      "source": [
        "def nf_step(model, x, T, optimizer, scheduler):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    loss = - torch.mean(model(x, T))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f2b7f33a",
      "metadata": {
        "id": "f2b7f33a"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_MainModel(model, dataset, batch_size, epochs, optimizer, scheduler):\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset,batch_size=batch_size)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        turn = epoch % 2\n",
        "        # turn = - turn + 1\n",
        "        \n",
        "        for batch, x in enumerate(dataloader):\n",
        "            loc_batch_size = x.shape[0]\n",
        "            T = torch.ones([batch_size,1], dtype=torch.int64)*my_dataset.sequence_length\n",
        "            if turn==1:\n",
        "                hmm_step(model.hmm, x, T)\n",
        "            else:\n",
        "                nf_step(model, x, T, optimizer, scheduler)\n",
        "            turn = - turn + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b033750c",
      "metadata": {
        "id": "b033750c"
      },
      "outputs": [],
      "source": [
        "block = [CouplingFlow, ReverseFlow]\n",
        "ref_distrib = distrib.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
        "main_model = MainModel(2,block,4,[ref_distrib]*2,device=device)\n",
        "main_model.to(device)\n",
        "flow_optimizer = optim.Adam(main_model.parameters(), lr=0.0003, weight_decay=0.001)\n",
        "flow_scheduler = optim.lr_scheduler.ExponentialLR(flow_optimizer, 0.99995)\n",
        "\n",
        "train_MainModel(main_model,my_dataset,5,10,flow_optimizer,flow_scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c3b7c5",
      "metadata": {
        "id": "80c3b7c5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c805c4d",
      "metadata": {
        "id": "1c805c4d"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ref_distrib = distrib.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
        "block = [CouplingFlow, ReverseFlow]\n",
        "my_flow = NormalizingFlow(dim = 2, device = device, blocks = block, flow_length = 8, base_distrib = ref_distrib)\n",
        "# Create optimizer algorithm\n",
        "flow_optimizer = optim.Adam(flow.parameters(), lr=0.0003, weight_decay=0.001)\n",
        "# Add learning rate scheduler\n",
        "flow_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99995)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e309f86",
      "metadata": {
        "id": "8e309f86"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "hmm_with_nf_development.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}